
* Cancer Classifier
In this project, we will be using several Python libraries to make a K-Nearest Neighbor classifier that is trained to predict whether a patient has breast cancer.

** Explore the data

*** Task 1
Let's begin by importing the breast cancer data form sklearn. We want to import the function ~load_breast_cancer~ from sklearn.datasets.

Once we've imported the dataset, let's load the data into a variable called ~breast_cancer_data~. Do this by setting ~breast_cancer_data~ equal to the function ~load_breast_cancer()~.

*** Task 2
Before jumping into creating our classifier, let's take a look at the data. Begin by printing ~breast_cancer_data.data[0]~. That's the first datapoint in our set. But what do all those numbers represent? Let's also print ~breast_cancer_data.feature_names~.

*Hint*
To see all the data, do the following:
print(breast_cancer_data.data)

*** Task 3
We now have a sense of what the data looks like, but what are we trying to classify? Let's print both ~breast_cancer_data.target~ and ~breast_cancer_data.target_names~.

Was the very first data point tagged as malignant or bening?

*Hint*
~breast_cancer_data.target~ gives you the labels of every data point. The very first data point in the set had a label of 0. By looking at the ~target_names~, we know that 0 corresponds to malignant.

** Splitting the data into training and validation sets

*** Task 4
We have our data, but now it needs to be split into training and validation sets. Luckily, sklearn has a function that does that for us. Begin by importing the train_test_split function from sklearn.model_selection.

*** Task 5
Call the train_test_split function. It takes several parameters:

    - The data you want to split (for us ~breast_cancer_data.data~)

    - The labels associated with that data (for us, ~breast_cancer_data.target~)

    - The ~test_size~. This is what percentage of your data you want to be in your testing set. Let's use ~test_size = 0.2~.

    - ~random_state~. This will ensure that every time you run your code, the data split in the same way. This can be any number. We use ~random_state = 100~.

      *Hint*
      The third and fourth parameters need the name of the parameter. For example look at the code below:

      #+begin_src python
  train_test_split(___,___,test_size = 0.2, random_state = 100)
      #+end_src

*** Task 6
Right now we're not storing the return value of ~train_test_split~. ~train_test_split~ returns four values in the following order:

    - The training set

    - The validation set

    - The training labels

    - The validation labels

  Store those values in variables named ~training_data~, ~validation_data~,~ training_labels~, ~validation_labels~.

*** Task 7
Let's confirm that worked correctly. Print out the length of ~training_data~ and ~training_labels~. They should be the same size - one label for every piece of data!

** Running the classifier

*** Task 8
Now that we've created training and validation sets, we can create a KNeighborsClassifier and test its accuracy. Begin by importing KNeighborsClassifier from sklearn.neighbors.

*** Task 9
Create a KNeighborsClassifier where n_neighbors = 3. Name the classifier ~classifier~.

*** Task 10
Train your classifier using the fit function. This funtion takes two parameters: the training set and the training labels.

*** Task 11
Now that the classifier has been trained, let's find how accurate it is on the validation set. Call the classifier's ~score~ function. ~score~ takes two parameters: the validation set and the validation labels. Print the result!


* Script.Py
#+begin_src python :results output
  from sklearn.datasets import load_breast_cancer
  from sklearn.model_selection import train_test_split
  from sklearn.neighbors import KNeighborsClassifier

  breast_cancer_data = load_breast_cancer()

  print(breast_cancer_data.data[0])

  print(breast_cancer_data.feature_names)

  print(breast_cancer_data.target)

  print(breast_cancer_data.target_names)

  training_data, validation_data, training_labels, validation_labels = train_test_split(breast_cancer_data.data, breast_cancer_data.target, test_size = 0.2, random_state = 100)

  print(len(training_data))

  print(len(training_labels))

  classifier = KNeighborsClassifier(n_neighbors = 3)

  classifier.fit(training_data, training_labels)

  print(classifier.score(validation_data, validation_labels))

  for k in range(1, 101):
      classifier = KNeighborsClassifier(n_neighbors = k)
      classifier.fit(training_data, training_labels)
      print(classifier.score(validation_data, validation_labels))

#+end_src

#+RESULTS:
#+begin_example
[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01
 1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02
 6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01
 1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01
 4.601e-01 1.189e-01]
['mean radius' 'mean texture' 'mean perimeter' 'mean area'
 'mean smoothness' 'mean compactness' 'mean concavity'
 'mean concave points' 'mean symmetry' 'mean fractal dimension'
 'radius error' 'texture error' 'perimeter error' 'area error'
 'smoothness error' 'compactness error' 'concavity error'
 'concave points error' 'symmetry error' 'fractal dimension error'
 'worst radius' 'worst texture' 'worst perimeter' 'worst area'
 'worst smoothness' 'worst compactness' 'worst concavity'
 'worst concave points' 'worst symmetry' 'worst fractal dimension']
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0
 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1
 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1
 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0
 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1
 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1
 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1
 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0
 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1
 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1
 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1
 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0
 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 0 0 0 0 0 0 1]
['malignant' 'benign']
455
455
0.9473684210526315
0.9298245614035088
0.9385964912280702
0.9473684210526315
0.9473684210526315
0.9473684210526315
0.9473684210526315
0.9473684210526315
0.9473684210526315
0.956140350877193
0.956140350877193
0.956140350877193
0.956140350877193
0.956140350877193
0.956140350877193
0.956140350877193
0.956140350877193
0.956140350877193
0.956140350877193
0.956140350877193
0.956140350877193
0.956140350877193
0.956140350877193
0.9649122807017544
0.9649122807017544
0.956140350877193
0.956140350877193
0.956140350877193
0.956140350877193
0.9473684210526315
0.9473684210526315
0.9473684210526315
0.9473684210526315
0.9473684210526315
0.9473684210526315
0.9473684210526315
0.9473684210526315
0.956140350877193
0.956140350877193
0.956140350877193
0.956140350877193
0.956140350877193
0.956140350877193
0.956140350877193
0.9473684210526315
0.956140350877193
0.9473684210526315
0.956140350877193
0.956140350877193
0.956140350877193
0.956140350877193
0.9473684210526315
0.9473684210526315
0.9473684210526315
0.956140350877193
0.956140350877193
0.9649122807017544
0.9473684210526315
0.9473684210526315
0.9385964912280702
0.9298245614035088
0.9298245614035088
0.9385964912280702
0.9473684210526315
0.9385964912280702
0.9385964912280702
0.9385964912280702
0.9385964912280702
0.9385964912280702
0.9385964912280702
0.9385964912280702
0.9385964912280702
0.9385964912280702
0.9385964912280702
0.9385964912280702
0.9385964912280702
0.9385964912280702
0.9298245614035088
0.9298245614035088
0.9298245614035088
0.9298245614035088
0.9210526315789473
0.9298245614035088
0.9210526315789473
0.9385964912280702
0.9298245614035088
0.9385964912280702
0.9385964912280702
0.9385964912280702
0.9298245614035088
0.9298245614035088
0.9210526315789473
0.9385964912280702
0.9210526315789473
0.9298245614035088
0.9298245614035088
0.9385964912280702
0.9298245614035088
0.9385964912280702
0.9298245614035088
0.9298245614035088
#+end_example
