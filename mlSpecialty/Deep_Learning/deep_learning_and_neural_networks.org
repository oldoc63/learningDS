
* What Is Deep Learning?
A quick overview of deep learning and its applications

** What is Deep Learning?
Have you ever wondered what powers ChatGPT? What technology developers are using to create self-driving cars? How your phone can recognize faces? Why it seems like your photos app is better at recognizing your friend's faces than even you can? What is behind all of this? Is it magic? Well, not exactly; it is a powerful technology called /deep learning (DL)/. Let's dive and see what this is all about!

** Deep Learning vs. Machine Learning
First, let's focus on /learning/. If you have come across machine learning before, you might be familiar with the concept of a /learning model/. Learning describes the process by which models analyse data and finds patterns. A machine learning algorithm learns from patterns to find the best representation of this data, which it then uses to make predictions about new data that is has never seen before.

Deep learning is a subfield of machine learning, and the concept of /learning/ is pretty much the same.

    - We create our model carefully

    - Throw relevant data at it

    - Train it on this data

    - Have it make predictions for data it has never seen

Deep learning models are used with many different types of data, such as text, images, audio, and more, making them applicable to many different domains.

** What does "deep" mean?
That leaves the question: what is this "deep" aspect of deep learning? It separates deep learning from typical machine learning models and why it is a powerful tool that is becoming more prevalent in today's society.

The deep part of deep learning refers to the numerous "layers" that transform data. This architecture mimics the structure of the brain, where each succesive layer attempts to learn progressively complex patterns from the data fed into the model. This may seem a bit abstract, so let's look at a concrete example, such as facial recognition. With facial recognition, a deep learning model takes in a photo as an input, and numerous layers perform specific steps to identify whose face is in the picture. The steps taken by each layer might be the following:

    1. Find the face within the image using edge detection.

    2. Analyze the facial features (eyes, nose, mouth, etc.).

    3. Compare against faces within a repository.

    4. Output a prediction!

This estructure of many abstract layers makes deep learning incredibly powerful. Feeding high volumes of data into the model makes the connections between layers more intricate. Deep learning models tend to perform better with more massibe amounts of data than other learning algorithms.

** High volumen of data
Notice thah without large amounts of data, deep learning models are no more powerful (and maybe even less accurate) than less complex learning models. However, with large amounts od data, deep learning models can improve performance to the point that they outperform humans in tasks such as classifying objects or faces in images or driving. Deep learning is fundamentally a future learning system (also known as representation learning). It learns from raw data without human intervention. Hence, given massive amounts of data, a deep learning system will perform better than traditional machine learning systems that rely on feature extractions from developers.

[[./deep_learning_performance.png]]

Autonomous vehicles, such as Tesla, have to process thousands, even millions of stop signs, to understand that cars are supposed to stop when they see a red hexagon with "Stop" written on them (and this is only for U.S. stop signs!). Think about the enormous number of situations a self-driving vehicle must train on to ensure safety.

** With deep learning comes deep responsability
Even beyond identifying objects, deep learning models can generate audio and visual content that is deceivingly real. They can modify existing images, such as in this cool applet that allows you to add in trees or alter buildings in a set of photos. However, they can have a darker side. DL models can produce artificial media in which the identity of someone in an image, video, or audio is replaced with someone else. These are known as deepfakes, and they can have scary implications, such as financial fraud and the distribution of fake news and hoaxes.

** Graphics Processing Units
One final thing to note about deep learning is that with large amounts of data and layers of complexity, you may imagine that this takes a lot of time and processing power. That intuition would be correct. These models often require high-performance GPUs (graphics processing units) to run in a reasonable amount of time. These processors have a large memory bandwidth and can process multiple computations simultaneously. CPUs can run deep learning models as well; however, they will be much slower.

The development of GPUs has been critical to the success of deep learning. It is interesting to note that one of the driving factors for this development was not the need for better deep learning tools, but the demand for better video game graphics. It just so happened that GPUs are perfect for processing large datasets. This makes them a perfect tool for learning models and has put deep learning specifically at the forefront of machine learning conversations.

We have only begun our dive into deep learning. Let's keep digging in and see where our juorney takes us!

* Deep Learning Math

** Introduction
Before we dive into creating our deep learning models, let's take a step back and unbox the mechanisms of these models. In this lesson, we will investigate the foundations that run through the inner workings of neural networks. Hopefully, after reading about the steps our data takes on its deep learning journey, you will have a clearer picture of the overall process and feel ready to get your hands on some code!

We are not going to assume that you have a deep understanding of linear algebra, and you will not need a high-level math background to follow along.

** Scalars, Vectors, and Matrices
To start, let us go over a couple of topics that will be integral to understanding the mathematical operations that are present in deep learning, including how data is represented:

    - /Scalars:/ A scalar is a single quantity that you can think of as a number. In machine learning models, we can use scalar quantities to manipulate data, and we often modify them to improve our model's accuracy. We can also represent data as scalar vaues depending on what dataset we are working with.

          - Code example:

    #+begin_src python
    x = 5
    #+end_src

    - /Vectors:/ Vectors are arrays of numbers. In Python, we often denote vectors ad NumPy arrays. Each value in the array can be identified by its index (location within the array).

          - Code example:

    #+begin_src python
    x = np.array([1,2,3])
    #+end_src

    - /Matrices:/ Matrices are grids of information with rows and columns. We can index a matrix just like an array; however, when indexing on a matrix, we need two arguments: one for the row and one for the column.

          - Code example:

    #+begin_src python
    x = np.array([[1,2,3], [4,5,6], [7,8,9]])
    #+end_src

[[./scalar_vector_matrix.png]]

** Tensors
/Scalars, vectors,/ and /matrices/ are foundational objects in linear algebra. Understanding the different ways they interact with each other and can be manipulated through matrix algebra is integral before diving into  deep learning. This is because the data structure we use in deep learning is called a /tensor,/ which is a generalized form of a vector and matrix: a multidimensional array.

A tensor allows for more flexibility with the type of data you are using and how you can manipulate that data.

[[./tensor-applet.svg]]

Let's us visualize what they look like and how they can be represented using NumPy arrays. Use the applet on this page to get a feel for a tensor's structure and how it is represented.

In this applet, you are given boxes where you can fill in negative and positive whole numbers. On the right side is a set of nested arrays. These nested arrays form a 3-dimensional tensor that can be viewed as a "stack" or "layer" of grids, as shown on the left.

The shape of this tensor is (3, 2, 5), as outlined on the diagram. The shape of our data is an important factor when we are feeding it into our neural network. It affects the way our model interacts with our inputs. This is something you will see in future lessons!

If you would like to see the applet in action, feel free to do so [[https://content.codecademy.com/courses/deeplearning-with-tensorflow/deep-learning-math/applet_3_new.html][here]]!

If you would like to read more about tensors and see more ways to visualize tensors, feel free to do so [[https://www.tensorflow.org/guide/tensor][here]]!

** Matrix Algebra
The following gifs walkthrough matrix multiplication, addition and transpose. You can perform element-wise operations on tensors using matrix algebra as well, which you can read more about [[https://en.wikipedia.org/wiki/Matrix_(mathematics)#Addition,_scalar_multiplication,_and_transposition][here]].

*** Matrix Addition:

[[./matrix_addition.webp]]

*** Scalar Multiplication:

[[./scalar_multi.webp]]

*** Matrix Multiplication:

This is the [[https://content.codecademy.com/courses/deeplearning-with-tensorflow/deep-learning-math/matrix_interactive/index.html][most complicated]].

*** Transpose:

[[./transpose.webp]]

This is all of the matrix algebra we need to proceed with the rest of our deep learning investigation! These concepts are the fundamental building blocks of why deep learning models are so powerful. When we are training our models, we are performing operations on tensors. This data is analyzed, manipulated, and shaped by the matrix algebra we have quickly gone over.

** Neural Networks [[https://content.codecademy.com/courses/deeplearning-with-tensorflow/deep-learning-math/applet_1.html][Concept Overview]]
Let's take a look at the journey our /inputs/ take inside of a neural network! By an input, we mean a data point from our dataset. Our input can have many different features, so in our /input layer,/ each node represents a different input feature. For example, if we were working with a dataset of different types of food, some of our features might be size, shape, nutrition, etc., where the value for each of these features would be held in an input node.

Besides an input layer, our neural network has two other different types of layers:

    /Hidden layers/ are layers that come between the input layer and the output layer. They introduce complexity into our neural network and help with the learning process. You can have as many hidden layers as you want in a neural network (including zero of them).

    The /output layer/ is the final layer in our neural network. It produces the final result, so every neural network must have only one output layer.

Each layer in a neural network contains nodes. Nodes between each layer are connected by /weights/. These are the learning parameters of our neural network, determining the strength of the connection between each linked node.

The weighted sum between nodes and weights is calculated between each layer. For example, from our input layer, we take the weighted sum of the inputs and our weights with the following equation:

$$
weighted\ sum = (inputs \cdot weight\ transpose) + bias
$$

We then apply an activation function to it.

$$
Activation(weighted\ sum)
$$

The two formulas we have gone over take all the inputs through one layer of a neural network. Aside from the activation function, all of the transformations we have done so far are linear. Activation functions /introduce nonlinearity/ in our learning model, creating more complexity during the learning process.

This is what makes activation functions important. A neural network with many hidden layers but no activation functions would just be a series of succesive layers that would be no more effective or accurate than simple linear regression.

An activation function decides what is fired to the next neuron based on its calculation for the weighted sums. Various types of activation functions can be applied at each layer. The most popular one for hidden layers is /ReLU./

[[./ReLU.png]]

Others commonly used, often for the output layer, are /sigmoid/ and /softmax/. You will learn more about these functions as you use them later in this course.

[[./sigmoid.png]]

In the diagram, we see a basic neural network with no hidden layers. Use your mouse to hover over each section of the image to get a feel for how each step of the neural network works. Each part of the diagram contains a description to indicate its role in a neural network.

(Note: on some browsers, you may need to scroll down to see the descriptions)

** The [[https://content.codecademy.com/courses/deeplearning-with-tensorflow/deep-learning-math/applet_2_new.html][Math Behind]] the Journey
Let's bring all of these concepts together and see how they function in a neural network with one hidden layer. As you scroll over each section, you will see the inputs/weights/calculations associated with it and see how inputs get from the starting point and make their way to the end!

The process we have been going through is known as /forward propagation./ Inputs are moved forward from the input layer through the hidden layer(s) until they reach the output layer.

In the applet, you can scroll over each part of the neural network and observe the mathematics behind the diagram.

When you scroll over the input section, you should see how the input is represented as a vector. Scrolling over the weights, we see how each set of weights (blue and yellow) is represented as a vector. When brought together, they make up the ~weights_matrix~ and ~weights_matrix_transpose~.

When scrolling through the hidden nodes sections, you will notice that there are two parts. In the first step, we take the weighted sum of our data using the ~weights_matrix_transpose~. From this, we end up with a vector and apply our ReLU activation function to it.

This takes us to our teal weights. These are represented as a vector. The ~weights_teal_transpose~ turns our /row vector/ into a /column vector/. Then we take another weighted sum in our output layer, this time between our ~hidden_nodes~ and our ~weights_teal_transpose~. Following this, we have a sigmoid activation function, which gives us our output.

Feel free to open this applet in a separate window for a larger viewing screen.

We now understand the adventure our data takes on one journey through our neural network. We are not quite finished yet, though. Letâ€™s keep exploring!

** Loss Functions
We have seen how we get to an output! Now, what do we do with it? When a value is outputted, we calculate its error using a loss function. Our predicted values are compared with the actual values within the training data. There are two commonly used loss calculation formulas:

    - Mean squared error, which is most likely familiar to you if you have come across linear regression. This gif below show how mean squared error is [[https://content.codecademy.com/courses/deeplearning-with-tensorflow/deep-learning-math/Loss.gif][calculated]] for a /line of best fit/ in /linear regression./

    - Cross-entropy loss, which is used for classification learning models rather than regression.

You will learn more about this as you use loss functions in your deep learning models.

The [[https://content.codecademy.com/programs/data-science-path/line-fitter/line-fitter.html][interactive visualization]] in the browser lets you try to find the line of best fit for a random set of data points:

    - The slider on the left controls the ~m~ (slope)

    - The slider on the right controls the ~b~ (intercept)

    - You can see the *total squared error* on the right side of the visualization. To get the line of best fit, we want this loss to be as small as possible.

To check if you got the best line, check the "Plot Best-Fit" box.

Randomize a new set of points and try to fit a new line by entering the number of points you want (try 8!) in the textbox and pressing Randomize Points.

*Note:*
This visualization works best with fewer than 50 points but will continue to work up to 1000 points. Attempting to add more than 1000 points may result in significant lag or the applet crashing.

Play around with the interactive applet, and notice what method you use to minimize loss:

    - Do you first get the slope to where it produces lowest loss, and then move the intercept to where it produces lowest loss?

    - Do you create a rough idea in your mind where the line should be first, and then enter the parameters to match that image?

** Backpropagation
This all seems fine and dandy so far. However, what if our output values are inaccurate? Do we cry? Try harder next time? Well, we can do that, but the good news is that there is more to our deep learning models.

This is where /backpropagation/ and /gradient descent/ come into play. Forward propagation deals with feeding the input values through hidden layers to the final output layer. Backpropagation refers to the computation of gradients with an algorithm known as gradient descent. This algorithm continuosly updates and refines the weights between neurons to minimize our loss function.

By gradient, we mean the rate of change with respect to the parameters of our loss function. From this, backpropagation determines how much each weight is contributing to the error in our loss function, and gradient descent will update  our weight values accordingly to decrease this error.

This is a [[https://content.codecademy.com/courses/deeplearning-with-tensorflow/deep-learning-math/interactives/index.html][conceptual overview]] of backpropagation. If you would like to engage with the gritty mathematics of it, you can do so [[https://en.wikipedia.org/wiki/Backpropagation][here]].

Let's take a look at what happens with backpropagation and gradient descent on a neural network directly. In the applet in the learning enviroment, watch as weights are updated and error is decreased after each iteration. Without backpropagation, neural networks would be much less accurate.

** Gradient Descent
We have the overall process of backpropagation down! Now, let's zoom in on what is happening during gradient descent.

If we think about the concept graphically, we want to look for the minimum point of our loss function because this will yield us the highest accuracy. If we start at a random point on our loss function, gradient descent will take "steps" in the "downhill direction" towards the negative gradient. The size of the "step" taken is depending on our learning rate. Choosing the optimal learning rate is important because it affects both the efficiency and accuracy of our results.

The formula used with learning rate to update our weight parameters es the following:

$$
parameter\ new = parameter\ old + learning\ rate \cdot gradient(loss\ function(parameter\ old))
$$

The learning rate we choose affects how large the "steps" our pointer takes when trying to optimize our error function. Initial intuition might indicate that you should choose a large learning rate; however, as shown above, this can lead you to overshoot the value we are looking for and cause a divergent search.

Now you might think that you should choose an incredibly small learning rate; however, if it is too small, it could cause your model to be unbearably inefficient or get stuck in a local minimun and never find the optimum value. It is a tricky game of finding the correct combination of efficiency and accuracy.

Take a look at the graphs, which depict some common issues you may run into when selecting a learning rate. If you select a learning rate that is too small, it will take a very long time to find the minimum value. However, if you choose a learning rate that is too large, it might overshoot the minimum value an end up eith a divergent algorithm. Choosing an ideal learnign rate means it should find the ideal loss value efficiently and accurately.

[[./Gradient_descent.gif]]

** Stochastic Gradient Descent
This leads us to the final point about gradient descent. In deep learning models, we are often dealing with extremely large datasets. Because of this, performing backpropagation and gradient descent calculations on all of our data may be ineficient and computationally exhaustive no matter what learning rate we choose.

To solve this problem, a variation of gradient descent known as /Scochastic Gradient Descent (SGD)/ was developed. Let's say we have 100,000 data points and 5 parameters. If we did 1000 iterations (also known as epochs in Deep Learning) we would end up with 100000 \cdot 5 \cdot 1000 = 500,000,000 computations. We do not want our computer to do that many computations on top of the rest of the learning model; it will take forever.

This is where SGD comes to play. Instead of performing gradient descent on our entire dataset, we pick out a random data point to use at each iteration. This cuts back on computation time immensely while still yielding accurate results.

The diagram below shows the performance differences between SGD and GD. You may notice that the SGD graph is a bit more sporadic. There is a reason for this, and we will address it in the next exercise.

The main point is that both will reach the ideal parameters, and SGD will be easier and more efficient for your computer processor. Because of this, SGD is almost universally used in favor of normal GD.

However, as well will see next, there are even more variants of gradient descent.

[[./stochastic.svg]]

** More variant of gradient descent
Just when you thought SGD solved all our problems, even more options come into the picture!

There are also other variants of gradient descent such as -Adam optimization algorithm- and /mini-batch gradient descent./ Adam is an adaptive learning algorithm that finds individuals learning rates for each parameter. Mini-batch gradient descent is similar to SGD except instead of iterating on one data point at a time, we iterate on small batches of fixed size.

Adam optimizer's ability to have an adaptive learning rate has made it an ideal varian of gradient descent and is commonly used in deep learning models. Mini-batch gradient descent was developed as an ideal trade-off between GD and SGD. Since mini-batch does not depend on just one training sample, it has a much smoother curve and is less affected by outliers and noisy data making it a more optimal algorithm for gradient descent than SGD.

These are just some quick notes! You can read more about Adam [[https://arxiv.org/abs/1412.6980][here]] and more about mini-batch [[https://arxiv.org/pdf/1609.04747][here]]. Experts in deep learning are constantly coming up with ways to improve these algorithms to make them more efficient and accurate, so the ability to adapt and build upon what you learn as you dive into this domain will be key!

[[./GD_variants.webp]]

This is a diagram you will become accustomed to in future lessons. It shows us our loss function performance over many /epochs/ (iterations) of our deep learning model with different types of gradient descent.

These graphs are extremely useful when creating learning models because they offer us a detailed view of their performance.

** Review
This overview completes  the necessary mathematical intuition you need to move forward and begin coding your own learning models! To recap all the things we have learned:

    - Scalars, vectors, matrices, and tensors

          - A /scalar/ is a singular quantity like a number.

          - A /vector/ is an array of numbers (scalar values).

          - A /matrix/ is a grid of information with rows and columns.

          - A /tensor/ is a multidimensional array and is a generalized version of a vector and matrix.

    - Matrix Algebra

          - In /scalar multiplication,/ every entry of the matrix is multiplied by a scalar value.

          - In /matrix addition,/ corresponding matrix entries are added together.

          - In /matrix multiplication,/ the dot product between the corresponding rows of the first matrix and columns of the second matrix is calculated.

          - A /matrix transpose/ turns the rows of a matrix into columns.


    - In /forward propagation,/ data is sent through a neural network to get initial outputs and error values.

          - /Weights/ are the learning parameters of a deep learning model that determine the strength of the connection between two nodes.

          - A /bias node/ shifts the activation function either left or right to create the best fit for the given data in a deep learning model.

          - /Activation Functions/ are used in each layer of a neural network and determine whether neurons should be "fired" or not based on output from a weighted sum.

          - /Loss functions/ are used to calculate the error between the predicted values and actual values of our training set in a learning model.

    - In /backpropagation,/ the gradient of the loss function is calculated with respect to the weight parameters within a neural network.

          - /Gradient descent/ updates our weight parameters by iteratively minimizing our loss function to increase our model's accuracy.

          - /Stochastic gradient descent/ is a variant of gradient descent, where instead of using all data points to update parameters, a random data point is selected.

          - /Adam optimization/ is a variant of SGD that allows for adaptive learning rates.

          - /Mini-batch gradient descent/ is a variant of GD that uses random batches of data to update parameters instead of a random datapoint.

* Dangers of the black box
Deep learning models have deep implications.

** What makes deep learning models dangerous?
When talking about machine learning, deep learning, and artificial inteligence, people tend to focus on the progress and amazing feats we could potentially achieve. While it is true that these disciplines have the potential to change the world we live in and allow us to perform otherwise impossible feats, there are often unintended consequences.

We live in an imperfect world, and the learning algorithms we design are not immune to these imperfections. Before we dive itno creating our models, we must review some of the issues and implications that they can have on people's lives. Deep learning models can be especially scary as they are some of the most powerful, and they are becoming more commonplace in society. They are also most often /black boxes/ (hard for users to understand as to why and how specific results occur).

** When to use them
Due to this nature of deep learning models, we should only ever use them if there is an apparent, significant reason for using one. If there is not a clear reason, we can use basic machine learning or statistical analysis approaches depending on what suffices.

When choosing solutions to a problem, we have to juggle many numerous factors, including:

    - speed

    - accuracy

    - training time

    - interpretability

    - maintenance

    - enhancement

    - size of the trained model

along with even more before beginning to prototype. Asking questions along these lines is vital because we design learning algorithms that can harm individuals or even entire communities in their daily lives if we do not craft them with care and awareness.

** Misconceptions
There are often misconceptions about AI and deep learning models and what implications they have on our world. Science fiction has illustrated the dangers as some sort of robot apocalypse shere humans are outsmarted and overpowered. This depiction does not reflect the real risks that are already presetn from the growing dependence on learning models in oru everyday lives. Healthcare, banking, ant the criminal justice system have all seen a massive rise in the reliance on learning algorithms to make decision in recent years. While these have led to increased efficiency and developments, trusting these systems to make high-risk decisions has also led to various risks.

** Key Issues
There are some key things to address about machine learning models that can lead to potentially problematic implications. Let's dive into this through the lens of healthcare.

    1. *Machine learning algorithms can only be as good as the data it is trained on.*

       If we train the data on a model that does not match environmental data well, accuracy will not translate into the real world. A good example is this [[https://academic.oup.com/jamia/article/24/6/1052/3096776?login=false][study]] on personalized risk assessments for acute kidney injury. While patient data evolved and disease patterns changed, the predictive model became less accurate and, therefore, much less useful. It is crucial for developers to monitor outputs pericodically and account for data-shift problems over time. The work is not complete after we deploy a model; it must be managed and continuosly refined to account for continuous environmental developments.

       There is also a history of the health industry not including enough women and people of color in medical studies. Different demographics can have unique manifestations and risk factors with dieseases. If training data is not diverse and representative of all potential sample individuals, inaccuracies and misdiagnoses could occur. It is vital to have orthogonal data in that it is both high in volume and diversity. Without attention to this, social health disparities that are already present could become widened.

    2. *Machine learning models do not understand the impact of a false negative vs. a false positive diagnostic (at least not like humans can)*

       When diagnosing patients, doctor's often "err on the side of caution." For example, being falsely diagnosed with a malignant tumor (false positive) is much less dangerous than being incorrectly diagnosed with a benign tumor (false negative). Further testing would occur in the former situation, whereas teh latter could cause much scarier consequences with a maignant tumor being unaddressed.

       Models may not have this "err on the side of caution" attitude, especially if we do not design them with this implication in mind. If we solely train it to be as accurate as possible, it is at the expense of missing malignant diagnoses. Developers can create custom loss functions that can account for the implications of false negatives vs. false positives. However, to do this, they must understand the domain well.

    3. *For many of the clinicians and the patients, the models are a black box.*

       Stakeholders only can make decisions based on the outcome, and the predictions are not open to inspection. Therefore, it is hard for anyone to determine that there are patterns of inaccurate predictions until prolonged use has already occurred. For example, imagine an X-ray analysis model that is inacucurate under certain conditions because it was not present in the training data. The doctor would not identify this until continuously observing incorrect diagnoses because the only aspect of the model focuses on is the outcome.

** More examples
This is just the tip of the iceberg for concerns about the use of learning models in the healthcare industry, and we have yet to even go into implications within other sectors. Facial recognition has shown implicit bias within Google's algorithm, which [[https://web.archive.org/web/20201004111743/https://www.wnycstudios.org/podcasts/notetoself/episodes/deep-problem-deep-learning][incorrectly identified a woman and her friend as gorillas]]. An error like this leads to emotional harm, and it shows that learning models are immune to social issues and can reproduce or even exacerbate them.

Employing black box technology becomes more of an issue when used in contexts without transparency. For example, in criminal justice or banking, biased data is used to deny people of color loans at a higher rate or label them as "high risk" repeat offenders. A real-life example of this is a machine learning questionnaire algorithm known as COMPAS (Correctional Offender Management Profiling for Alternative Sanctions). It was used to determine the risk of whether an arrestee would being a repeat offender. A [[https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing][study]] showed that this algorithm was racially-biased despite never explicitly asking about race. Individuals were asked questions that modeled existing social inequalities, and minorities, particularly blacks, were more likely to be labeled "high-risk" repeat offenders. The graph below shows the distribution of risk scores for black defendants and white defendants. You can read more about the analysis of this report [[https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm][here]].

Models like these can even go beyond mirroring existing inequity. They can go onto perpetuate them and contribute to vicious cycles. For example, if we were to use systems like these to determine patrol routes, this could bias crime data for individuals in those communities. As we add more data to the learning model, the problem is exacerbated. What is even scarier about these models is that they are often beyond dispute or appeal. Given the result of a mathematical formula, we usually take it as fact. Someone who ends up negatively affected by this cannot argue against it or reason with a machine. They cannot wxplain the full reality to a computer. Instead, a machine will define its own reality to justify its results. While an outside observer can question, "why did this individual get a high-risk score despite only a minor crime?" a machine is merely operating under its historical data and findings.

** Personal Responsability
This is not to say that we should not use machine learning models in ways that impact everyday lives. It is meant to outline some of the issues that can arise when used to make high-stakes decisions and how they can cause harm. As you move forward to developing your own neural networks, consider the social implications of what you have made. As developers, we must work to ensure that our models are free from bias and will not misrepresent certain populations. The Institute or Ethical AI and Machine Learning has a [[https://ethical.institute/principles.html][framework for developing responsible machine learning]] that we encourage you to read up on.

** Interpretability and transparency
It is also imperative that the models we build are used in ways that are transparent to potential stakeholders. If someone is impacted due to the output of a learning model, they should understand:

    - why the model is being used

    - how their personal data is being used

    - what personal data is being used

The final thing to consider is making your model's inner workings understandable for your users, also known as /interpretable machine learning./ Mking the inner workings of a model understandable allows users to identify inaccuracies earlier and explain results to potential stakeholders. A great example of this is a simple classifier model that identifies huskies vs. wolves. In the case below, a husky is incorrectly classified as a wolf. With the implementation of a system called [[https://arxiv.org/pdf/1602.04938][LIME]], predictions are explained and can be understood by users. In this case, the explanation is that if a picture contains snow, it will be classified as a wolf. This information gives developers an indicator of why their model contains inaccuracies and clarifies how to improve it.

** In Conclusion
There is no doubt that machine learning can help us make waves of progress. However, in a world riddled with inequity, developers must attempt design systems so that no one is left behind. By designing learning models that account for limitations of interpretability and likelihood of bias, we can take strides to ensure that individuals and communities are not unjustly harmed.

* What are Neural Networks?
An artificial neural network is an interconnected group of nodes, an attempt to mimic to the vast network of neurons in a brain.

** Understanding Neural Networks
As you are reading this article, the very same brain that sometimes forgets why you walked into a room is magically translating these pixels into letters, words, and sentences -a feat that puts the world's fastest supercomputers to shame. Within the brain, thousands of neurons are firing at incredible speed and accuracy to help us recognize text, images, and the world at large.

A *neural network* is a programming model inspired by the human brain. Let's explore how it came into existence.

** The birth of an artificial neuron
Computers have been designed to excel at number-crunching tasks, something that most humans find terrifying. On the other hand, humans are naturally wired to effortlessly recognize objects and patterns, something that computers find difficult.

This juxtaposition brought up two important questions in the 1950s:

    - "How can computers be better at solving problems that humans find effortless?"

    - "How can computers solve such problems in the way a human brain does?"

In 1957, [[https://en.wikipedia.org/wiki/Frank_Rosenblatt][Frank Rosenblatt]] explored the second question and invented the *Perceptron* algorithm that allowed an artificial neuron to simulate a biological neuron! The artificial neuron could take in an input, process it based on some rules, and fire a result. But computers had been doing this for years -what was so remarkable?

[[./perceptron.webp]]

There was a final step in the Perceptron algorithm that would give rise to the incredible mysterious world of Neural Networks -the artificial neuron could /train itself based on it on results, an fire better results in the future./ In other words, it could learn by trail and error, just like a biological neuron.

** More neurons
The perceptron algorithm used multiple artificial neurons, or perceptrons, for image recognition tasks and opened up a whole new way to solve computational problems. However, a it turns out, this wasn't enough to solve a wide range of problems, and interest in the perceptron algorithm along with neural networks waned for many years.

But many years later, the neurons fired back.

It was found out that creating multiple layers of neurons -with one layer feeding its output to the next layer as input -could process a wide range of inputs, make compex decisions, and still produce meaninful results. With some tweaks, the algorithm became known as the *Multilayer Perceptron*, which led to the rise of feedforward neural networks.

** 60 years later ...
With feedforward networks, the results improved. But it was only recently, with the development of high-speed processors, that neural networks finally got the necessary computing power to semlessly integrate into daily human life.

Today, the applications of neural networks have become widespread -from simple tasks like speech recognition to more complicated tasks like self-driving vehicles.

In 2012, [[https://qz.com/1307091/the-inside-story-of-how-ai-got-good-enough-to-dominate-silicon-valley][Alex Krizhevsky]] and his team at University of Toronto entered the ImageNet competition (the annual Olympics of computer vision) and trained a deep convolutional neural network [[https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf][pdf]]. No one truly understood how it made the decisions it did, but it worked better than any other traditional classifier, by a huge 10.8% margin.

** Summary
The neurons have come a long way. They have braved tha AI winter and remained patient amidst the lack of computing power int the 20th century. They have now taken the world by storm and deservedly so.

Neural networks are ridiculously good at generating results but also mysteriously complex; the apparent complexity of the decision-making process makes it difficult to say exactly how neural networks arrive at their superhuman level of accuracy.

Let's dive into the world of Neural Networks and relish in all its mystery!

* What is a Perceptron?
Similar to how atoms are the building blocks of matter and how microprocessors are the building blocks of a computer, perceptrons are the building blocks of Neural Networks.

If you look closely, you might notice that the word "perceptron" is a combination of two words:

    - *Perception* (noum) the ability to sense something

    - *Neuron* (noum) a nerve cell in the human brain that turns sensory input into meaningful information

Therefore, the perceptron is an artificial neuron that simulates the task of a biological neuron to solve problems through its own "sense" of the world.

Although the perceptron comes with its own artificial design and set of parameters, at its core, a single perceptron is trying to make a simple decision.

Let's take the example a simple self-driving car that is based on a perceptron. If there's an obstacle on the left, the car would have to steer right. Similarly, if there's and obstacle on the right, the car would have to steer left.

For this example, a perceptron could take the position of the obstacle as inputs, and produce a decision -left turn or right turn- based on those inputs.

And here's the cool part -the perceptron can correct itself based on the result of its decision to make better decisions in the future!

Of course, the real world scenario isn't that simple. But if you combine a bunch of such perceptrons, you will get a neural network that can even make better decisions on your behalf!

* Representing a Perceptron
So the perceptron is an artificial neuron that can make a simple decision. Let's implement one from scratch in Python!

The perceptron has three main components:

    - *Inputs:* Each input corresponds to a feature. For example, in the case of a person, features could be age, height, weight, college degree, etc.

    - *Weights:* Each input also has a weight which assigns a certain amount of importance to the input. If an input's weight is large, it means this input plays a bigger role in determining the output. For example, a team's skill level will have a bigger weight than the average age of players in determining the ocutcome of a match.

    - *Output:* Finally, the perceptron uses the inputs and weights to produce an output. The type of the output varies depending on the nature of the problem. For example, to predict whether or not it's going to rain, the output has to be binary -1 for Yes and 0 for No. However, to predict the temperature for the next day, the range of the output has to be larger -say a number from 70 to 90.

** Task 1
Our ~Perceptron~ class by default takes two inputs and a pre-defined weight for each input.

Complete the ~__init__()~ method inside the ~Perceptron~ class by creating instance variables ~self.num_inputs~ and ~self.weights~ that represent the attributes of a ~Perceptron~ object.

Assign the parameters num_inputs and weights to self.num_imputs and self.weights respectively.

** Task 2
Create a ~Perceptron~ object called ~cool_perceptron~ (without any arguments) and print it out to see what it looks like.

** Script.py

#+begin_src python :results output
  class Perceptron:
      def __init__(self, num_inputs=2, weights=[1,1]):
          self.num_inputs = num_inputs
          self.weights = weights

  cool_perceptron = Perceptron()

  print(cool_perceptron)
#+end_src

#+RESULTS:
: <__main__.Perceptron object at 0x72585a1021e0>
