
* What Is Deep Learning?
A quick overview of deep learning and its applications

** What is Deep Learning?
Have you ever wondered what powers ChatGPT? What technology developers are using to create self-driving cars? How your phone can recognize faces? Why it seems like your photos app is better at recognizing your friend's faces than even you can? What is behind all of this? Is it magic? Well, not exactly; it is a powerful technology called /deep learning (DL)/. Let's dive and see what this is all about!

** Deep Learning vs. Machine Learning
First, let's focus on /learning/. If you have come across machine learning before, you might be familiar with the concept of a /learning model/. Learning describes the process by which models analyse data and finds patterns. A machine learning algorithm learns from patterns to find the best representation of this data, which it then uses to make predictions about new data that is has never seen before.

Deep learning is a subfield of machine learning, and the concept of /learning/ is pretty much the same.

    - We create our model carefully

    - Throw relevant data at it

    - Train it on this data

    - Have it make predictions for data it has never seen

Deep learning models are used with many different types of data, such as text, images, audio, and more, making them applicable to many different domains.

** What does "deep" mean?
That leaves the question: what is this "deep" aspect of deep learning? It separates deep learning from typical machine learning models and why it is a powerful tool that is becoming more prevalent in today's society.

The deep part of deep learning refers to the numerous "layers" that transform data. This architecture mimics the structure of the brain, where each succesive layer attempts to learn progressively complex patterns from the data fed into the model. This may seem a bit abstract, so let's look at a concrete example, such as facial recognition. With facial recognition, a deep learning model takes in a photo as an input, and numerous layers perform specific steps to identify whose face is in the picture. The steps taken by each layer might be the following:

    1. Find the face within the image using edge detection.

    2. Analyze the facial features (eyes, nose, mouth, etc.).

    3. Compare against faces within a repository.

    4. Output a prediction!

This estructure of many abstract layers makes deep learning incredibly powerful. Feeding high volumes of data into the model makes the connections between layers more intricate. Deep learning models tend to perform better with more massibe amounts of data than other learning algorithms.

** High volumen of data
Notice thah without large amounts of data, deep learning models are no more powerful (and maybe even less accurate) than less complex learning models. However, with large amounts od data, deep learning models can improve performance to the point that they outperform humans in tasks such as classifying objects or faces in images or driving. Deep learning is fundamentally a future learning system (also known as representation learning). It learns from raw data without human intervention. Hence, given massive amounts of data, a deep learning system will perform better than traditional machine learning systems that rely on feature extractions from developers.

[[./deep_learning_performance.png]]

Autonomous vehicles, such as Tesla, have to process thousands, even millions of stop signs, to understand that cars are supposed to stop when they see a red hexagon with "Stop" written on them (and this is only for U.S. stop signs!). Think about the enormous number of situations a self-driving vehicle must train on to ensure safety.

** With deep learning comes deep responsability
Even beyond identifying objects, deep learning models can generate audio and visual content that is deceivingly real. They can modify existing images, such as in this cool applet that allows you to add in trees or alter buildings in a set of photos. However, they can have a darker side. DL models can produce artificial media in which the identity of someone in an image, video, or audio is replaced with someone else. These are known as deepfakes, and they can have scary implications, such as financial fraud and the distribution of fake news and hoaxes.

** Graphics Processing Units
One final thing to note about deep learning is that with large amounts of data and layers of complexity, you may imagine that this takes a lot of time and processing power. That intuition would be correct. These models often require high-performance GPUs (graphics processing units) to run in a reasonable amount of time. These processors have a large memory bandwidth and can process multiple computations simultaneously. CPUs can run deep learning models as well; however, they will be much slower.

The development of GPUs has been critical to the success of deep learning. It is interesting to note that one of the driving factors for this development was not the need for better deep learning tools, but the demand for better video game graphics. It just so happened that GPUs are perfect for processing large datasets. This makes them a perfect tool for learning models and has put deep learning specifically at the forefront of machine learning conversations.

We have only begun our dive into deep learning. Let's keep digging in and see where our juorney takes us!

* Deep Learning Math

** Introduction
Before we dive into creating our deep learning models, let's take a step back and unbox the mechanisms of these models. In this lesson, we will investigate the foundations that run through the inner workings of neural networks. Hopefully, after reading about the steps our data takes on its deep learning journey, you will have a clearer picture of the overall process and feel ready to get your hands on some code!

We are not going to assume that you have a deep understanding of linear algebra, and you will not need a high-level math background to follow along.

** Scalars, Vectors, and Matrices
To start, let us go over a couple of topics that will be integral to understanding the mathematical operations that are present in deep learning, including how data is represented:

    - /Scalars:/ A scalar is a single quantity that you can think of as a number. In machine learning models, we can use scalar quantities to manipulate data, and we often modify them to improve our model's accuracy. We can also represent data as scalar vaues depending on what dataset we are working with.

          - Code example:

    #+begin_src python
    x = 5
    #+end_src

    - /Vectors:/ Vectors are arrays of numbers. In Python, we often denote vectors ad NumPy arrays. Each value in the array can be identified by its index (location within the array).

          - Code example:

    #+begin_src python
    x = np.array([1,2,3])
    #+end_src

    - /Matrices:/ Matrices are grids of information with rows and columns. We can index a matrix just like an array; however, when indexing on a matrix, we need two arguments: one for the row and one for the column.

          - Code example:

    #+begin_src python
    x = np.array([[1,2,3], [4,5,6], [7,8,9]])
    #+end_src

[[./scalar_vector_matrix.png]]

** Tensors
/Scalars, vectors,/ and /matrices/ are foundational objects in linear algebra. Understanding the different ways they interact with each other and can be manipulated through matrix algebra is integral before diving into  deep learning. This is because the data structure we use in deep learning is called a /tensor,/ which is a generalized form of a vector and matrix: a multidimensional array.

A tensor allows for more flexibility with the type of data you are using and how you can manipulate that data.

[[./tensor-applet.svg]]

Let's us visualize what they look like and how they can be represented using NumPy arrays. Use the applet on this page to get a feel for a tensor's structure and how it is represented.

In this applet, you are given boxes where you can fill in negative and positive whole numbers. On the right side is a set of nested arrays. These nested arrays form a 3-dimensional tensor that can be viewed as a "stack" or "layer" of grids, as shown on the left.

The shape of this tensor is (3, 2, 5), as outlined on the diagram. The shape of our data is an important factor when we are feeding it into our neural network. It affects the way our model interacts with our inputs. This is something you will see in future lessons!

If you would like to see the applet in action, feel free to do so [[https://content.codecademy.com/courses/deeplearning-with-tensorflow/deep-learning-math/applet_3_new.html][here]]!

If you would like to read more about tensors and see more ways to visualize tensors, feel free to do so [[https://www.tensorflow.org/guide/tensor][here]]!

** Matrix Algebra
The following gifs walkthrough matrix multiplication, addition and transpose. You can perform element-wise operations on tensors using matrix algebra as well, which you can read more about [[https://en.wikipedia.org/wiki/Matrix_(mathematics)#Addition,_scalar_multiplication,_and_transposition][here]].

*** Matrix Addition:

[[./matrix_addition.webp]]

*** Scalar Multiplication:

[[./scalar_multi.webp]]

*** Matrix Multiplication:

This is the [[https://content.codecademy.com/courses/deeplearning-with-tensorflow/deep-learning-math/matrix_interactive/index.html][most complicated]].

*** Transpose:

[[./transpose.webp]]

This is all of the matrix algebra we need to proceed with the rest of our deep learning investigation! These concepts are the fundamental building blocks of why deep learning models are so powerful. When we are training our models, we are performing operations on tensors. This data is analyzed, manipulated, and shaped by the matrix algebra we have quickly gone over.

** Neural Networks [[https://content.codecademy.com/courses/deeplearning-with-tensorflow/deep-learning-math/applet_1.html][Concept Overview]]
Let's take a look at the journey our /inputs/ take inside of a neural network! By an input, we mean a data point from our dataset. Our input can have many different features, so in our /input layer,/ each node represents a different input feature. For example, if we were working with a dataset of different types of food, some of our features might be size, shape, nutrition, etc., where the value for each of these features would be held in an input node.

Besides an input layer, our neural network has two other different types of layers:

    /Hidden layers/ are layers that come between the input layer and the output layer. They introduce complexity into our neural network and help with the learning process. You can have as many hidden layers as you want in a neural network (including zero of them).

    The /output layer/ is the final layer in our neural network. It produces the final result, so every neural network must have only one output layer.

Each layer in a neural network contains nodes. Nodes between each layer are connected by /weights/. These are the learning parameters of our neural network, determining the strength of the connection between each linked node.

The weighted sum between nodes and weights is calculated between each layer. For example, from our input layer, we take the weighted sum of the inputs and our weights with the following equation:

$$
weighted\ sum = (inputs \cdot weight\ transpose) + bias
$$

We then apply an activation function to it.

$$
Activation(weighted\ sum)
$$

The two formulas we have gone over take all the inputs through one layer of a neural network. Aside from the activation function, all of the transformations we have done so far are linear. Activation functions /introduce nonlinearity/ in our learning model, creating more complexity during the learning process.

This is what makes activation functions important. A neural network with many hidden layers but no activation functions would just be a series of succesive layers that would be no more effective or accurate than simple linear regression.

An activation function decides what is fired to the next neuron based on its calculation for the weighted sums. Various types of activation functions can be applied at each layer. The most popular one for hidden layers is /ReLU./

[[./ReLU.png]]

Others commonly used, often for the output layer, are /sigmoid/ and /softmax/. You will learn more about these functions as you use them later in this course.

[[./sigmoid.png]]

In the diagram, we see a basic neural network with no hidden layers. Use your mouse to hover over each section of the image to get a feel for how each step of the neural network works. Each part of the diagram contains a description to indicate its role in a neural network.

(Note: on some browsers, you may need to scroll down to see the descriptions)

** The [[https://content.codecademy.com/courses/deeplearning-with-tensorflow/deep-learning-math/applet_2_new.html][Math Behind]] the Journey
Let's bring all of these concepts together and see how they function in a neural network with one hidden layer. As you scroll over each section, you will see the inputs/weights/calculations associated with it and see how inputs get from the starting point and make their way to the end!

The process we have been going through is known as /forward propagation./ Inputs are moved forward from the input layer through the hidden layer(s) until they reach the output layer.

In the applet, you can scroll over each part of the neural network and observe the mathematics behind the diagram.

When you scroll over the input section, you should see how the input is represented as a vector. Scrolling over the weights, we see how each set of weights (blue and yellow) is represented as a vector. When brought together, they make up the ~weights_matrix~ and ~weights_matrix_transpose~.

When scrolling through the hidden nodes sections, you will notice that there are two parts. In the first step, we take the weighted sum of our data using the ~weights_matrix_transpose~. From this, we end up with a vector and apply our ReLU activation function to it.

This takes us to our teal weights. These are represented as a vector. The ~weights_teal_transpose~ turns our /row vector/ into a /column vector/. Then we take another weighted sum in our output layer, this time between our ~hidden_nodes~ and our ~weights_teal_transpose~. Following this, we have a sigmoid activation function, which gives us our output.

Feel free to open this applet in a separate window for a larger viewing screen.

We now understand the adventure our data takes on one journey through our neural network. We are not quite finished yet, though. Letâ€™s keep exploring!

** Loss Functions
We have seen how we get to an output! Now, what do we do with it? When a value is outputted, we calculate its error using a loss function. Our predicted values are compared with the actual values within the training data. There are two commonly used loss calculation formulas:

    - Mean squared error, which is most likely familiar to you if you have come across linear regression. This gif below show how mean squared error is [[https://content.codecademy.com/courses/deeplearning-with-tensorflow/deep-learning-math/Loss.gif][calculated]] for a /line of best fit/ in /linear regression./

    - Cross-entropy loss, which is used for classification learning models rather than regression.

You will learn more about this as you use loss functions in your deep learning models.

The [[https://content.codecademy.com/programs/data-science-path/line-fitter/line-fitter.html][interactive visualization]] in the browser lets you try to find the line of best fit for a random set of data points:

    - The slider on the left controls the ~m~ (slope)

    - The slider on the right controls the ~b~ (intercept)

    - You can see the *total squared error* on the right side of the visualization. To get the line of best fit, we want this loss to be as small as possible.

To check if you got the best line, check the "Plot Best-Fit" box.

Randomize a new set of points and try to fit a new line by entering the number of points you want (try 8!) in the textbox and pressing Randomize Points.

*Note:*
This visualization works best with fewer than 50 points but will continue to work up to 1000 points. Attempting to add more than 1000 points may result in significant lag or the applet crashing.

Play around with the interactive applet, and notice what method you use to minimize loss:

    - Do you first get the slope to where it produces lowest loss, and then move the intercept to where it produces lowest loss?

    - Do you create a rough idea in your mind where the line should be first, and then enter the parameters to match that image?
