
* Perceptron Logic Gates
In this project, we will use perceptrons to model the fundamental building blocks of computers -logic gates.

[[./logic_gates.png]]

For example, the table below show the results of a AND gate. Given two inputs, an AND gate will output a 1 only if both inputs are a 1:

| Input 1 | Input 2 | Output |
|       0 |       0 |      0 |
|       0 |       1 |      0 |
|       1 |       0 |      0 |
|       1 |       1 |      1 |

We'll discuss how an AND gate can be thought of as linearly separable data and train a perceptron to perform AND.

We'll also investigate an XOR gate - a gate that outputs a 1 only if one of the inputs is a 1:

| Input 1 | Input 2 | Output |
|       0 |       0 |      0 |
|       0 |       1 |      1 |
|       1 |       0 |      1 |
|       1 |       1 |      0 |

We'll think about why an XOR gate isn't linearly separable and show how a perceptron fails to learn XOR.

** Creating and visualizing AND Data

*** Task 1
To begin, let's think of an AND gate as a data set of four points. The four points should be the four possible inputs to the AND gate. For example, the first point in the dataset should be ~[0, 0]~.

Create a variable named ~data~ that is a list that contains the four posible inputs to an AND gate.

*** Task 2
Each data point should have a label associated with it. The label will be the result of the AND gate given the input.

Create a variable named ~labels~. This should be a list where each label corresponds to a point in ~data~. For example, if the last item in ~data~ is ~[1, 1]~, the last label should be ~1~.

*** Task 3
Let's plot these four points on a graph.

Call ~plt.scatter()~ using three parameters:

    - The first parameter should be a list of the $x$ values of each point. You can get this using list comprehension - ~[point[0] for point in data]~.

    - The second parameter should be a list of the $y$ values of each point. You can get this using list comprehension - ~[point[1] for point in data]~.

    - The third parameter should be ~c = labels~. This will make the points with label 1 a different color than points with label 0.

After calling ~plt.scatter()~ call ~plt.show()~. As you continue to write your code, make sure ~plt.show()~ is always the last line of code.

Why is this data linearly separable?

The data is linearly separable because you can draw a straight line that completely separates the points of each class.

** Building the perceptron

*** Task 4
Now that we have our data, let's build a perceptron to learn AND. Create a ~Perceptron~ object named ~classifier~. For now, set the parameter ~max_iter~ to 40, and set ~random_state~ to 22.

~max_iter~ set the number of times the perceptron loops through the training data. The default is ~1000~, so we're cutting the training pretty short! Let's see if our algorithm learns AND, even with very little training.

*** Task 5
We'll now train the model. Call the ~.fit()~ method using ~data~ and ~labels~ as parameters.

*** Task 6
Let's see if the algorithm learned AND. Call ~classifier~'s ~.score()~ method using ~data~ and ~labels~ as parameters. Print the results. This will print the accuracy of the model on the data points.

Note that it is pretty unusual to train and test on the same dataset. In this case, since there are only four possible inputs to AND, we're stuck training on every possible input and testing on those same points.

*** Task 7
Your perceptron should have 100% accuracy! You just taught it an AND gate!

Let's change the labels so your data now represents an XOR gate. The label should be a 1 only if one of the inputs is a 1. What is the accuracy of the perceptron now? Is the data linearly separable?

*** Task 8
Try changing the data to represent an OR gate. The label of a point should be a ~1~ if /any/ of the input values are a ~1~.

Before running your code, predict what will happen. Is the data linearly separable? What do you expect the accuracy of the perceptron to be?

The data is linearly separable because you can draw a straight line that completely separates the points of each class.

** Script.py

#+begin_src python :results output
  import seaborn as sns
  from sklearn.linear_model import Perceptron
  import matplotlib.pyplot as plt
  import numpy as np
  from itertools import product

  # Inputs to AND
  data = [[0, 0], [0, 1], [1, 0], [1, 1]]

  # Labels for AND
  labels = [0, 1, 1, 1]

  plt.scatter([point[0] for point in data], [point[1] for point in data], c=labels)

  plt.show()

  classifier = Perceptron(max_iter=40, random_state=22)

  classifier.fit(data, labels)
  print(classifier.score(data, labels))

#+end_src

#+RESULTS:
: 1.0
