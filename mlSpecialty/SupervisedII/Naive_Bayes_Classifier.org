
* Introduction to Bayes' Theorem
In this lesson, we'll learn about Bayes' Theorem. Bayes' Theorem is the basis of a branch of statistics called /Bayesian Statistics/, where we take prior knowledge into account before calculating new probabilities.

This allows us to find narrow solutions from a huge universe of possibilities. British mathematician Alan Turing used it to crack the German Enigma code during WWII. And now it is used in:

    - Machine Learning

    - Statistical Modeling

    - A/B Testing

    - Robotics

By the end of this lesson, you'll be able to solve simple problems involving prior knowledge.

Before we learn this theorem, we'll need to review independence and conditional probability.

[[./bayes_theorem.png]]

* Independent Events
The ability to determine whether two events are /independent/ is an important skill for statistics.

If two events are *independent*, then the occurrence of one event does not affect the probability of the other event. Here are some examples of independent events:

    - I wear a blue shirt; my coworker wears a blue shirt

    - I take the subway to work; I eat sushi for lunch

    - The NY Giants win their football game; the NY Rangers win their hockey game

If two events are *dependent* , then when one event occurs, the probability of the other event occurring changes in a predictable way.

Here are some examples of dependent events:

    - It rains on Tuesday; I carry an umbrella on Tuesday

    - I eat spaghetti; I have a red stain on my shirt

    - I wear sunglasses; I go to the beach

** Task 1
A certain family plans to have three children. Is the event that the couple's third child is a girl independent of the event that the couple's first two children are girls?

Save your answer('~independent~' or '~not independent~') to the variable ~third_child~.

* Conditional Probability
/Conditional Probability/ is the probability that two events happen. It's easiest to calculate  conditional probability when the two events are independent.

*Note*:
For the rest of this lesson, we'll be using the statistical convention that the probability of an event is written as ~P(event)~.

If the probability of event $A$ is $P(A)$ and the probability of event $B$ is $P(B)$ and the two events are independent, then the probability of both events occurring id the product of the probabilities:

$$
P(A \cap B) = P(A) \times P(B)
$$

The simbol \cap just means "and", so P(A \cap B) means the probability that both A and B happen.

For instance, suppose we are rolling a pair of dice, and want to know the probability of rolling two sixes.

[[./two_sixes.png]]

Each die has six sides, so the probability of rolling a six is 1/6. Each die is independent (i.e., rolling one six does not increase or decrease our chance of rolling a sencond six), so:

$$
P(6 \cap 6) = P(6) \times P(6) = \frac{1}{6} \times \frac{1}{6} = \frac{1}{36}
$$

** Task 1
This week, there is a 30% probability that it will rain on any given day. At a certain high school, gym class is held on three days out of the five day school week.

On a school day, what is the probability that it is raining and the strudents have gym class?

Save your answer to the variable ~p_rain_and_gym~.

#+begin_src python :results output
  import numpy as np

  p_rain_and_gym = .3 * (3/5)

  print(p_rain_and_gym)

#+end_src

#+RESULTS:
: 0.18

* Testing for a rare disease
Suppose you are a doctor and you need to test if a patient has a certain rare disease. The test is very accurate: it's correct 99% of the time. The disease is very rare: only 1 in 100,000 patients have it.

You administer the test and it comes back positive, so your patient must have the disease, right?

Not necessarily. If we just consider the test, there is only a 1% chance that it is wrong, but we actually have more information: we know how rare the disease is.

Given that the test came back positive, there are two possibilities:

    1. The patient had the disease, and the test correctly diagnosed the disease.

    2. The patient didn't have the disease and the test incorrectly diagnosed that they had the disease.

** Task 1
What is the probability that the patient had the disease *and* the test correctly diagnosed the disease?

Save your answer to the variable ~p_disease_and_correct~.

*Hint*
The disease is rare, so the probability that the patient had the disease is 1 out of 100,000:

$$
P(disease) = \frac{1}{100000}
$$

The test is only wrong 1% of the time, so it is correct 99% of the time:

$$
P(test \ is \ correct) = 0.99
$$

** Task 2
What is the probability that the patient /does not/ have the disease and the test incorrectly diagnosed the disease?

Save your answer to the variable ~p_no_disease_and_incorrect~.

*Hint*
The disease is rare, so the probability that the patient does not have the disease is 99,999 out of 100,000:

$$
P(disease) = \frac{99999}{100000}
$$

The test is only wrong 1% of the time:

$$
P(test\ is\ correct) = 0.01
$$

** Script.py

#+begin_src python :results output
  import numpy as np

  p_disease_and_correct = (1.0 / 100000) * 0.99

  print(p_disease_and_correct)

  p_no_disease_and_incorrect = (99999.0 / 100000) * 0.01

  print(p_no_disease_and_incorrect)

#+end_src

#+RESULTS:
: 9.9e-06
: 0.0099999

* Bayes' Theorem
In the previous exercise, we determine two probabilities:

    1. The patient had the disease, and the test correctly diagnosed the disease \approx 0.000001

    2. The patient didn't have the disease and the test incorrectly diagnosed that they had the disease \approx 0.01

Both events are rare, but we can see that it was about 1000 times more likely that the test was incorrect than that the patient had this rare disease.

We're able to come to this conclusion because we had more information than just the accuracy of the test; we alse knew the prevalence of this disease.

In statistics, if we have two events (A and B), we write the probability that event A will happen, given that event B already happened as $P(A \mid B)$. In our example, we want to find $P(rare\ disease \mid positive\ result)$. In other words, we want to find the probability that the patient has the disease /given/ the test came back positive.

We can calculate $P(A \mid B)$ using Bayes' Theorem, which states:

$$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
$$

So in this case we'd say:

$$
P(rare\ disease \mid positive\ result) = \frac{P(positive\ result \mid rare\ disease) \cdot P(rare\ disease)}{P(positive\ result)}
$$

It is important to note that on the right side of the equation, we have the term $P(B \mid A)$. This is the probability that event B will happen given that event A has already happened. This is very different from $P(A \mid B)$, which is the probability we are trying to solve for. The order matters!

** Task 1
Calculate $P(positive\ result \mid rare\ disease)$, or the probability of a positive test result, given that a patient really has this rare disease.

Save your answer (as a decimal) to ~p_positive_given_disease~.

*Hint*
The test is 99% accurate, given the fact that the patient has the disease, we know that there is a 99% probability that the test will return a positive result.

This is exactly $P(positive\ result \mid rare\ disease)$.

** Task 2
What is $P(rare\ disease)$, the probability that a randomly selected patient has the rare disease?

Save your answer to ~p_disease~.

*Hint*
The disease is very rare. Only 1 in 100,000 people have it.

** Task 3
We now need to compute the denominator; we need to find $P(positive\ result)$:

    - The patient had the disease, *and* the test correctly diagnosed the disease.

    - The patient didn't have the disease *and* the test incorrectly diagnosed that they had the disease.

Using these two probabilities, calculate the total probability that a randomly selected patient receives a positive test result, $P(positive\ result)$.

Save your answer to the variable ~p_positive~.

*Hint*
The probability that the patient had the disease, *and* the test correctly diagnosed the disease is:

#+begin_src python
1.0 / 100000.0 * 0.99
#+end_src

The probability that the patient didn't have the disease *and* the test incorrectly diagnosed that they had the disease is:

#+begin_src python
99999.0 / 100000 * 0.01
#+end_src

The probability of either event A or event B happening is given by:

$$
P(A\ or\ B) = P(A) + P(B)
$$

** Task 4
Substitute all three of these values into Baye's Theorem and calculate $P(rare\ disease \mid positive\ result)$.

Save your result as ~p_disease_given_positive~.

** Script.py

#+begin_src python :results output
  import numpy as np

  p_positive_given_disease = 0.99

  p_disease = 1 / 100000

  p_positive = (1.0 / 100000.0 * 0.99) + (99999.0 / 100000 * 0.01)

  p_disease_given_positive = ((p_positive_given_disease * p_disease)) / p_positive

  print(p_disease_given_positive)

#+end_src

#+RESULTS:
: 0.0009890307498651321

* Spam Filters
Let's explore a different example. Email spam filters use Baye's Theorem to determine if certain words indicate that an email is spam.

Let's take a word that often appears in spam: "enhancement".

With just 3 facts, we can make some preliminary steps towards a good spam filter:

    1. "enhancement" appears  in just 0.1% of non-spam emails

    2. "enhancement" appears in 5% of spam emails

    3. Spam emails make up about 20% of total emails

Given that an email contains "enhancement" what is the probability that the email is spam?

** Task 1
In this example, we are dealing with two probabilities:

    - ~P(enhancement)~ -the probability that the word "enhancement" appears in an email.

    - ~P(spam)~ -the probability that an email is spam.

Using Bayes' Theorem to answer our question means that we want to calculate $P(A \mid B)$.

But what are A and B referring to in this case?

Save the string 'spam' to the variable ~a~.

Save the string 'enhancement' to the variable ~b~.

** Task 2
What is P(spam)?

Save your answer to ~p_spam~.

** Task 3
What is $P(enhancement \mid spam)$?

Save your answer to ~p_enhancement_given_spam~.

** Task 4
We want to know the overall probability that any email (spam or no spam) contains "enhancement".

Because we know the probability of "enhancement" ocurring in both spam (0.05) and non-spam (0.001) emails, we can use a /weighted average/ to calculate the probability of "enhancement" occurring in an email:

$$
P(enhancement) = P(enhancement \mid spam)×P(spam)+P(enhacement \mid not\  spam)×P(not\ spam)
$$

Save your answer to ~p_enhancement~.

** Task 5
Now that we know:

    - $P(spam)$

    - $P(enhancement \mid spam)$

    - $P(enhancement)$

We can plug this into Bayes' Theorem:

$$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
$$

Save your answer as ~p_spam_enhancement~.

** Task 6
Print ~p_spam_enhancement~. This is the probability that an email is spam given that it contains the word "enhancement".

Should we block all emails that contain "enhancement"?

How much non-spam email would we block?

** Script.py

#+begin_src python :results output
  import numpy as np

  p_spam = 0.2

  p_enhancement_given_spam = 0.05

  p_enhancement = 0.05 * 0.2 + 0.001 * (1 - 0.2)

  p_spam_enhancement = (p_enhancement_given_spam *  p_spam) / p_enhancement

  print(p_spam_enhancement)

#+end_src

#+RESULTS:
: 0.9259259259259259

* Review
In this course, we learned several new definitions:

    - Two events are /independent/ if the occurrence of one event does not affect the probability of the second event

    - If two events are independent then:

$$
 P(A \cap B) = P(A) \times P(B)
 $$

    - Bayes' Theorem is the following:

$$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A) }{P(B)}
$$

* The Naive Bayes Classifier
A Naive Bayes classifier is a supervised machine learning algorithm that leverages Bayes' Theorem to make predictions and classifications. Recall Bayes' Theorem:

$$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
$$

This equation is finding the probability of A given B. This can be turned into a classifier if we replace B with a /data point/ and A with a /class./ For example, let's say we're trying to classify an email as either ~spam~ or ~not spam~. We could calculate $P(spam \mid email)$ and $P(not\ spam \mid email)$. /*Whichever probability is higher will be the classifier's prediction*./ Naive Bayes classifiers are often used for text classification.

So why is this a supervised machine learning algorithm? In order to compute the probabilities used in Bayes' Theorem, we need previous data points. For example, in the spam example, we'll need to compute $P(spam)$. This can be found by looking at a tagged dataset of emails and finding the ratio of spam to non-spam emails.

With the naive Bayes classifier, we are finding the probability that a data point d is a member of a class C using Bayes’ theorem: P(C|d) = P(C)*P(d|C) / P(d) . We can determine P(C) because this model is supervised and we have data. But what is P(d) ? How do we find the probability of a data point?

* Investigate the Data
In this lesson, we are going to create a Naive Bayes classifier that can predict whether a review for a product is positive or negative. This type of classifier could be extremely helpful for a company that is curious about the public reaction to a new product. Rather than reading thousands of reviews or tweets about the product, you could feed those documents into the Naive Bayes classifier and instantly find out how many are positive and how many are negative.

The dataset we will be using for this lesson contains Amazon product reviews for baby products. The original dataset contained many different features including the reviewer's name, the data the review was made, and the overall score. We've removed many of those features; the only features that we're interested in are the text of the review and whether the review was "positive" or "negative". We labeled all reviews with a score less than 4 as a negative review.

Note that in the next two lessons, we've only imported a small percentage of the data to help the code run faster. We'll import the full dataset later when we put everything together!

** Task 1
Let's look at the data given to us. Print ~pos_list[0]~. Would you classify this review as positive or negative?

** Task 2
Take a look at the first review in ~neg_list[0]~ as well. Does that one look negative?

** Task 3
We've also created a [[https://docs.python.org/3/library/collections.html#collections.Counter][Counter object]] for all the positive reviews and one for all of the negative reviews. These counters are like Python dictionaries -you could find the number of times the word "baby" was used in the positive reviews by printing ~pos_counter['baby']~.

Print the number of times the word "crib" was used in the positive and negative reviews. In which set was it used more often?

** Script.py

#+begin_src python
  from reviews import neg_list, pos_list, neg_counter, pos_counter

  print(pos_list[0])
  print(neg_list[0])

  print(pos_counter['crib'])
  print(neg_counter['crib'])

#+end_src

#+RESULTS:
: Traceback (most recent call last):
:   File "<stdin>", line 2, in <module>
: _pickle.UnpicklingError: invalid load key, '\xef'.
: [ Babel evaluation exited with code 1 ]

* Bayes Theorem I
For the rest of this lesson, we're going to write a classifier that can predict whether the review "This crib was amazing" is a positive or negative review. We want to compute both $P(positive \mid review)$ and $P(negative \mid review)$ and find which probability is larger. To do this, we'll be using Bayes' Theorem. Let's look at Bayes' Theorem for $P(positive \mid review)$.

$$
P(positive \mid review) = \frac{P(review \mid positive) \cdot P(positive)}{P(review)}
$$

The first part of Bayes' Theorem that we are going to tackle is $P(positive)$. This is the probability that any review is positive. To find this, we need to look at all of our reviews in our dataset -both positive and negative- and find the percentage of reviews that are positive.

We've bolded the part of Bayes' Theorem we're working on.

$$
P(positive \mid review) = \frac{P(review \mid positive) \cdot \textbf{P(positive)}}{P(review)}
$$

** Task 1
Find the total number of positive reviews by finding the length of ~pos_list~. Do the same for ~neg_list~.

Add those two numbers together and save the sum in a variable called ~total_reviews~.

** Task 2
Create variables named ~percent_pos~ and ~percent_neg~. ~percent_pos~ should be the number of positive reviews divided by ~total_reviews~. Do the same for ~percent_neg~.

** Task 3
Print ~percent_pos~ and ~percent_neg~. They should add up to 1!

** Script.py

#+begin_src python
  from reviews import neg_list, pos_list, neg_counter, pos_counter

  number_positives_reviews = len(pos_list)
  number_negatives_reviews = len(neg_list)
  total_reviews = number_positives_reviews + number_negatives_reviews

  percent_pos = number_positives_reviews / total_reviews

  percent_neg = number_negatives_reviews / total_reviews

  print(percent_pos)
  print(percent_neg)
#+end_src

* Bayes Theorem II
Let's continue to try to classify the review "This crib was amazing".

The second part of Bayes' Theorem is a bit more extensive. We now want to compute $P(review \mid positive)$.

$$
P(positive \mid review) = \frac{\textbf{P(review} \mid \textbf{positive}) \cdot P(positive)}{P(review)}
$$

In other words, if we assume that the review is positive, what is the probability that the words "This", "crib", "was", and "amazing" are the only words in the review?

To find this, we have to assume that each word is conditionally independent. This means hat one word appearing doesn't affect the probability of another word form showing up. This is a pretty big assumption!

We now have this equation. You can scroll to the right to see the full equation.

$$
\begin{array}{c}
P("This\ crib\ was\ amazing" \mid positive) = P("This" \mid positive) \cdot \\
P("crib" \mid positive) \cdot \\
P("was" \mid positive) \cdot \\
P("amazing" \mid positive)
\end{array}{c}
$$

Let's break this down even further by looking at one of these terms. $P("crib" \mid positive)$ is the probability that the word "crib" appears in a positive review. To find this, we need to count up the total number of times "crib" appeared in our dataset of positive reviews. If we take that number and divide it by the total number of words in our positive review dataset, we will end up with the probability of "crib" appearing in a positive review.

$$
P("crib" \mid positive) = \frac{Number\ of\ "crib"\ in\ positive}{Number\ of\ words\ in\ positive}
$$

If we do this for every word in our review and multiply the results together, we have $P(review \mid positive)$.

** Task 1
Let's first find the total number of words in all positive reviews and store that number in a variable named ~total_pos~.

To do this, we can use the built-in Python ~sum()~ function. ~sum()~ takes a list as a parameter. The list that you want to sum is the ~values~ of the dictionary ~pos_counter~, which you can get by using ~pos_counter.values()~.

Do the same for ~total_neg~.

#+begin_src python
  total_pos = sum(pos_counter.values())
  total_neg = sum(neg_counter.values())
#+end_src

** Task 2
Create two variables named ~pos_probability~ and ~neg_probability~. Each of these variables should start at 1. These are the variables we are going to use to keep track of the probabilities.

** Task 3
Create a list of the words in ~review~ and store it in a variable named ~review_words~. You can do this by using Python's ~.split()~ function.

For example if the string ~test~ contained "Hello there", then ~test.split()~ would return ["Hello", "there"].

** Task 4
Loop through every ~word~ in review_words. Find the number of times ~word~ appears in ~pos_counter~ and ~neg_counter~. Store those values in variables named ~word_in_pos~ and ~word_in_neg~.

In the next steps, we'll use this variable inside the for loop to do a series of multiplications.

** Task 5
Inside the for loop, set ~pos_probability~ to be ~pos_probability~ multiplied by ~word_in_pos / total_pos~.

For example, when ~word~ is ~"crib"~, you’re calculating the following:

$$
P("crib" \mid positive) = \frac{Number\ of\ "crib"\ in\ positive}{Number\ of\ words\ in\ positive}
$$

** Script.py

#+begin_src python
  from reviews import neg_counter, pos_counter

  review = "This crib was amazing"

  percent_pos = 0.5
  percent_neg = 0.5

  total_pos = sum(pos_counter.values())
  total_neg = sum(neg_counter.values())

  pos_probability = 1
  neg_probability = 1

  review_words = review.split()

  for word in review_words:
      word_in_pos = pos_counter[word]
      word_in_neg = neg_counter[word]
      pos_probability = pos_probability * word_in_pos / total_pos
      neg_probability = neg_probability * word_in_neg / total_neg

#+end_src

* Smoothing
In the last exercise, one of the probabilities that we computed was the following:

$$
P("crib" \mid positive) = \frac{Number\ of\ "crib"\ in\ positive}{Number\ of\ words\ in\ positive}
$$

But what happens if "crib" was never in any of the positive reviews in our dataset? This fraction would then be 0, and since everything is multiplied together, the entire probability $P(review \mid positive)$ would become 0.

This is especially problematic if there are typos in the review we are trying to classify. If the unclassified review has a typo in it, it is very unlikely that that same exact typo will be in the dataset, and the entire probability will be 0. To solve this problem, we will use a technique called /smoothing./

In this case, we smooth by adding 1 to the numerator of each probability and ~N~ to the denominator of each probability. ~N~ is the number of unique words in oru review dataset.

For example, $P("crib" \mid positive)$ goes from this:

$$
P("crib" \mid positive) = \frac{Number\ of\ "crib"\ in\ positive}{Number\ of\ words\ in\ positive}
$$

To this:
$$
P("crib" \mid positive) = \frac{Number\ of\ "crib"\ in\ positive + 1}{Number\ of\ words\ in\ positive + N}
$$

** Task 1
Let's demonstrate how these probabilities break if there's a word that never appears in the given datasets.

Change review to ~"This cribb was amazing"~. Notice the second ~b~ in cribb.

** Task 2
Inside your ~for~ loop, when you multiply ~pos_probability~ and ~neg_probability~ by a fraction, add ~1~ to the numerator.

Make sure to include parentheses around the numerator!

** Task 3
In the denominator of those fractions, add the number of unique words in the appropriate dataset.

For the positive probability, this should be the length of ~pos_counter~ which can be found using ~len()~.

Again, make sure to put parentheses around your denominator so the division happens after the addition!

Did smoothing fix the problem?


** Script.py

#+begin_src python
  from reviews import neg_counter, pos_counter

  review = "This crib was amazing"

  percent_pos = 0.5
  percent_neg = 0.5

  total_pos = sum(pos_counter.values())
  total_neg = sum(neg_counter.values())

  pos_probability = 1
  neg_probability = 1

  review_words = review.split()

  for word in review_words:
      word_in_pos = pos_counter[word]
      word_in_neg = neg_counter[word]

      pos_probability *= word_in_pos / total_pos
      neg_probability *= word_in_neg / total_neg

  print(pos_probability)
  print(neg_probability)
#+end_src

* Classify
If we look back to Bayes' Theorem, we've now completed both parts of the numerator. We now need to multiply them together.

$$
P(positive \mid review) = \frac{\bf{P(review \mid positive) \cdot P(positive)}}{P(review)}
$$

Let's now consider the denominator $P(review)$. In our small example, this is the probability that "This", "crib", "was", and "amazing" are the only words in the review. Notice that this is extremely similar to $P(review \mid positive)$. The only difference is that we don't assume that the review is positive.

However, before we start to compute the denominator, let's think about what our ultimate question is. We want to predict whether the review "This crib was amazing" is a positive or negative review. In other words, we're asking whether $P(positive \mid review)$ is greater than $P(negative \mid review)$. If we expand those two probabilities, we end up with the following equations:

$$
P(positive \mid review) = \frac{P(review \mid positive) \cdot P(positive)}{P(review}
$$

$$
P(negative \mid review) = \frac{P(review \mid negative) \cdot P(negative)}{P(review)}
$$

Notice that P(review) is in the denominator of each. That value will be the same in both cases! Since we're only interested in comparing these two probabilities, there's no reason why we need to divide them by the same value. We can completely ignore the denominator!

Let's see if our review was more likely to be positive or negative!

** Task 1
After the for loop, multiply ~pos_probability~ by ~percent_pos~ and ~neg_probability~ by ~percent_neg~. Store the two values in ~final_pos~ and ~final_neg~ and print both.

** Task 2
Compare ~final_pos~ to ~final_neg~:

    - If final_pos was greater than final_neg, print "The review is positive"

    - Otherwise print "The review is negative".

Did our Naive Bayes Classifier get it right for the review "This crib was amazing"?

** Task 3
Replace the review "This crib was amazing" with one that you think should be classified as negative. Run your program again.

Did your classifier correctly classify the new review?

** Script.py

#+begin_src python
  from reviews import neg_counter, pos_counter

  review = "This crib was amazing"

  percent_pos = 0.5
  percent_neg = 0.5

  total_pos = sum(pos_counter.values())
  total_neg = sum(neg_counter.values())

  pos_probability = 1
  neg_probability = 1

  review_words = review.split()

  for word in review_words:
    word_in_pos = pos_counter[word]
    word_in_neg = neg_counter[word]

    pos_probability *= (word_in_pos + 1) / (total_pos + len(pos_counter))
    neg_probability *= (word_in_neg + 1) / (total_neg + len(neg_counter))

  final_pos = pos_probability * percent_pos
  final_neg = neg_probability * percent_neg

  print(final_pos)
  print(final_neg)

  if final_pos > final_neg:
      print("The review is positive")
  else:
      print("The review is negative")

#+end_src
