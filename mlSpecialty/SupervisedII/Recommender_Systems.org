
* Introduction to Recommender Systems

** What are Recommender Systems?
With the growth of the internet, the number of options people have for everything from watching videos, to buying clothes, to finding a date has increased dramatically. Having potentially thousands of options, how are we to find and choose the best options for us?

Many internet companies have provided us tools to help us navigate these seemingly infinite number of choices. For example, when you visit a product page on an ecommerce site, you may have noticed a section of the page that suggests other products you might like based on things you have purchased in the past. Perhaps you have received a push notification on your mobile phone recommending a video to watch. The closer you look, the more likely you are to find all sorts of recommendations being made in the websites and mobile apps you use.

All these tools are powered by recommender systems. Recommender systems are algorithms that use data about products and user's preferences to make recommendations to users about the best options to choose from a set of options.

We will learn about the properties of recommender systems, the ways they differ from traditional supervised learning, and the different types of recommender systems.

** Recommender Systems versus Supervised Learning
As you learned about recommender systems in the previous exercise, you may be wondering how recommender systems differ from traditional supervised learning techniques you have learned about in previous sections. In general, recommender systems do often utilize machine learning techniques. So what is the difference between the two terms?

The difference is not in the technique, but in purpose. Recommender systems are built to address problems of determining the *best action* for a user to take given a set of options. Supervised learning is generally used to describe machine learning to predict *outcomes*.

Let's take the example of an E-Commerce website selling shoes. If the website built a machine learning model that was designed to determine how much each user would like each shoe, that model would be considered a recommender system. However, if the purpose of the machine learning model is built to determine how many shoes the site would sell next month, that would be considered just an application of supervised learning.

Recommender systems also differ from traditional supervised learning in the specific properties are important for measuring their performance, as we shall see in the following section.

| Recommender Systems  | Machine Learning          |
|----------------------+---------------------------|
| Suggesting shoes     | Shoes that will sell      |
| Apply for a given CC | Transsaction is faudulent |
| Users similar to you | Clustering                |

** Properties of Good Recommender Systems
When considering the performance of a recommender system, there are a couple of different dimensions we should consider:

    - /Relevance:/ When a recommender system makes a recommendation, it should be relevant to the user. More specifically, these recommendations should be ones the user would likey rate highly. For example, a female user who only looks  at and buys women's shoes on an ecommerce website should generally only receive recommendations for women's shoes. For many data practitioners, this is the most obvious and important property to optimize. However, it is critical to understand this is not the only important property of a recommender system.

    - /Novelty:/ Recommender systems should be ideally making recommendations the user has not seen before. For example, a recommender system that consistently recommends only the most popular women's shoes to a female user may be considered a poor recommender system.

    - /Serendipity:/ Good recommender systems generally make recommendations that are somewhat unexpected. These sorts of recommendations (assuming they are relevant) can often delight users. Going back to our shoe ecommerce site example, a user that often purchases running shoes might get recommended sandals that are good for running from a serendipitous recommender system.

    - /Recommendation Diversity:/ Recommender systems that recommend many different types of items are more likely to have at least one item liked by the user. Looking at our store site example, a recommender system that suggest to users not only shoes, but socks and shoe polish may be preferable to one that only recommends shoes.

    - /Technical Complexity:/ Recommender systems may often consist of many complex algorithms and parts. As a result, these systems require maintenance and some level of interpretability by technical staff. Therefore, recommender systems that are less complex and easier to understand are preferable from a cost and risk perspective.

** Types of Recommender Systems
Recommender systems can generally be classified into one of three different groups depending on the algorithmic approach they take to make recommendations:

    - /*Collaborative Filtering:*/ Collaborative filtering is a recommender system technique that makes recommendations for a target user by using ratings information from other users. The driving principle behind collaborative filtering is that users that have similar ratings for items have similar tastes.

    - /*Content-based Filtering:*/ Content-based Filtering is a recommender system technique that uses data about user preferences and attributes of item to model the likelihood a given user will like a specific item. This type of recommender systems tends to look more like the traditional machine learning models used in supervised learning.

    - /*Knoledge-Based:*/ Knowledge-based recommender systems are a class of recommender systems used when there is not a lot of data available. Rules are explicitly programmed based on user preferences and domain knowledge. While an important class of recommender systems, we will not be discussing them in detail in this module.

We're primarily going to focus on collaborative filtering in the course of this module.

[[./Types_of_recommender_systems.png]]

** Ratings Matrix: Representing User Preferences
The first step in building a recommender system is to have a mathematical representation of data relating to user's preferences. Often, the representation used is a matrix of numbers called a *ratings matrix*, where each row represents a user, each column represents an item, and the intersection of a row and a column contains the rating for an item given by a user. Ratings inside of a ratings matrix can generally be represented as either *explicit ratings* or *implicit ratings*.

Explicit ratings involve using the ratings given by users for items they have rated. Items that are not rated by users are left blank. Often they are normalized to help model performance. The major downside of this representation is explicit data may be scarce. Often, users skip rating items in an application. Often, rating data is not available at all.

On the other hand, implicit ratings do not require users to submit ratings. Instead, user events on the app or website are viewed as endorsements of an item. For example, purchasing an item on an E-commerce website could be viewed as an endorsement of an item. Any item that a user purchases could then be represented as a 1 in a ratings matrix, and anything they do not purchase can be represented as a 0. The main advantage of implicit ratings is that data is much more readily available. The major downside is the data is not as granular as that of explicit ratings, and therefore recommendations can degrade accordingly.

And sometimes an implicit rating can be converted into an explicit rating according to the data scientis's discretion. For instance, a user may have listened to one song fifty times on an audio streaming service and another one, only five times. There is relative information here as regards the user's preference on two items that can be converted to explicit ratings.

[[./explicit_implicit.png]]

** Collaborative Filtering
Previously, we introduced collaborative filtering. However collaborative filtering can be further classified into two major subclasses: *memory-based methods* (also called *neighborhood-based methods*) and *model-based methods*.

Memory-based methods work through the concept of /similarity/. Fundamentally, memory-based methods work in one of two ways:

    1. The algorithm finds similar users to the target users, and recommends items those similar users liked. This approach in known as *user-user collaborative filtering*.

    2. The algorithm finds similar items to ones the target user liked by measuring the similarity of how users rated items. This approach is known as *item-item* collaborative filtering.

In contrast, model-based methods work by building models that attempt to predict a rating for a user-item pair by using ratings as features. One particular method that is often used in practice is /matrix factorization./ This method model the user-item ratings matrix as the product of a set of users vector and product vectors. The rating of any user-item pair can then be predicted by multiplying the relevant user vector by the relevant product vector.

After creating a ratings matrix, various data transformations may be performed on the ratings matrix. These transformations are done generally to improve model performance, similar to how normalizing features in a machine learning model can help improve performance.

One such transformation is ratings normalization. Ratings normalization is a technique where the value of each rating for a given row is adjusted based on the statistical properties of that row. The primary reason this transformation is done is because different users may have different approaches to rating something. For example, some users may give a 5-star rating to any positive experience they have. Other users may be more selective, and only give 5 star ratings very rarely. Ratings normalization provides a way to control for these differences.

One of two approaches is usually used for normalizing ratings. The first, mean centering, involves subtracting the mean rating of a row from every value of the row. The other method, z-score normalization, involves using the mean and standard deviation of each row to calculate a z-score for each element of the row. Can you think of other ways to make sure that the ratings between different users and items can be meaningfully compared?

[[./will_e_like_shoe_3.png]]

** Review
We learned about recommender systems. We learned about how they compare to traditional supervised learning and what are the properties of good recommender systems. We also discussed the different types of recommender systems, as well as how we can transform data for use in recommender systems. We are now ready to learn how we can use Python to implement and build recommender systems.

* Simple Python Recommender Engine (Surprise!)
Learn how to build a recommender system using the Python library surprise!

** Introduction
The engineering and development of a recommender system requires work across data science, software engineering, infrastructure, product, and design. With so many things that need to be executed in order to produce a good recommender system, having tools that make writing and testing the code for recommender systems is critical to success.

In the specific case of machine learning engineering, one group of tools that makes development easier is open-source software libraries. These standardized and well-tested libraries that are shared across the machine learning community help increase the speed of writing code without an explicit need to develop and maintain one's own libraries. If you have worked with scikit-learn, then you are familiar with how this open-source library makes it very easy to built sophisticated machine learning models using only a few lines of code. It would be ideal to have a similar library for recommender systems. Surprise was designed to solve this problem.

** What is Surprise?
Surprise (an abbreviation of /Simple Python Recommendation System Engine/) is a  ~scikit~, a software library built as an add-on to the numerical computation library ~SciPy~. Much like how ~scikit-learn~ makes developing and testing different machine learning models easy, Surprise makes development and testing various recommender system algorithms easy. Surprise comes with several modules that make it easy to transform data, train recommender systems, and measure recommender system performance. It also comes with a solid base of documentation that makes it easy to understand and explore the library's capabilities.

** Building a simple recommender system
We're now going to use Surprise to built a basic recommender system. We will be using a classic dataset, the [[https://grouplens.org/datasets/movielens/][MovieLens Dataset]] , to built a recommender system that suggest movies to users based on the ratings they gave several movies. The MovieLens dataset is a set of 100,000 movie ratings for 9,000 movies provided by 600 users. The ratings come from the website [[https://movielens.org/][movielens.org]], a non-commercial site dedicated to giving users personalized movie recommendations. This data is very similar to the type of data your would find at large commercial streaming services.

*** Installation
To install Surprise on your computer, simply use the following pip command:

#+begin_src python
  pip install surprise
#+end_src

To test that the command worked, open up your terminal or Jupyter notebook or IDE and verify the following code runs:

#+begin_src python :results output
  import surprise
#+end_src

#+RESULTS:

*** Loading Data
Surprise comes preloaded with datasets for building and testing recommender systems, including the MovieLens dataset. To load the MovieLens dataset, run the following code:

#+begin_src python :results output
  from surprise import Dataset

  movie_data = Dataset.load_builtin('ml-100k')
#+end_src

#+RESULTS:

*** Train/Test Split of Data
Like other machine learning problems, recommender systems can be modeled as a supervised machine learning problem where the movie ratings of users are being utilized as features ot predict ratings for movie users have not watched yet. Viewing recommender systems in this way, we can split the data into training and testing sets to help validate the performance of our recommender system. The code below will split the Movielens dataset into a 75%, 25% train and test set:

#+begin_src python :results output
  import surprise
  from surprise import Dataset
  from surprise.model_selection import train_test_split

  movie_data = Dataset.load_builtin('ml-100k')

  trainset, testset = train_test_split(movie_data, test_size=.2, random_state=42)
#+end_src

#+RESULTS:

To understand what the data looks like, let's look at the ratings. To look at the ratings, we will use the ~ur~ method of ~trainset~. The method ~ur~ returns a dictionary where the keys are user ids, and the values are a list of tuples, where each tuple is in the form ~({item_id}, {rating})~. For example, to see the rating of user 590:

#+begin_src python :results output
  import surprise
  from surprise import Dataset
  from surprise.model_selection import train_test_split

  movie_data = Dataset.load_builtin('ml-100k')

  trainset, testset = train_test_split(movie_data, test_size=.2, random_state=42)

  print(trainset.ur[590])

#+end_src

#+RESULTS:
: [(699, 4.0), (114, 4.0), (339, 5.0), (769, 5.0), (230, 4.0), (212, 5.0), (21, 5.0), (33, 3.0), (94, 4.0), (74, 2.0), (22, 4.0), (471, 5.0), (8, 3.0), (305, 3.0), (645, 3.0), (558, 2.0), (166, 4.0), (129, 4.0), (100, 5.0), (232, 3.0), (391, 3.0), (204, 3.0), (508, 4.0), (79, 3.0), (83, 4.0), (426, 1.0), (6, 3.0), (58, 3.0), (1119, 5.0), (76, 5.0)]

*** Training a recommender system
Now that we have proper datasets for training and testing, we will now train a basic recommender system. Specifically, we will be training a simple user-user collaborative filter using Surprise's ~KNNBasic~ algorithm. KNNBasic is a very simple implementation of user-user /collaborative filtering./ This algorithm works by using the k-nearest neighbors (KNN) algorithm to determine the similarity of users based on how they rated movies. The average of the movie ratings from the most similar users are then used as the predicion for the target user. The recommender system is initialized and trained like so:

#+begin_src python :results output
  import surprise
  from surprise import Dataset
  from surprise.model_selection import train_test_split
  from surprise import KNNBasic

  movie_data = Dataset.load_builtin('ml-100k')

  trainset, testset = train_test_split(movie_data, test_size=.2, random_state=42)

  #print(trainset.ur[590])

  movie_recommender = KNNBasic()
  movie_recommender.fit(trainset)

#+end_src

#+RESULTS:
: Computing the msd similarity matrix...
: Done computing similarity matrix.

*** Evaluating Recommender System Performance
Finally, we will evaluate the performance of the ~KNNBasic~ recommender system. First, let's create some predictions on the test set.

#+begin_src python
  predicions = movie_recommender.test(testset)
#+end_src

The ~predictions~ are a list of ~Prediction~ objects. Each Prediction object contains a user id (~uid~), and item id representing the movie (~iid~), and actual rating given by the user for that movie (~r_ui~), an estimated rating from the recommender system (~est~) and some additional metadata about each prediction (~details~). An example of a ~Prediction~ object from our ~predictions~ can be seen below.

#+begin_src python :results output
  import surprise
  from surprise import Dataset
  from surprise.model_selection import train_test_split
  from surprise import KNNBasic

  movie_data = Dataset.load_builtin('ml-100k')

  trainset, testset = train_test_split(movie_data, test_size=.2, random_state=42)

  #print(trainset.ur[590])

  movie_recommender = KNNBasic()
  movie_recommender.fit(trainset)

  predictions = movie_recommender.test(testset)

  print(predictions[0])

#+end_src

#+RESULTS:
: Computing the msd similarity matrix...
: Done computing similarity matrix.
: user: 907        item: 143        r_ui = 5.00   est = 4.04   {'actual_k': 40, 'was_impossible': False}

This Prediction object contains a prediction for the movie 'The Sound of Music' (to see which movie each id corresponds to, take a look at this [[https://files.grouplens.org/datasets/movielens/ml-100k/u.item][movie data file]]). As seen above, the recommender system predicted a rating of about 4 stars, which is not very far from the actual prediction on 5 stars.

We can measure one aspect ot the model's performance by looking at the root-mean square error, or RMSE. The RMSE is an average measure of how far off predictions will be from their actual values. The closer the RMSE is to 0, the more accurate the model. The RMSE can also be thought of as a measure of *relevance*, or how likely the items recommended will be something the user would be interested in based on their previous behavior. It is important to note that while relevance is one important aspect of a recommender system, it is not the only important thing to optimize in a recommender system.

As seen below, Surprise makes it very easy to measure model RMSE.

#+begin_src python :results output
  import surprise
  from surprise import Dataset
  from surprise.model_selection import train_test_split
  from surprise import KNNBasic
  from surprise import accuracy

  movie_data = Dataset.load_builtin('ml-100k')

  trainset, testset = train_test_split(movie_data, test_size=.2, random_state=42)

  #print(trainset.ur[590])

  movie_recommender = KNNBasic()
  movie_recommender.fit(trainset)

  predictions = movie_recommender.test(testset)

  print(predictions[0])

  print(accuracy.rmse(predictions))

#+end_src

#+RESULTS:
: Computing the msd similarity matrix...
: Done computing similarity matrix.
: user: 907        item: 143        r_ui = 5.00   est = 4.04   {'actual_k': 40, 'was_impossible': False}
: RMSE: 0.9802
: 0.980150596704479

As seen above, the trained recommender system has an RMSE of about 0.98. We can now use this number as a bseline to see if there is a gain or loss in model performance versus other algorithms.

** Conclusion
And that's it! Wiht just 10 lines of code, Surprise allowed us to train and test a basic recommender system with ease. Surprise also has several other built-in recommender systems algorithms, including alternative forms of collaborative filtering, Singular Value Decomposition (SVD), and other algorithms that will be explored in the course. Additionally, much like ~scikit-learn~ models, each of these models have hyperparameters that can be adjusted to help further optimize the performance of recommender systems.

This article has hopefully given you a taste of the power and usefulness of Surprise. While Surprise by itself is not sufficient to build a scalable and well-performing recommender system for large-scale use, it is an excellent starting point for developing recommender systems, and a powerful addition to a machine learning practitioner's toolkit.

** Assessment
Now it's your turn! Using what you have learned, train a simple SVD recommender system using Surprise's ~SVD~ algorithm (documentation can be found [[https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD][here]]) and the provided training data. Then calculate its RMSE. Is this an improvement over the ~KNNBasic~ algorithm above? How can you tell?

#+begin_src python :results output
  import surprise
  from surprise import Dataset
  from surprise.model_selection import train_test_split
  from surprise import KNNBasic, SVD
  from surprise import accuracy

  movie_data = Dataset.load_builtin('ml-100k')

  trainset, testset = train_test_split(movie_data, test_size=.2, random_state=42)

  #print(trainset.ur[590])

  movie_recommender = KNNBasic()
  movie_recommender.fit(trainset)

  predictions = movie_recommender.test(testset)

  print(predictions[0])

  print(accuracy.rmse(predictions))

  svd_recommender = SVD()
  svd_recommender.fit(trainset)

  svd_predictions = svd_recommender.test(testset)

  accuracy.rmse(svd_predictions)

#+end_src

#+RESULTS:
: Computing the msd similarity matrix...
: Done computing similarity matrix.
: user: 907        item: 143        r_ui = 5.00   est = 4.04   {'actual_k': 40, 'was_impossible': False}
: RMSE: 0.9802
: 0.980150596704479
: RMSE: 0.9394
