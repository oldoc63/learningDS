
* Why Regularize?
Regularization plays a very important role in real world implementations of machine learning models. It is a statistical technique that /minimizes overfitting/ and is executed during the /model fitting step./ It is also an embedded feature selection method because it is implemented while the parameters of the model are being calculated.

Machine learning algorithms are rarely deployed to production without using some form of regularization. The reason for this is as follows: In practice, every model has to deal with the question o how well it can generalize from known to unknown data. We can train, test and tune models on known data and make them as accurate as possible. However, in deploying models, we are applying them on /new data./ Regularization makes sure that our model is still accurate.

In this lesson, we are going to learn how regularization minimizes overfitting and how ot use it as a feature selection method. Along the way, we are also going to learn two concepts that are very relevant to regularization and important just as standalone topics within machine learning, namely, the /bias-variance tradeoff/ and /hyperparameter tuning./ But first, we're going to answer the question: what is overfitting?

** Task 1
In the image to the right, we have three curves generated by different model fits to the same data. Can you identify which of these might be:

    underfitting the data?
    overfitting the data?

Fill in the number corresponding to the plots that best matches these on the code editor.

[[./overfitting_underfitting.png]]

* What is overfitting?
It might have been pretty apparent which of the three curves in the previous exercise overfit the set of points. However, often we have too many features to be able to visually assess if our model is overfitting our data or not. And so we need to qualify what we mean by overfitting exactly.

If a model is able to represent a particular set of data points effectively buy is not able to fit new data well, it is overfitting the data. Such a model has one or more of the followin attributes:

    - It fits the training data well but performs significantly worse on test data

    - It typically has more parameters than necessary, i.e., it has /high model complexity/

    - It might be fitting for features that are multi-collinear (i.e., features that are highly negatively or positively correlated)

    - It might be fitting the noise in the data and likely mistaking the noise for features

In practice, we often catch overfitting by comparing the model performance on the training data versus test data. For instance if the R-squared score is high for training data but the model performs pooly on test data, it's a strong indicator of overfitting. We're going to look at one such example now. A dataset has been loaded for you from the [[https://archive.ics.uci.edu/dataset/320/student+performance][UCI Machine Learning Repository]] that describes the performance in mathematics of students form two Portuguese schools. We're going to implement a multiple linear regression model to predict the final grade of the students based on a number of features in the dataset.

** Task 1
We've implemented some code to print the first five rows, set the target variable as y and the predictor variables as the matrix X. Calculate the number of features in the dataset, save it as num_features and press Run. (Note that the number of features is equal to the number of columns in X).

*Hint*
Set ~num_features~ equal to ~len(X.columns)~

** Task 2
We're about to fit the data to a model with 42 parameters! (Intercept plus 41 coefficients, one for each predictor variable). This might already make us apprehensive of the model potentially overfitting the data because:

    - the model likely has a high degree of complexity

    - we haven't checked for collinearity between the features

In order to assess whether we are overfitting, we're going to fit the model. In the workspace, you'll see that we've provided code to split the data into a training and test set, fit a regression on all 41 features, and calculated the mean squared error (MSE) on the training data.

In the section below the comment # 2.Testing Error, fill in the code to calculate the MSE for the test set. Save the result as ~MSE_test~ (currently set to None).

** Task 3
We see that the error on the test data is almost double the error with the training data! This is definitely an indicator of overfitting. We're now going to check the coefficients evaluated by the model to see if we can catch this.

Uncomment the plotting code below the comment #3:  Plotting the Coefficients and press Run to plot values of the coefficients produced.

We see that there are a few negatively correlated coefficients here (coefficients with roughly similar values mirrored about the axis), which is also a sign of overfitting.


** Script.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt

  df = pd.read_csv("student_math.csv")
  print(df.head())

  # setting target and predictor variables
  y = df['Final_Grade']
  X = df.drop(columns = ['Final_Grade'])

  # 1. Number of features
  num_features = len(X.columns)
  print("Number of features: ", num_features)

  # Performing a Train-Test split
  from sklearn.model_selection import train_test_split
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

  # Fitting a Linear Regression Model
  from sklearn.linear_model import LinearRegression
  model = LinearRegression()
  model.fit(X_train, y_train)

  # Training Error
  pred_train = model.predict(X_train)
  MSE_train = np.mean((pred_train - y_train)**2)
  print("Training Error: ", MSE_train)

  # 2. Testing Error
  pred_test = model.predict(X_test)
  MSE_test = np.mean((pred_test - y_test)**2)
  print("Testing Error: ", MSE_test)

  # Calculating the regression coefficients
  predictors = X.columns
  coef = pd.Series(model.coef_,predictors).sort_values()

  # 3. Plotting the Coefficients
  plt.figure(figsize = (15,10))
  coef.plot(kind='bar', fontsize = 20)
  plt.title("Regression Coefficients", fontsize = 30)
  plt.show()

#+end_src

#+RESULTS:
#+begin_example
   age  Medu  Fedu  ...  higher_yes  internet_yes  romantic_yes
0   18     4     4  ...           1             0             0
1   17     1     1  ...           1             1             0
2   15     1     1  ...           1             1             0
3   15     4     2  ...           1             1             1
4   16     3     3  ...           1             0             0

[5 rows x 42 columns]
Number of features:  41
Training Error:  2.6268841802196254
Testing Error:  4.987346573982337
#+end_example

* The Loss Function
Let's revisit how the coefficients (or parameters) of a linear regression model are obtained. Suppose we want to fit a linear regression with two features, $x_1$ and $x_2$:

$$
y = b_0 + b_1x_1 + b_2x_2 + e
$$

We solve for the intercept b0 and coefficients b1 and b2 by minimizing the mean squared error. This is also referred to as the *loss function* (or alternately, as the /objective function/ or /cost function/) in machine learning:

$$
Loss\ function = \frac{1}{n} \sum_{i=1}^n (y_i - b_0 - b_1x_{1i} - b_2x_{2i})^2
$$

The loss function is minimized by the method of Ordinary Least Squares (OLS) and for larger datasets, the gradient descent algorithm is used to arrive at the minimum. As we have seen already however, the "best fit" as obtained by minimizing the loss function might not be the fit that generalizes well to new data. Regularization modifies the loss function in a way that might help with this.

Before going into more detail, we're going to plot the loss function as a function of the coefficients b1 and b2. For this, we're going to generate a dataset for two-feature linear regression using the datasets module from scikit-learn. The default data generated by this module is centered, meaning that our intercept, b0 is already set to zero.

** Task 1
We've written some code here to generate our data for the two feature scenario. The ~n_samples~ input parameter lets us set how many data points we'd like -we've set it to 100. ~n_features~ lets us specify the number of features we want, which would be 2 in our case.

The ~coef = True~ attribute gives us the coefficients generated by fitting a linear regression model. Print the coefficients given by this model. (Remember that intercept b0 is zero as the data is centered - so weâ€™re printing the best fit values of b1 and b2 here!)

** Task 2
We're going to write a pythonic function to output the value of the loss function at each data point for all possible values of b1 and b2. Fill in the code for the ~loss_function()~ so that it outputs loss according to the formula below:

$$
Loss\ function = \frac{1}{n} \sum_{i=1}^n (y_i - b_0 - b_1x_{1i} - b_2x_{2i})^2
$$

Note that we've done some of the work for you, so you just need to fill in the code to calculate the ~error~ variable inside the function.

*Hint*
The error for each datapoint can be obtained using:
#+begin_src python
  error = y - b1*x1 - b2*x2
  #Remember, b0 is 0 here!
#+end_src

** Task 3
A script ~plot_loss.py~ has been loaded for you in the workspace. It takes in the values of the coefficients as inputs to a function to output the loss as a contour plot.

** Task 4
Each ellipse in this plot represents the points where the loss function has the /same value/. The larger the ellipse, the higher the value of loss for all the points (b1,b2) that fall on the ellipse, as we can see from the labels on the contours.

Followin this logic, the center of this set of concentric ellipses should represent the minimun value of the loss function and the value of the coefficients (b1,b2) at this point should be the solution of the regression problem and represent the best fit for our data. Let's check if this is so by plotting the coefficients we had obtained earlier on!

** Scritp.py

#+begin_src python :results output
  import numpy as np
  import matplotlib.pyplot as plt

  from sklearn import datasets
  data, y, coefficients = datasets.make_regression(n_samples = 100, n_features = 2, coef = True, random_state = 23)
  x1 = data[:,0]
  x2 = data[:,1]

  # 1. Print the coefficients
  print(coefficients)

  # 2. Loss function
  def loss_function(b1,b2,y,x1,x2):
      error = y - b1*x1 - b2*x2
      loss = np.mean(error**2)
      return loss

  # 3. Plot loss function for data
  from plot_loss import plot_loss_function
  b1 = np.linspace(-150, 150, 501)
  b2 = np.linspace(-150, 150, 501)
  contour_plot = plot_loss_function(b1,b2,y,x1,x2)


  # 4. Plot the best fit coefficients
  best_fit_b1 = coefficients[0]
  best_fit_b2 = coefficients[1]
  plt.scatter(best_fit_b1, best_fit_b2, s=50., color='green')
  plt.show()

#+end_src

#+RESULTS:
: [77.30856619 18.87896326]

* The regularization term
Regularization penalizes models for overfitting by adding a "penalty term" to the lost function. The two most commonly used ways of doing this are known as /Lasso/ (or L1) and /Ridge/ (or L2) regularization.

Both of these rely on penalizing overfitting by controlling how large the coefficients can get in the first place. The penalty term or regularization term is multiplied by a factor alpha and added to the old loss function as follows:

$$
new\ loss\ function = old\ loss\ function + \alpha * regularization\ term
$$

Because of the additive term, minimizing the new loss function will necessarily mean "true loss" goes up, i.e., the scores on this will be lower on the training data than regression without regularization! But this is what we want when we are regularizing. Remember that the reason we're regularizing is because our model is overfitted to our data (i.e., it is performing well on training data but doesn't generalize well on test data).

The regularization term is the sum of the /absolute values/ of the coefficients in the case of L1 regularization and the sum of the /squares of the coefficients/ in the case of L2.

    - L1 regularization term: $|b_1| + |b_2|$

    - L2 regularization term: $b_1^2 + b_2^2$

      [[./regularization_term.png]]

In mathematics, the sum of the magnitudes of a vector is known as its L1 norm (related to "Manhattan distance") and the square root of the sum of the magnitudes (or the "Euclidean distance" from the origin) is known as its L2 norm -and that is the reason for the names of both methods!

* L1 or Lasso Regularization
In the case of the two-feature linear regression scenario we were looking at in the previous exercise, our new loss function for L1 regularization looks as follows:

$$
L1\ Loss =  \frac{1}{n} \sum_{i=1}^n (y_i - b_0 - b_1x_{1i} - b_2x_{2i})^2 + \alpha * (|b_1| + |b_2|)
$$

Minimizing this new loss function essentially means restricting the size of the regularization term while minimizing the old loss function. Let's say that our regularization term can take a maximum value, s:

$$
\vert b_1 \vert + \vert b_2 \vert \leq s
$$

This is equivalent to confining our coefficients to a surface around the origin bounded by 4 lines: b1+b2 = s, b1-b2 = s, b2-b1 = s and -b1-b2 = s.

[[./four_lines_surface.png]]

The figure replicates the elliptical contours for the loss function that we'd plotted earlier. If we plot these four lines as with b1 and b2 as X and Y axes respectively, we get the diamond shaped blue shaded region that we've shown here. We have chosen a value of s = 50 for this figure -this means that either coefficient can have a maximum value of 50. The choice of ~s~ is deeply tied to the choice of ~alpha~ as they are inversely related.

The value of (b1, b2) that satisfies the regularization constrain while minimizing the loss function is denoted by the white dot. Notice that it is exactly at the tip of the diamond where it intersects with a contour from our loss function. It also happens to fall exactly on the X axis, thus setting the value of b2 to 0!

Why is this the point that minimizes the new loss function? Remember that the goal here is to minimize the original loss function /while/ meeting the regularization constrain. We still want to be as close to the center of the contours (the original loss function minimum) as possible. The point that's closests to the center of the contours /while/ simultaneously lying within the regularization surface boundary is the white dot!

The word Lasso is actually an acronym for "Least Absolute Shrinkage and Selection Operator" -it shrinks the absolute value of coefficients and selects parameters by way of setting some of the coefficients to zero. If the coefficient of a regression model is set to zero, it means that we've pretty much eliminated the feature associated with it. In models with a high number of features, lasso regularization tends to set a significant fraction of its parameters to zero, thus acting as a feature selection method.

We've covered the basics of how Lasso regularization minimizes overfitting and simultaneously acts as a feature selection method using a two-feature example. How does this extend to a multiple feature dataset? The loss function for a multiple linear regression case with m features looks as follows:

$$
L1\ Loss =  \frac{1}{n} \sum_{i=1}^n (y_i - b_0 - b_1x_{1i} - b_2x_{2i} \cdots- b_mx_{mi} )^2  + \alpha * (|b_1| + |b_2| + \cdots + |b_m|)
$$

We're going to examine this by reapplying a multiple linear regression model to the student performance dataset we were looking at earlier on -only this time, we are going to do this with L1 regularization. A quick reminder that our original unregularized coefficients look as shown in the image below:

[[./coefficients_without_regularization.png]]

We've loaded a file, script.py here containing the analysis we did earlier on. Some of the things we'd noted earlier on were:

    - too many features possibly (from the number of columns in the DataFrame)

    - testing error is higher than training error (by looking at he MSE)

    - highly negatively correlated features (inferred from the plot of the coefficients)

** Task 1
We can implement Lasso for this dataset by importing the Lasso module within scikit-learn's class of ~linear_model~'s. We're choosing  a value of s by setting alpha to 0.1 -the scikit-learn default is 1. We've written some code in script.py to do a train-test split, implement regression with Lasso regularization and calculate the training error.

** Task 2
Following the syntax for training error, calculate the testing error using the following steps:

    - set the variable ~l1_pred_test~ equal to the result of applying the model to ~X_test~

    - set the variable ~l1_mse_test~ equal to the Mean Squared Error of the test data

    - print ~l1_mse_test~

The training and test Mean Squared Errors obtained from implementing the regression without regularization were 2.627 and 4.987 (rounded to three decimals!) respectively. How do our new errors compare?

** Task 3
We see how our training error went up but our test error went down, which makes sense as our model is performing slightly better on test data at the cost of performing slightly worse on training data. What do our coefficients look like post Lasso regularization?

Add plt.show() and press "Run".

Lasso has shrunk more than half our coefficients to zero! Additionally, while the value of all the coefficients have shrunk, Lasso has increased the relative importance of some. An important thing to note here is that we manually set the value of alpha to 0.1. The number of features that get eliminated due to Lasso is definitely tied to the value of alpha -we're going to examine this more after we cover the basics of L2 or Ridge regularization.

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import helpers

  df = pd.read_csv("student_math.csv")
  y = df['Final_Grade']
  X = df.drop(columns = ['Final_Grade'])

  # 1. Train-test split and fitting an L1-regularized regression model
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import Lasso

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
  lasso = Lasso(alpha=0.1)
  lasso.fit(X_train, y_train)

  l1_pred_train = lasso.predict(X_train)
  l1_mse_train = np.mean((l1_pred_train - y_train)**2)
  print("Lasso (L1) Training Error: ", l1_mse_train)

  # 2. Calculate testing error
  l1_pred_test = lasso.predict(X_test)
  l1_mse_test = np.mean((l1_pred_test - y_test)**2)
  print("Lasso (L1) Testing Error: ", l1_mse_test)

  # 3. Plotting the coefficients
  predictors = X.columns
  coef = pd.Series(lasso.coef_, predictors).sort_values()
  plt.figure(figsize = (12,8))
  plt.ylim(-1.0,1.0)
  coef.plot(kind='bar', title='Regression Coefficient with Lasso (L1) Regularization')

  plt.show()
#+end_src

#+RESULTS:
: Lasso (L1) Training Error:  3.0118464559476825
: Lasso (L1) Testing Error:  4.272174214435374
