
* Why Regularize?
Regularization plays a very important role in real world implementations of machine learning models. It is a statistical technique that /minimizes overfitting/ and is executed during the /model fitting step./ It is also an embedded feature selection method because it is implemented while the parameters of the model are being calculated.

Machine learning algorithms are rarely deployed to production without using some form of regularization. The reason for this is as follows: In practice, every model has to deal with the question o how well it can generalize from known to unknown data. We can train, test and tune models on known data and make them as accurate as possible. However, in deploying models, we are applying them on /new data./ Regularization makes sure that our model is still accurate.

In this lesson, we are going to learn how regularization minimizes overfitting and how ot use it as a feature selection method. Along the way, we are also going to learn two concepts that are very relevant to regularization and important just as standalone topics within machine learning, namely, the /bias-variance tradeoff/ and /hyperparameter tuning./ But first, we're going to answer the question: what is overfitting?

** Task 1
In the image to the right, we have three curves generated by different model fits to the same data. Can you identify which of these might be:

    underfitting the data?
    overfitting the data?

Fill in the number corresponding to the plots that best matches these on the code editor.

[[./overfitting_underfitting.png]]

* What is overfitting?
It might have been pretty apparent which of the three curves in the previous exercise overfit the set of points. However, often we have too many features to be able to visually assess if our model is overfitting our data or not. And so we need to qualify what we mean by overfitting exactly.

If a model is able to represent a particular set of data points effectively buy is not able to fit new data well, it is overfitting the data. Such a model has one or more of the followin attributes:

    - It fits the training data well but performs significantly worse on test data

    - It typically has more parameters than necessary, i.e., it has /high model complexity/

    - It might be fitting for features that are multi-collinear (i.e., features that are highly negatively or positively correlated)

    - It might be fitting the noise in the data and likely mistaking the noise for features

In practice, we often catch overfitting by comparing the model performance on the training data versus test data. For instance if the R-squared score is high for training data but the model performs pooly on test data, it's a strong indicator of overfitting. We're going to look at one such example now. A dataset has been loaded for you from the [[https://archive.ics.uci.edu/dataset/320/student+performance][UCI Machine Learning Repository]] that describes the performance in mathematics of students form two Portuguese schools. We're going to implement a multiple linear regression model to predict the final grade of the students based on a number of features in the dataset.

** Task 1
We've implemented some code to print the first five rows, set the target variable as y and the predictor variables as the matrix X. Calculate the number of features in the dataset, save it as num_features and press Run. (Note that the number of features is equal to the number of columns in X).

*Hint*
Set ~num_features~ equal to ~len(X.columns)~

** Task 2
We're about to fit the data to a model with 42 parameters! (Intercept plus 41 coefficients, one for each predictor variable). This might already make us apprehensive of the model potentially overfitting the data because:

    - the model likely has a high degree of complexity

    - we haven't checked for collinearity between the features

In order to assess whether we are overfitting, we're going to fit the model. In the workspace, you'll see that we've provided code to split the data into a training and test set, fit a regression on all 41 features, and calculated the mean squared error (MSE) on the training data.

In the section below the comment # 2.Testing Error, fill in the code to calculate the MSE for the test set. Save the result as ~MSE_test~ (currently set to None).

** Task 3
We see that the error on the test data is almost double the error with the training data! This is definitely an indicator of overfitting. We're now going to check the coefficients evaluated by the model to see if we can catch this.

Uncomment the plotting code below the comment #3:  Plotting the Coefficients and press Run to plot values of the coefficients produced.

We see that there are a few negatively correlated coefficients here (coefficients with roughly similar values mirrored about the axis), which is also a sign of overfitting.


** Script.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt

  df = pd.read_csv("student_math.csv")
  print(df.head())

  # setting target and predictor variables
  y = df['Final_Grade']
  X = df.drop(columns = ['Final_Grade'])

  # 1. Number of features
  num_features = len(X.columns)
  print("Number of features: ", num_features)

  # Performing a Train-Test split
  from sklearn.model_selection import train_test_split
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

  # Fitting a Linear Regression Model
  from sklearn.linear_model import LinearRegression
  model = LinearRegression()
  model.fit(X_train, y_train)

  # Training Error
  pred_train = model.predict(X_train)
  MSE_train = np.mean((pred_train - y_train)**2)
  print("Training Error: ", MSE_train)

  # 2. Testing Error
  pred_test = model.predict(X_test)
  MSE_test = np.mean((pred_test - y_test)**2)
  print("Testing Error: ", MSE_test)

  # Calculating the regression coefficients
  predictors = X.columns
  coef = pd.Series(model.coef_,predictors).sort_values()

  # 3. Plotting the Coefficients
  plt.figure(figsize = (15,10))
  coef.plot(kind='bar', fontsize = 20)
  plt.title("Regression Coefficients", fontsize = 30)
  plt.show()

#+end_src

#+RESULTS:
#+begin_example
   age  Medu  Fedu  ...  higher_yes  internet_yes  romantic_yes
0   18     4     4  ...           1             0             0
1   17     1     1  ...           1             1             0
2   15     1     1  ...           1             1             0
3   15     4     2  ...           1             1             1
4   16     3     3  ...           1             0             0

[5 rows x 42 columns]
Number of features:  41
Training Error:  2.6268841802196254
Testing Error:  4.987346573982337
#+end_example

* The Loss Function
Let's revisit how the coefficients (or parameters) of a linear regression model are obtained. Suppose we want to fit a linear regression with two features, $x_1$ and $x_2$:

$$
y = b_0 + b_1x_1 + b_2x_2 + e
$$

We solve for the intercept b0 and coefficients b1 and b2 by minimizing the mean squared error. This is also referred to as the *loss function* (or alternately, as the /objective function/ or /cost function/) in machine learning:

$$
Loss\ function = \frac{1}{n} \sum_{i=1}^n (y_i - b_0 - b_1x_{1i} - b_2x_{2i})^2
$$

The loss function is minimized by the method of Ordinary Least Squares (OLS) and for larger datasets, the gradient descent algorithm is used to arrive at the minimum. As we have seen already however, the "best fit" as obtained by minimizing the loss function might not be the fit that generalizes well to new data. Regularization modifies the loss function in a way that might help with this.

Before going into more detail, we're going to plot the loss function as a function of the coefficients b1 and b2. For this, we're going to generate a dataset for two-feature linear regression using the datasets module from scikit-learn. The default data generated by this module is centered, meaning that our intercept, b0 is already set to zero.

** Task 1
We've written some code here to generate our data for the two feature scenario. The ~n_samples~ input parameter lets us set how many data points we'd like -we've set it to 100. ~n_features~ lets us specify the number of features we want, which would be 2 in our case.

The ~coef = True~ attribute gives us the coefficients generated by fitting a linear regression model. Print the coefficients given by this model. (Remember that intercept b0 is zero as the data is centered - so weâ€™re printing the best fit values of b1 and b2 here!)

** Task 2
We're going to write a pythonic function to output the value of the loss function at each data point for all possible values of b1 and b2. Fill in the code for the ~loss_function()~ so that it outputs loss according to the formula below:

$$
Loss\ function = \frac{1}{n} \sum_{i=1}^n (y_i - b_0 - b_1x_{1i} - b_2x_{2i})^2
$$

Note that we've done some of the work for you, so you just need to fill in the code to calculate the ~error~ variable inside the function.

*Hint*
The error for each datapoint can be obtained using:
#+begin_src python
  error = y - b1*x1 - b2*x2
  #Remember, b0 is 0 here!
#+end_src

** Task 3
A script ~plot_loss.py~ has been loaded for you in the workspace. It takes in the values of the coefficients as inputs to a function to output the loss as a contour plot.

** Task 4
Each ellipse in this plot represents the points where the loss function has the /same value/. The larger the ellipse, the higher the value of loss for all the points (b1,b2) that fall on the ellipse, as we can see from the labels on the contours.

Followin this logic, the center of this set of concentric ellipses should represent the minimun value of the loss function and the value of the coefficients (b1,b2) at this point should be the solution of the regression problem and represent the best fit for our data. Let's check if this is so by plotting the coefficients we had obtained earlier on!

** Scritp.py

#+begin_src python :results output
  import numpy as np
  import matplotlib.pyplot as plt

  from sklearn import datasets
  data, y, coefficients = datasets.make_regression(n_samples = 100, n_features = 2, coef = True, random_state = 23)
  x1 = data[:,0]
  x2 = data[:,1]

  # 1. Print the coefficients
  print(coefficients)

  # 2. Loss function
  def loss_function(b1,b2,y,x1,x2):
      error = y - b1*x1 - b2*x2
      loss = np.mean(error**2)
      return loss

  # 3. Plot loss function for data
  from plot_loss import plot_loss_function
  b1 = np.linspace(-150, 150, 501)
  b2 = np.linspace(-150, 150, 501)
  contour_plot = plot_loss_function(b1,b2,y,x1,x2)


  # 4. Plot the best fit coefficients
  best_fit_b1 = coefficients[0]
  best_fit_b2 = coefficients[1]
  plt.scatter(best_fit_b1, best_fit_b2, s=50., color='green')
  plt.show()

#+end_src

#+RESULTS:
: [77.30856619 18.87896326]

* The regularization term
Regularization penalizes models for overfitting by adding a "penalty term" to the lost function. The two most commonly used ways of doing this are known as /Lasso/ (or L1) and /Ridge/ (or L2) regularization.

Both of these rely on penalizing overfitting by controlling how large the coefficients can get in the first place. The penalty term or regularization term is multiplied by a factor alpha and added to the old loss function as follows:

$$
new\ loss\ function = old\ loss\ function + \alpha * regularization\ term
$$

Because of the additive term, minimizing the new loss function will necessarily mean "true loss" goes up, i.e., the scores on this will be lower on the training data than regression without regularization! But this is what we want when we are regularizing. Remember that the reason we're regularizing is because our model is overfitted to our data (i.e., it is performing well on training data but doesn't generalize well on test data).

The regularization term is the sum of the /absolute values/ of the coefficients in the case of L1 regularization and the sum of the /squares of the coefficients/ in the case of L2.

    - L1 regularization term: $|b_1| + |b_2|$

    - L2 regularization term: $b_1^2 + b_2^2$

      [[./regularization_term.png]]

In mathematics, the sum of the magnitudes of a vector is known as its L1 norm (related to "Manhattan distance") and the square root of the sum of the magnitudes (or the "Euclidean distance" from the origin) is known as its L2 norm -and that is the reason for the names of both methods!
