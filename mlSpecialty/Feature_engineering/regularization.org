
* Why Regularize?
Regularization plays a very important role in real world implementations of machine learning models. It is a statistical technique that /minimizes overfitting/ and is executed during the /model fitting step./ It is also an embedded feature selection method because it is implemented while the parameters of the model are being calculated.

Machine learning algorithms are rarely deployed to production without using some form of regularization. The reason for this is as follows: In practice, every model has to deal with the question o how well it can generalize from known to unknown data. We can train, test and tune models on known data and make them as accurate as possible. However, in deploying models, we are applying them on /new data./ Regularization makes sure that our model is still accurate.

In this lesson, we are going to learn how regularization minimizes overfitting and how ot use it as a feature selection method. Along the way, we are also going to learn two concepts that are very relevant to regularization and important just as standalone topics within machine learning, namely, the /bias-variance tradeoff/ and /hyperparameter tuning./ But first, we're going to answer the question: what is overfitting?

** Task 1
In the image to the right, we have three curves generated by different model fits to the same data. Can you identify which of these might be:

    underfitting the data?
    overfitting the data?

Fill in the number corresponding to the plots that best matches these on the code editor.

[[./overfitting_underfitting.png]]
