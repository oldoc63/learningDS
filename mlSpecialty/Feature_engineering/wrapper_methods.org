
* Introduction to wrapper methods
Machine learning problems often involve datasets with many features. Some of those features might be very important for a specific machine learning model. Other features might be irrelevant. Given a feature set and a model, we would like to be able to distinguish between important and uninportant features (or even important combinations of features). Wrappers methods do exactly that.

A /wrapper method/ for feature selection is an algorithm that selects features by evaluating the performance of a machine learning model on different subsets of features. These algorithms add or remove features one at a time based on how useful those features are to the model.

Wrapper methods have some advantages over filter methods. The main advantage is that wrapper methods evaluate features based on their performance with a /specific model./ Filter methods, on the other hand, can't tell how important a feature is to a model.

Another upside of wrapper methods is that they can take into account relationships between features. Sometimes certain features aren't very useful on their own but instead perform well only when combined with other features. Since wrapper methods test subsets of features, they can account for those situations.

This lesson will explain five different wrapper methods:

    - Sequential forward selection

    - Sequential backward selection

    - Sequential forward floating selection

    - Sequential backward floating selection

    - Recursive feature elimination

You'll learn how to implement these algorithms in Python and evaluate the results.

Before we get started, let's take a look at a dataset that you'll use throughout this lesson.

** Task 1
The data in the workspace was taken from the UCI Machine Learning Repository. The outcome variable, ~Classification~, is a 1 if a patient has breast cancer and a 0 if not. The features are Age, BMI (body mass index), Glucose, Insulin, HOMA (homeostatic model assessment, a measure of insulin resistance), as well as four different protein levels: Leptin, Adiponectin, Resistin, and MCP.1.

Uncomment print(health.head()) and take a look at the data.

What are some pros and cons of having this many features?

** Script.py
#+begin_src python :results output
  import pandas as pd

  # Load the data
  health = pd.read_csv("dataR2.csv")

  print(health.head())

#+end_src

#+RESULTS:
:    Age        BMI  Glucose  ...  Resistin    MCP.1  Classification
: 0   48  23.500000       70  ...   7.99585  417.114               1
: 1   83  20.690495       92  ...   4.06405  468.786               1
: 2   82  23.124670       91  ...   9.27715  554.697               1
: 3   68  21.367521       77  ...  12.76600  928.220               1
: 4   86  21.111111       92  ...  10.57635  773.920               1
:
: [5 rows x 10 columns]

* Setting Up a Logistic Regression Model
Before we can use a wrapper method, we need to specify a machine learning model. We'll train a logistic regression model on the health data and see how well it performs.

We'll prepare the data by splitting it into a pandas DataFrame ~X~ and a pandas Series ~y~. X will contain the observations of the independent variables, and y will contain the observations of the dependent variable.

Here's an example of how to do this. The fire dataset below was taken form the UCI Machine Learning Repository and cleaned for our analysis. Its features are ~Temperature~, ~RH~ (relative humidity), ~Ws~ (wind speed), ~Rain~, ~DMC~ (Duff Moisture Code), and ~FWI~ (Fire Weather Index). The final column, ~Classes~, contains a ~1~ if there is a forest fire at a specific location on a given day and ~0~ if not.

#+begin_src python
  import pandas as pd

  # Load the data
  fire = pd.read_csv("fire.csv")

  # Split independent and dependent variables
  X = fire.iloc[:, :-1]
  y = fire.iloc[:, -1]
#+end_src

We can create a logistic regresion model and fit it to X and y with scikit-learn using the following code:

#+begin_src python
  from sklearn.linear_model import LogisticRegression

  # Create and fit the logistic regression model
  lr = LogisticRegression()
  lr.fit(X, y)
#+end_src

Logistic regression models give a /probability/ that an observation belongs to a category. In the fire dataset, probabilities greater than 0.5 are considered predictions that there is a fire, and probabilities less than 0.5 are considered predictions than there is no fire. In the health dataset, probabilities greater than 0.5 are considered predictions that a patient has breast cancer.

The /accuracy/ of a logistic regression model is the percentage of correct predictions that it makes on a testing set. In scikit-learn, you can check the accuracy of a model with the ~.score()~ method.

#+begin_src python
  print(lr.score(X, y))
#+end_src

This outputs:

#+begin_src python
  0.9836065573770492
#+end_src

For our testing set, our logistic regression model correctly predicts whether a fire occurred 98.4% of the time.

** Task 1
The dataset health has been loaded and split into features X and outcome y for you in script.py. Use the .fit() method to fit lr to X and y.

#+begin_comment
*Note*
scikit-learn uses an algorithm that goes through many iterations to find optimal logistic regression coefficients. For this particular dataset, the algorithm doesn’t converge after the default maximum number of iterations, so we’ve included max_iter=1000 as a parameter in the LogisticRegression object just to make sure the algorithm converges.
#+end_comment

** Task 2
Use the .score() method to print the accuracy of the model and print it. How often does this model correctly predict whether or not a patient has breast cancer?

** Script.py
#+begin_src python :results output
  import pandas as pd
  from sklearn.linear_model import LogisticRegression

  # Load the data
  health = pd.read_csv("dataR2.csv")

  # Split independent and dependent variables
  X = health.iloc[:, :-1]
  y = health.iloc[:, -1]

  # Logistic regression model
  lr = LogisticRegression(max_iter=1000)

  # Fit the model
  lr.fit(X, y)

  # Print the accuracy of the model
  print(lr.score(X, y))

#+end_src

#+RESULTS:
: 0.8017241379310345

* Sequential Forward Selection
Now that we have a specific machine learning model, we can use a wrapper method to choose a smaller feature subset.

Sequential forward selection is a wrapper method that builds a feature set by starting with no features and then adding one feature at a time until a desired number of features is reached. In the first step, the algorithm will train and test a model using only one feature at a time. The algorithm keeps the feature that perform best.

In each subsequent step, the algorithm will test the model on each possible new feature addition. Whichever feature improves model performance the most is then added to the feature subset. This process stops once we have the desired number of features.

Let's say we want to use three features, and we have five features to choose from: age, height, weight, ~blood_pressure~, and ~resting_heart_rate~. Sequential forward selection will train your machine learning model on five different feature subsets: one for each feature.

If the model perform best on the subset {age}, the algorithm will then train and test the model on the following four subsets:

    - {~age, height~}

    - {~age, weight~}

    - {~age, blood_pressure~}

    - {~age, resting_heart_rate~}

If the model perform best on {~age, resting_heart_rate~}, the algorithm will test the model on the following three subsets:

    - {~age, height, resting_heart_rate~}

    - {~age, weight, resting_heart_rate~}

    - {~age, blood_pressure, resting_heart_rate~}

If the model perform best on {~age, weight, resting_hear_rate~}, it will stop the algorithm and use that feature set.

Sequential forward selection is a /greedy algorithm/: instead of checking every possible feature set by brute force, it adds whichever feature gives the best inmediate performance gain.

** Task 1
If you used sequential forward selection to add a fourth feature to the feature set {age, weight, resting_hear_rate} that you saw in the preceding example, what two feature sets would the algorithm test in the next step? Create sets called set1 and set2 that contain the appropriate features.

For example, if the answer was {height, resting_heart_rate} and {weight, resting_heart_rate}, you would type:

#+begin_src python
set1 = {"height", "resting_heart_rate"}
set2 = {"weight", "resting_heart_rate"}
#+end_src

** Script.py

#+begin_src python
set1 = {"age", "height", "weight", "resting_heart_rate"}
set2 = {"age", "weight", "blood_pressure", "resting_heart_rate"}
#+end_src

* Sequential Forward Selection with mlxtend
Recall from a previous exercise that the logistic regression model was about 80.2% accurate at predicting if a patient had breast cancer. But there were NINE different features. Are all of those features necessary, or is it possible that the model could make accurate predictions with fewer features? That would make the model easier to understand, and it could simplity diagnosis.

We will use the SFS class from Python's mlxtend library to implement sequential forward selection and choose a subset of just THREE features for the logistic regression model that se use earlier.

#+begin_src python
  # Set up SFS parameters
  sfs = SFS(lr,
              k_features=3, #number of features to select
              forward=True,
              floating=False,
              scoring='accuracy',
              cv=0)
  # Fit SFS to our features X and outcome y
  sfs.fit(X, y)
#+end_src

    - The first parameter is the name of the model that you're using, In the previous exercise, we called the logistic regression model ~lr~.

    - The parameter ~k_features~ determines how many features the algorithm will select.

    - ~forward=True~ and ~floating=False~ ensure that we are using sequential forward selection.

    - ~scoring~ determines how the algorithm will evaluate each feature subset. It's often okay to use the default value None because mlxtend will automatically use a metric that is suitable for whatever scikit-learn model you are using. For this lesson, we'll set it to 'accuracy'.

    - ~cv~ allows you to do k-fold cross-validation. We'll leave it at 0 for this lesson an only evaluate performance on the training set.

We'll see which features were selected in the next exercise. For now, we just want to fit the SFS model.

** Task 1
Use mlxtend to create an SFS object called sfs. Set it to select three features, and make sure that you set forward and floating arguments to ensure that you are using sequential forward selection. Also set scoring='accuracy' and cv=0.

** Script.py

#+begin_src python :results output
  import pandas as pd
  from sklearn.linear_model import LogisticRegression
  from mlxtend.feature_selection import SequentialFeatureSelector as sfs

  # Load the data
  health = pd.read_csv("dataR2.csv")
  X = health.iloc[:, :-1]
  y = health.iloc[:, -1]

  # Logistic regression model
  lr = LogisticRegression(max_iter=1000)

  # Sequential forward selection
  sfs = sfs(lr,
            k_features=3,
            forward=True,
            floating=False,
            scoring='accuracy',
            cv=0)

  # Fit the sequential forward selection model
  sfs.fit(X, y)

#+end_src

#+RESULTS:
