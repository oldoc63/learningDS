
* Introduction to wrapper methods
Machine learning problems often involve datasets with many features. Some of those features might be very important for a specific machine learning model. Other features might be irrelevant. Given a feature set and a model, we would like to be able to distinguish between important and uninportant features (or even important combinations of features). Wrappers methods do exactly that.

A /wrapper method/ for feature selection is an algorithm that selects features by evaluating the performance of a machine learning model on different subsets of features. These algorithms add or remove features one at a time based on how useful those features are to the model.

Wrapper methods have some advantages over filter methods. The main advantage is that wrapper methods evaluate features based on their performance with a /specific model./ Filter methods, on the other hand, can't tell how important a feature is to a model.

Another upside of wrapper methods is that they can take into account relationships between features. Sometimes certain features aren't very useful on their own but instead perform well only when combined with other features. Since wrapper methods test subsets of features, they can account for those situations.

This lesson will explain five different wrapper methods:

    - Sequential forward selection

    - Sequential backward selection

    - Sequential forward floating selection

    - Sequential backward floating selection

    - Recursive feature elimination

You'll learn how to implement these algorithms in Python and evaluate the results.

Before we get started, let's take a look at a dataset that you'll use throughout this lesson.

** Task 1
The data in the workspace was taken from the UCI Machine Learning Repository. The outcome variable, ~Classification~, is a 1 if a patient has breast cancer and a 0 if not. The features are Age, BMI (body mass index), Glucose, Insulin, HOMA (homeostatic model assessment, a measure of insulin resistance), as well as four different protein levels: Leptin, Adiponectin, Resistin, and MCP.1.

Uncomment print(health.head()) and take a look at the data.

What are some pros and cons of having this many features?

* Script.py
#+begin_src python :results output
  import pandas as pd

  # Load the data
  health = pd.read_csv("dataR2.csv")

  print(health.head())

#+end_src

#+RESULTS:
:    Age        BMI  Glucose  ...  Resistin    MCP.1  Classification
: 0   48  23.500000       70  ...   7.99585  417.114               1
: 1   83  20.690495       92  ...   4.06405  468.786               1
: 2   82  23.124670       91  ...   9.27715  554.697               1
: 3   68  21.367521       77  ...  12.76600  928.220               1
: 4   86  21.111111       92  ...  10.57635  773.920               1
:
: [5 rows x 10 columns]

* Setting Up a Logistic Regression Model
Before we can use a wrapper method, we need to specify a machine learning model. We'll train a logistic regression model on the health data and see how well it performs.

We'll prepare the data by splitting it into a pandas DataFrame ~X~ and a pandas Series ~y~. X will contain the observations of the independent variables, and y will contain the observations of the dependent variable.

Here's an example of how to do this. The fire dataset below was taken form the UCI Machine Learning Repository and cleaned for our analysis. Its features are ~Temperature~, ~RH~ (relative humidity), ~Ws~ (wind speed), ~Rain~, ~DMC~ (Duff Moisture Code), and ~FWI~ (Fire Weather Index). The final column, ~Classes~, contains a ~1~ if there is a forest fire at a specific location on a given day and ~0~ if not.

#+begin_src python
  import pandas as pd

  # Load the data
  fire = pd.read_csv("fire.csv")

  # Split independent and dependent variables
  X = fire.iloc[:, :-1]
  y = fire.iloc[:, -1]
#+end_src

We can create a logistic regresion model and fit it to X and y with scikit-learn using the following code:

#+begin_src python
  from sklearn.linear_model import LogisticRegression

  # Create and fit the logistic regression model
  lr = LogisticRegression()
  lr.fit(X, y)
#+end_src

Logistic regression models give a /probability/ that an observation belongs to a category. In the fire dataset, probabilities greater than 0.5 are considered predictions that there is a fire, and probabilities less than 0.5 are considered predictions than there is no fire. In the health dataset, probabilities greater than 0.5 are considered predictions that a patient has breast cancer.

The /accuracy/ of a logistic regression model is the percentage of correct predictions that it makes on a testing set. In scikit-learn, you can check the accuracy of a model with the ~.score()~ method.

#+begin_src python
  print(lr.score(X, y))
#+end_src

This outputs:

#+begin_src python
  0.9836065573770492
#+end_src

For our testing set, our logistic regression model correctly predicts whether a fire occurred 98.4% of the time.

** Task 1
The dataset health has been loaded and split into features X and outcome y for you in script.py. Use the .fit() method to fit lr to X and y.

#+begin_comment
*Note*
scikit-learn uses an algorithm that goes through many iterations to find optimal logistic regression coefficients. For this particular dataset, the algorithm doesn’t converge after the default maximum number of iterations, so we’ve included max_iter=1000 as a parameter in the LogisticRegression object just to make sure the algorithm converges.
#+end_comment

** Task 2
Use the .score() method to print the accuracy of the model and print it. How often does this model correctly predict whether or not a patient has breast cancer?

* Script.py
#+begin_src python :results output
  import pandas as pd
  from sklearn.linear_model import LogisticRegression

  # Load the data
  health = pd.read_csv("dataR2.csv")

  # Split independent and dependent variables
  X = health.iloc[:, :-1]
  y = health.iloc[:, -1]

  # Logistic regression model
  lr = LogisticRegression(max_iter=1000)

  # Fit the model
  lr.fit(X, y)

  # Print the accuracy of the model
  print(lr.score(X, y))

#+end_src

#+RESULTS:
: 0.8017241379310345
