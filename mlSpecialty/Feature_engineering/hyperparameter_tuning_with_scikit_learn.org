
* Hyperparameter tunning with scikit-learn

** Introduction
In this lesson you will learn about the different methods one can use to tune hyperparameters in machine learning models and how to implement  them in Python. Specifically we will be diving deep into two methods: grid search (Grid SearchCV) and random search (RandomizedSearchCV).

To understand the implementation of different methods of hyperparameter tuning, we need to choose a dataset, a classification or regression problem we'd like to solve, and a machine learning model to solve it with. The image on the right-hand side lists some of the commolnly used machine learning models and their corresponding hyperparameters. Our choices for the rest of the lesson are as follows:

    - *Dataset and Model*: We're going to work with the commoly used breast cancer dataset that is available with scikit-learn. The prediction task is to classify tumors as bening or malignant and the data has 30 numeric predictor variables. We're goin to use logistic regression to perform this task.
    - *Hyperparameters*: So, which hyperparameters do we tune? There are many arguments to scikit-learn's logistic regression function and many of them can be treated as hyperparameters and tuned. However, we're specifically going to focus on the hyperparameters pertaining to regularization in this exercise.

This lesson assumes familiarity with the implementation of logistic regression models and regularizations techniques.

[[./sklearn_parameters.png]]

The terminology here between the words 'parameters' and 'hyperparameters' can be confusing, even in the table. There is a rather circular ezplanation for this:

    - In machine learning theory, the *only* parameters of a logistic regression model are its *coefficients* and the *intercept*. Subsequently, the inputs/arguments of a function are the hyperparameters (e.g. type of regularization, number of iterations).
    - In Python code and scikit-learn however, the inputs/arguments to a function are often referred to as parameters (instead of hyperparameters).

For this reason, we'll continue to use the term hyperparameters in the lesson except when tuning the models in scikit-learn, we'll use the term parameters. This is an unfortunate convention that has persisted so there's no way around it -but we hope this explanation clarifies it!

** An Introduction to the Grid Search Method
We'll start with the grid search algorithm. Grid search works by testing a model on a list of hyperparameter values deciden upon beforehand. Suppose we had two hperparameters we wanted to tune and we wanted to choose between  6 values for the first one and 5 values of the second, we'd be searching a grid of thirty values as shown below. Grid search would fit the model and evaluate its performance for each of the values represented by these points. We can then coose the hyperparameter values corresponding to the best performance and conclude our hyperparameter search!

[[./grid.png]]

In our case, where we're using a logistic regression model with regularization ([[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html][LogisticRegression]]), we might want to make decisions on the following using hyperparameter tuning:

    1. *The type of regularization to use*: The ~penalty~ parameter in scikit-learn represents this and let's say we'd like to figure out if Lasso ('l1') or Ridge regression ('l2') is the better choice here.
    2. *The strength of regularization to use*: The scikit-learn parameter ~C~ represents the inverse of regularization strength and let's say we'd like to test 3 possible values of C here: 1, 10 and 100.

This means we have to search a grid of 2*3 = 6 values. ~GridSearchCV~ in scikit_learn let us do this! It test the model performance at each of these values to come up with the best possible set of hyperparameters from the list given.

The two most important parameters in GridSearchCV that need to be specified are:
the /name of the model/ that we are testing and the name of a /dictionary of hyperparameters/ that we would initialize, represented by the argument parameters. To tune the hyperparameters, we can use ~.fit()~,  just as we would for a regular machine learning model.

We're going to be working with scikit-learn's breast cancer dataset that has already been loaded in the jupyter notebook. We've also performed a train-test-split. Run the setup cell.

*** Task 1
After running the setup cell, create a LogisticRegression model called lr. Since we want to use a solver that is compatible with both L1 and L2 regularization, set the parameter ~solver~ to ~'liblinear'~ (the default solver does not support L1 regularization!). To ensure that the model converges, set ~max_iter=1000~.

*** Task 2
Define a dictionary ~parameters~ that you will be performing the grid search over. The dictionary should have two keys:

    - ~'penalty'~, corresponding to the two regularization types, ['l1', 'l2'] (use a lowercase "L" with the number 1 and 2)

    - ~'C'~, the regularization strength to be set to [1, 10, 100]

*** Task 3
We can now set up grid search! Define a GridSearchCV() object named ~clf~. The first argument is ~estimator~, which corresponds to the logistic regression model you've created and the second argument is ~param_grid~, which corresponds to the dictionary you're performing the hyperparameter search on.

*** Script.py

#+begin_src python :results output
  from sklearn.datasets import load_breast_cancer
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LogisticRegression
  from sklearn.model_selection import GridSearchCV

  # Load the dataset
  cancer = load_breast_cancer()

  # Split into training and testing data
  X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target)

  ## Initializing model for grid search
  lr = LogisticRegression(solver='liblinear', max_iter=1000)

  #check output
  #print(lr.get_params())

  ## Initializing grid search dictionary parameters of hyperparameters search from
  parameters = {'penalty':['l1', 'l2'],
                'C':[1, 10, 100]}

  ## Set up grid search using GridSearchCV
  clf = GridSearchCV(lr, parameters)

#+end_src

#+RESULTS:

** Evaluating the Results of GridSearchCV
The CV in GridSearchCV is an acronym for *cross-validation*. It's best practice in machine learning to go beyond the usual train-test-split and have a holdout or validation dataset. Specifically, GridSearchCV uses a technique known as k-fold cross-validation. This works as follows.

*** Cross-validation in GridSearchCV
GridSearchCV subdivides the training data further into /another training and test data set./ It fits the model on this new training data and evaluates the model on the new test data. But to make sure that we don't accidentally have good performance in only one part of our dataset, GridSearchCV will do this process multiple times on different cross-validation splits so that every point in the data gets to be tested on at least once! The number of times this split happens is the "k" in "k-fold". For instance, in a 10-fold cross-validation, our data would be split into a 90:10 train-test split 10 times and GridSearchCV would evaluate the model on each fold.

[[./ten_fold.png]]

In scikit-learn, cv argument in GridSearchCV allows us to decide on the number of cross-validation splits we'd like. The default setting for this is 5.

*** Evaluating GridSearchCV results
After fitting a GridSearchCV model we can find out the results using the following attributes of the clf argument:

    - ~.best_estimator_~ gives us the best estimator

    - ~.best_score_~ gives us the mean cross-validated score corresponding to the best estimator

    - ~.best_params_~ gives us the set of hyperparameters that correspond to the best estimator

Additionally, the ~.cv_results_~ attribute gives us the scores for each hyperparameter combination in the grid. We're now ready to evaluate the grid search we set up earlier and we've preloaded the code from the previous exercise in the setup cell.

*** Task 1
After running the setup cell, fit the grid search classifier, clf to the training data. Set the best estimator to a variable best_model and print it to see which model performed best.

Uncomment the line that prints the best parameters to check if they match up!

*** Task 2
    - Set the score corresponding to the best estimator to a variable ~best_score~ and print it.

    - Calculate the accuracy of the best estimator on the test data and store it as the variable ~test_score~.
*** Task 3
We've successfully performed a grid search over a 2 by 3 grid of hyperparameters! Using the ~.cv_results_~ attribute, we can look at the scores for each of the values in the hyperparameters grid.

clf.cv_results_ is a dictionary of values.

    1. Use the key argument 'params' to get the list of parameters. Convert this to a pandas DataFrame named ~hyperparameter_grid~.

    2. Use the key argument 'mean_test_score' to get the list of mean test scores corresponding to the parameter values. Convert this to a single column pandas DataFrame named ~grid_scores~ and set the argument columns to ['score'].

 Uncomment the lines pertaining to concatenating the two dataframes and printing it to view the results of the grid search for each hyperparameter pair!

*** Script.py

#+begin_src python :results output
  import pandas as pd
  from sklearn.datasets import load_breast_cancer
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LogisticRegression
  from sklearn.model_selection import GridSearchCV

  # Load the dataset
  cancer = load_breast_cancer()

  # Split into training and testing data
  X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target)

  ## Initializing model and dictionary of hyperparameters
  lr = LogisticRegression(solver='liblinear', max_iter=1000)
  parameters = {'penalty':['l1', 'l2'], 'C':[1, 10, 100]}

  ## Setting up Grid Search
  clf = GridSearchCV(lr, parameters)

  ## Fit clf to training data and get best hyperparameters
  clf.fit(X_train, y_train)
  best_model = clf.best_estimator_

  print(best_model)
  print(clf.best_params_)

  ## Calculate training and test scores of the best estimator
  best_score = clf.best_score_
  test_score = clf.score(X_test, y_test)

  print(best_score)
  print(test_score)

  ## Viewing grid search results
  hyperparameter_grid = pd.DataFrame(clf.cv_results_['params'])
  grid_scores = pd.DataFrame(clf.cv_results_['mean_test_score'], columns = ['score'])

  df = pd.concat([hyperparameter_grid, grid_scores], axis=1)
  print(df)

#+end_src

#+RESULTS:
#+begin_example
LogisticRegression(C=10, max_iter=1000, penalty='l1', solver='liblinear')
{'C': 10, 'penalty': 'l1'}
0.9577291381668948
0.972027972027972
     C penalty     score
0    1      l1  0.946019
1    1      l2  0.946019
2   10      l1  0.957729
3   10      l2  0.950725
4  100      l1  0.957729
5  100      l2  0.953078
#+end_example
