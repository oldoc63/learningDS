
* Hyperparameter tunning with scikit-learn

** Introduction
In this lesson you will learn about the different methods one can use to tune hyperparameters in machine learning models and how to implement  them in Python. Specifically we will be diving deep into two methods: grid search (Grid SearchCV) and random search (RandomizedSearchCV).

To understand the implementation of different methods of hyperparameter tuning, we need to choose a dataset, a classification or regression problem we'd like to solve, and a machine learning model to solve it with. The image on the right-hand side lists some of the commolnly used machine learning models and their corresponding hyperparameters. Our choices for the rest of the lesson are as follows:

    - *Dataset and Model*: We're going to work with the commoly used breast cancer dataset that is available with scikit-learn. The prediction task is to classify tumors as bening or malignant and the data has 30 numeric predictor variables. We're goin to use logistic regression to perform this task.
    - *Hyperparameters*: So, which hyperparameters do we tune? There are many arguments to scikit-learn's logistic regression function and many of them can be treated as hyperparameters and tuned. However, we're specifically going to focus on the hyperparameters pertaining to regularization in this exercise.

This lesson assumes familiarity with the implementation of logistic regression models and regularizations techniques.

[[./sklearn_parameters.png]]

The terminology here between the words 'parameters' and 'hyperparameters' can be confusing, even in the table. There is a rather circular ezplanation for this:

    - In machine learning theory, the *only* parameters of a logistic regression model are its *coefficients* and the *intercept*. Subsequently, the inputs/arguments of a function are the hyperparameters (e.g. type of regularization, number of iterations).
    - In Python code and scikit-learn however, the inputs/arguments to a function are often referred to as parameters (instead of hyperparameters).

For this reason, we'll continue to use the term hyperparameters in the lesson except when tuning the models in scikit-learn, we'll use the term parameters. This is an unfortunate convention that has persisted so there's no way around it -but we hope this explanation clarifies it!

** An Introduction to the Grid Search Method
We'll start with the grid search algorithm. Grid search works by testing a model on a list of hyperparameter values deciden upon beforehand. Suppose we had two hperparameters we wanted to tune and we wanted to choose between  6 values for the first one and 5 values of the second, we'd be searching a grid of thirty values as shown below. Grid search would fit the model and evaluate its performance for each of the values represented by these points. We can then coose the hyperparameter values corresponding to the best performance and conclude our hyperparameter search!

[[./grid.png]]

In our case, where we're using a logistic regression model with regularization ([[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html][LogisticRegression]]), we might want to make decisions on the following using hyperparameter tuning:

    1. *The type of regularization to use*: The ~penalty~ parameter in scikit-learn represents this and let's say we'd like to figure out if Lasso ('l1') or Ridge regression ('l2') is the better choice here.
    2. *The strength of regularization to use*: The scikit-learn parameter ~C~ represents the inverse of regularization strength and let's say we'd like to test 3 possible values of C here: 1, 10 and 100.

This means we have to search a grid of 2*3 = 6 values. ~GridSearchCV~ in scikit_learn let us do this! It test the model performance at each of these values to come up with the best possible set of hyperparameters from the list given.

The two most important parameters in GridSearchCV that need to be specified are:
the /name of the model/ that we are testing and the name of a /dictionary of hyperparameters/ that we would initialize, represented by the argument parameters. To tune the hyperparameters, we can use ~.fit()~,  just as we would for a regular machine learning model.

We're going to be working with scikit-learn's breast cancer dataset that has already been loaded in the jupyter notebook. We've also performed a train-test-split. Run the setup cell.

*** Task 1
After running the setup cell, create a LogisticRegression model called lr. Since we want to use a solver that is compatible with both L1 and L2 regularization, set the parameter ~solver~ to ~'liblinear'~ (the default solver does not support L1 regularization!). To ensure that the model converges, set ~max_iter=1000~.

*** Task 2
Define a dictionary ~parameters~ that you will be performing the grid search over. The dictionary should have two keys:

    - ~'penalty'~, corresponding to the two regularization types, ['l1', 'l2'] (use a lowercase "L" with the number 1 and 2)

    - ~'C'~, the regularization strength to be set to [1, 10, 100]

*** Task 3
We can now set up grid search! Define a GridSearchCV() object named ~clf~. The first argument is ~estimator~, which corresponds to the logistic regression model you've created and the second argument is ~param_grid~, which corresponds to the dictionary you're performing the hyperparameter search on.

*** Script.py

#+begin_src python :results output
  from sklearn.datasets import load_breast_cancer
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LogisticRegression
  from sklearn.model_selection import GridSearchCV

  # Load the dataset
  cancer = load_breast_cancer()

  # Split into training and testing data
  X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target)

  ## Initializing model for grid search
  lr = LogisticRegression(solver='liblinear', max_iter=1000)

  #check output
  #print(lr.get_params())

  ## Initializing grid search dictionary parameters of hyperparameters search from
  parameters = {'penalty':['l1', 'l2'],
                'C':[1, 10, 100]}

  ## Set up grid search using GridSearchCV
  clf = GridSearchCV(lr, parameters)

#+end_src

#+RESULTS:

** Evaluating the Results of GridSearchCV
The CV in GridSearchCV is an acronym for *cross-validation*. It's best practice in machine learning to go beyond the usual train-test-split and have a holdout or validation dataset. Specifically, GridSearchCV uses a technique known as k-fold cross-validation. This works as follows.

*** Cross-validation in GridSearchCV
GridSearchCV subdivides the training data further into /another training and test data set./ It fits the model on this new training data and evaluates the model on the new test data. But to make sure that we don't accidentally have good performance in only one part of our dataset, GridSearchCV will do this process multiple times on different cross-validation splits so that every point in the data gets to be tested on at least once! The number of times this split happens is the "k" in "k-fold". For instance, in a 10-fold cross-validation, our data would be split into a 90:10 train-test split 10 times and GridSearchCV would evaluate the model on each fold.

[[./ten_fold.png]]

In scikit-learn, cv argument in GridSearchCV allows us to decide on the number of cross-validation splits we'd like. The default setting for this is 5.

*** Evaluating GridSearchCV results
After fitting a GridSearchCV model we can find out the results using the following attributes of the clf argument:

    - ~.best_estimator_~ gives us the best estimator

    - ~.best_score_~ gives us the mean cross-validated score corresponding to the best estimator

    - ~.best_params_~ gives us the set of hyperparameters that correspond to the best estimator

Additionally, the ~.cv_results_~ attribute gives us the scores for each hyperparameter combination in the grid. We're now ready to evaluate the grid search we set up earlier and we've preloaded the code from the previous exercise in the setup cell.

*** Task 1
After running the setup cell, fit the grid search classifier, clf to the training data. Set the best estimator to a variable ~best_model~ and print it to see which model performed best.

Uncomment the line that prints the best parameters to check if they match up!

*** Task 2
    - Set the score corresponding to the best estimator to a variable ~best_score~ and print it.

    - Calculate the accuracy of the best estimator on the test data and store it as the variable ~test_score~.
*** Task 3
We've successfully performed a grid search over a 2 by 3 grid of hyperparameters! Using the ~.cv_results_~ attribute, we can look at the scores for each of the values in the hyperparameters grid.

~clf.cv_results_~ is a dictionary of values.

    1. Use the key argument 'params' to get the list of parameters. Convert this to a pandas DataFrame named ~hyperparameter_grid~.

    2. Use the key argument '~mean_test_score~' to get the list of mean test scores corresponding to the parameter values. Convert this to a single column pandas DataFrame named ~grid_scores~ and set the argument columns to ['score'].

 Uncomment the lines pertaining to concatenating the two dataframes and printing it to view the results of the grid search for each hyperparameter pair!

*** Script.py

#+begin_src python :results output
  import pandas as pd
  from sklearn.datasets import load_breast_cancer
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LogisticRegression
  from sklearn.model_selection import GridSearchCV

  # Load the dataset
  cancer = load_breast_cancer()

  # Split into training and testing data
  X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target)

  ## Initializing model and dictionary of hyperparameters
  lr = LogisticRegression(solver='liblinear', max_iter=1000)
  parameters = {'penalty':['l1', 'l2'], 'C':[1, 10, 100]}

  ## Setting up Grid Search
  clf = GridSearchCV(lr, parameters)

  ## Fit clf to training data and get best hyperparameters
  clf.fit(X_train, y_train)
  best_model = clf.best_estimator_

  print(best_model)
  print(clf.best_params_)

  ## Calculate training and test scores of the best estimator
  best_score = clf.best_score_
  test_score = clf.score(X_test, y_test)

  print(best_score)
  print(test_score)

  ## Viewing grid search results
  hyperparameter_grid = pd.DataFrame(clf.cv_results_['params'])
  grid_scores = pd.DataFrame(clf.cv_results_['mean_test_score'], columns = ['score'])

  best  df = pd.concat([hyperparameter_grid, grid_scores], axis=1)
  print(df)

#+end_src

#+RESULTS:
#+begin_example
LogisticRegression(C=10, max_iter=1000, penalty='l1', solver='liblinear')
{'C': 10, 'penalty': 'l1'}
0.9577291381668948
0.972027972027972
     C penalty     score
0    1      l1  0.946019
1    1      l2  0.946019
2   10      l1  0.957729
3   10      l2  0.950725
4  100      l1  0.957729
5  100      l2  0.953078
#+end_example

** An introduction to the Random Search Method
We're now going to look at an alternate method to tune hyperparameters, the random search method. With grid search, we made a list of different hyperparameter values and tested each combination of them. What if instead of a pre-selected list, we tested some random values? A random search method that does this might look like the top half of the image as oppossed to an orderly grid like in grid search, (which is shown in the bottom half of the image for contrast).

To implement this, one needs to search over hyperparameters by drawing a list of random values from /distributions./ What would this mean for say, the hyperparameter C in logistic regression that we've been working with so far? Instead of selecting from a pre-determined list (like [1, 10, 100] which we've been using for grid search in previous exercises), we would draw 3 random values between say 0 and 100.

In scikit-learn, the [[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html][RandomSearchCV]] function implements random search with cross-validation. RandomSearchCV requires three arguments to be specified:

    1. ~estimator~: the machine learning model whose hyperparameters we're tuning; this is exactly the same for GridSearchCV

    2. ~param_distributions~: a dictionary which specifies the hyperparameters as keys and corresponding distributions to draw lists of values from for each hyperparameter. In GridSearchCV, we instead had ~param_grid~, a dictionary representing the grid of hyperparameters to search from

    3. ~n_iter~: the number of times the algorithm needs to randomly draw from the distributions. The default value for this is 10.

  We're now ready to dive into implementing the random search method!

  [[./Random_Grid.png]]

** Using RandomizedSearchCV
When we used grid search, we made a list of different hyperparameter values. With random search, we don't have to make a list, but we still have to provide some information about how we want to select random numbers. Do we want random numbers between 0 and 100? Between -10 and 10? Do we want the same chance of picking small numbers and picking large numbers?

We can do this by specifying a probability distribution for each hyperparameter.
#+begin_src python
  from scipy.stats import uniform
  distributions = {'penalty': ['l1', 'l2'], 'C': uniform(loc=0, scale=100)}
#+end_src

Let's take a closer look at each distribution:

    - The ~penalty~ hyperparameter of scikit-learn's LogisticRegression model has only two possible values: ~l1~ and ~l2~. We list them both. RandomizedSearchCV will treat this as a /discrete uniform distribution./ This just means that every item in the list has an equal chance of being selected. In this case, there's a 50% chance of drawing l1 and a 50% chance of drawing l2.

    - The hyperparameter ~C~ is the inverse of regularization strength. It can be any positive number, so we have to specify a probability distribution that allows us to randomly select a positive number. The scipy library has many probability distributions to choose from (you can see a list of them [[https://docs.scipy.org/doc/scipy/reference/stats.html][here]]). For this example, we're using the /uniform distribution/. This allow us to randomly select numbers between loc and loc + scale (in this case, between 0 and 100).

      Run the code under the setup cell to import libraries, load the breast cancer data set, and split the data into training and testing sets.

*** Task 1
Create a dictionary called distributions that contains the keys 'penalty' and 'C'. The values should be:

    - ['l1', 'l2'] for 'penalty'

    - uniform(loc=0, scale=100) for 'C'

*** Task 2
Let's do a couple of draws from the distribution for 'C' to make sure we do get numbers between 0 and 100.

    - Create an array ~first_draw~ to use .rvs() to generate 10 random variable values for 'C' from the distributions dictionary.

    - Repeat the same process but with an array named ~second_draw~.

Print ~first_draw~ and ~second_draw~ to view the outcomes of these draws!

*** Task 3
We're now ready to create a RandomizedSearchCV model. To do so:

    - First, define a logistic regression model like we've done in the previous exercises, setting solver to 'liblinear' and ~max_iter~ to 1000.

    - Initialize a random search model clf using the parameters lr, distributions, and  ~n_iter~, in that order. Set ~n_iter~ to 8 to test 8 hyperparameters combinations.

*** Script.py

#+begin_src python :results output
  import pandas as pd
  from sklearn.datasets import load_breast_cancer
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LogisticRegression
  from sklearn.model_selection import RandomizedSearchCV
  from scipy.stats import uniform

  # Load the dataset
  cancer = load_breast_cancer()

  # Split the data into training and testing sets
  X = cancer.data
  y = cancer.target
  X_train, X_test, y_train, y_test = train_test_split(X, y)

  ## Create distributions to draw hyperparameters from
  distributions = {'penalty': ['l1', 'l2'], 'C': uniform(loc=0, scale=100)}

  ## Check Distributions
  first_draw = distributions['C'].rvs(10)
  second_draw = distributions['C'].rvs(10)
  print(first_draw, second_draw)

  ## Define model and initialize random search
  # The logistic regression model
  lr = LogisticRegression(solver = 'liblinear', max_iter = 1000)

  # Create a RandomizedSearchCV model
  clf = RandomizedSearchCV(lr, distributions, n_iter=8)

#+end_src

#+RESULTS:
: [ 0.95151198 18.17832879 98.29225059 98.63309585 64.87990249 54.72965383
:  79.61273531 97.34706443  1.81621269 14.69381351] [54.68761162 27.23025496  7.84063553 57.73277895 67.43024428 33.60107592
:   2.43227937 70.81002413 17.63107877 43.62842266]

** Evaluating the Results of RandomizedSearchCV
We can now follow a similar process to what we did with GridSearchCV to evaluate the results of RandomizedSearchCV.

After fitting a RandomizedSearchCV model we can find out the results using the following attributes of the clf argument:

    - ~.best_estimator_~ gives us the best estimator

    - ~.best_score_~ gives us the mean cross-validated score corresponding to the best estimator

    - ~.best_params_~ gives us the set of hyperparameters that correspond to the best estimator

Additionally, the ~.cv_results_~ attribute gives us the scores for each hyperparameter combination in the grid. We're now ready to evaluate the random search we set up earlier.

*** Task 1
Fit the random search classifier, clf to the training data. Set the best estimator to a variable ~best_model~ and print it to see which model performed best.

Uncomment the line that prints the best parameters to check if they match up!

*** Task 2
     - Set the score corresponding to the best esimator to a variable ~best_score~ and print it.

     - Calculate the accuracy of the best estimator on the test data and store it as the variable ~test_score~.

How do the scores of the best model compare to the training and test data?

It is common to have different accuracy scores for the training and testing data sets. A small difference can be expected but a large difference may be from overfitting the training data (e.g. if the training score is significantly higher than the testing score).

*** Task 3
We have succesfully performed a random search over the two regularization options and 8 randomly chosen regularization strength values! Let's view the results of the random search and verify that our previous checkpoint gave the right values:

    - Use the attribute ~.cv_results_~ and the key argument ~'params'~ to get the list of parameters. Convert this to a pandas DataFrame named ~hyperparameters_values~.

    - Use the attribute ~.cv_results_~ and the key argument ~'mean_test_score'~ to get the list of mean test scores corresponding to the parameter values. Convert this to a single column pandas DataFrame named ~randomsearch_scores~ and set the argument columns to ['score'].

Uncomment the lines pertaining to concatenating the two data frames and print it to view the results of the grid search for each parameter pair!


*** Scritp.py

#+begin_src python :results output
  import pandas as pd
  from sklearn.datasets import load_breast_cancer
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LogisticRegression
  from sklearn.model_selection import RandomizedSearchCV
  from scipy.stats import uniform

  # Load the dataset
  cancer = load_breast_cancer()

  # Split the data into training and testing sets
  X = cancer.data
  y = cancer.target
  X_train, X_test, y_train, y_test = train_test_split(X, y)

  # Create distributions to draw hyperparameters from
  distributions = {'penalty':['l1', 'l2'], 'C':uniform(loc=0, scale=100)}

  # The logistic regression model
  lr = LogisticRegression(solver='liblinear', max_iter=1000)

  # Create a RandomizedSearchCV model
  clf = RandomizedSearchCV(lr, distributions, n_iter=8)

  # Fit clf to training data and get best hyperparameters
  clf.fit(X_train, y_train)
  best_model = clf.best_estimator_

  print(best_model)
  print(clf.best_params_)

  # Calculate training and test scores of the best estimator
  best_score = clf.best_score_
  test_score = clf.score(X_test, y_test)

  print(best_score)
  print(test_score)

  # Viewing random search results
  hyperparameter_values = pd.DataFrame(clf.cv_results_['params'])
  randomsearch_scores = pd.DataFrame(clf.cv_results_['mean_test_score'], columns = ['score'])

  df = pd.concat([hyperparameter_values, randomsearch_scores], axis = 1)
  print(df)

#+end_src

#+RESULTS:
#+begin_example
LogisticRegression(C=45.50673040064779, max_iter=1000, penalty='l1',
                   solver='liblinear')
{'C': 45.50673040064779, 'penalty': 'l1'}
0.9671135430916553
0.958041958041958
           C penalty     score
0  36.879496      l2  0.962435
1  51.729072      l2  0.964761
2  99.524521      l2  0.960082
3  45.506730      l1  0.967114
4  24.166957      l2  0.964788
5  33.082236      l1  0.962408
6   0.493598      l2  0.960082
7  17.894889      l1  0.967114
#+end_example

** Hyperparameter Tuning Review
You've learned how to tune the hyperparameters in a machine learning model! Let's go over some of the key points to take away.

    - In machine learning, hyperparameter tuning is altering the hyperparameter values of the model to get the model that's most suited to answering a question about the data.

    - Examples of hyperparameters: the learning rate, regularization penalty, number of neighbors (in a k-means model), tree depth (decision tree model), and minimum number of samples to split (decision tree model).

    - Grid Search is a method of hyperparameter tuning. It works bu testing a model on a list of hyperparameter values decided upon beforehand.

    - With a grid search, if we want to test 2 different types of regularization penalties and 3 different regularization strengths, this would give us 6 different combination of hyperparameters on our grid to test (2*3=6).

    - The Random Search method is an opposing method to grid search. In a random search model, a dictionary is given that specifies the distributions from which the model will randomly draw values for each hyperparameter. Additonally, the number of times the algorithm should randomly draw from these distributions is also specified.

    - Both GridSearchCV() and RandomSearchCV() use a technique known as *k-fold cross-validation*. This is when new combinations of the training and testing sets are created (using the same data) to conclude the most accurate model. This ensures a less biased model.

    - We used the following attributes to evaluate both the GridSearchCV() and RandomSearchCV() models; ~.best_estimator_~, ~.best_score_~, ~.best_params_~, and the ~.cv_results_~.

** Quiz

*** Question 1
Fill in the code to create a RandomizedSearchCV model called ~rand_search~
that uses the model clf and the dictionary distributions.

#+begin_src python
  clf = svm.SVC(kernel='linear')
  distributions = {'C':uniform(1, 10)}

  rand_search = RandomizedSearchCV(clf, distributions)
#+end_src

*** Question 2
Fill in the code to create a dictionary called distributions for use with RandomizedSearchCV(). Use a categorical distribution for penalty that accounts for 'l1' and 'l2' regularization. For c, use a uniform distribution between 1 and 5.

#+begin_src python
  from sklearn.model_selection import RandomizedSearchCV
  from scipy.stats import uniform

  distributions = {'penalty':['l1', 'l2'], 'C': uniform(loc=1, scale=)}

#+end_src

*** Question 3
Fill in the code that will print the estimator chosen by the random search algorithm.

#+begin_src python
  clf = RandomSearchCV(model, distributions)
  clf.fit(X_train, y_train)

  print(clf.best_estimator_)

#+end_src

*** Question 4
Below is some pseudocode involving grid search and an output.

#+begin_src python
  parameters = {'param_1': ['a', 'b', 'c'], 'param_2': [1, 10, 100]}
  model = Model()

  clf = GridSearchCV(model, parameters)
  clf.fit(X_train, y_train)

  print(clf.best_estimator_)
#+end_src

Based on the code and the output, which of the following statements is FALSE?

    - Of the hyperparameter values that were tested, the Model() model performed best with param_1='b' and 'param_2'=10.

    - Nine different hyperparameter combinations were tested: three different values for param_1 times three different values for param_2.

    - *Hyperparameters were tested by choosing random values for param_1 and param_2.*
      That's right! That would be a random search, but the code is implementing a grid search.

*** Question 5
Which of the following options best describes the grid search algorithm?

Grid search tests a machine learning algorithm on a specific list of hyperparameters.
