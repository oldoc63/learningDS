
* Implementing Regularization Methods in Python
Learn how to implement L1 and L2 regularization using scikit-learn and tune the regularization hyperparameter using ~GridSearchCV~ for linear and logistic regression.

** Implementing Regularization with Linear Regression
In the previous lesson, we saw how implementing Lasso and Ridge regularization methods with linear regression in scikit-learn was fairly straightforward. Both Lasso() and Ridge() functions have an input parameter ~alpha~ that sets the strength of the regularization. The not-so-straightforward part was the question of how to choose the rigtht alpha for the data and question at hand. We're now ready to dive into this!

For starters, we're going to run a quick Lasso implementation on the student performance dataset that we were working with. The dataset has 41 features relating to various factors that might influence a student's final grade. Let's take a sneak peek first.

We set our predictor and outcome variables and perform a train-test-split.

We're now ready to fit a Lasso regularized regression model by using the Lasso() function from the linear_model module.

We're going to look at the Mean Squared Error (MSE) using a function from the metrics module.

We see that the test error is more than 1.5 times that of the training error. This is an indicator that the model is still overfitting the data and that there is room to regularize more.

** Tuning the Regularization Hyperparameter
We chose alpha to be 0.05 (the scikit-learn default is 1.0). It is worth recalling here that an alpha of zero is equivalent to no regularization. And when alpha is allowed to get to big we have the opposite problem of biasing our model and consequently underfitting the data.

One way to figure out the "Goldilocks zone" in this bias-variance tradeoff is to try multiple values of alpha. Iterating over an array of alpha values and plotting the resulting training and test errors against the corresponding alpha's get us the following figure:

[[./iterating_alpha.png]]

We see that the test error goes down as we increase alpha until a certain value after which it begins to go up and then seems to slowly but steadily increase. Our optimal alpha thus lies somewhere in the blue band shown, likely a number between 0.12 and 0.16.

There is, however, another question to consider: would this be the same for a different train-test split? It is quite possible that our tuned hyperparameter might be different if we did a different train-test split. Luckily, scikit-learn has just the feature for us. *GridSearchCV*, short for "Grid Search Crossvalidation" helps us to satisfactorily resolve this while automating and speeding up our hyperparameter search -all within a couple of lines of code!

** Automate the Hyperparameter Search with GridsearchCV
GridSearchCV uses a "k-fold" cross-validation method to search for the optimal hyperparameter in a machine learning algorithm. The k-fold method iteratively splits the dataset into train-test splits "k" times such that every point in the sample gets to be within the test data at least once. For instance, if we wanted to do 5-fold cross-validation, our data would be split into a 80:20 train-test split five times. To visualize this, imagine that our data is contained within this green box as shown below.

Our five “folds” can be represented by the yellow box. Each location of the box shows a specific train-test split. If we iterated our model fit over these five folds, every part of the data gets to be “test data” once. Doing a k-fold cross-validation makes sure that the conclusions we’re drawing about our data are not the result of a sampling effect but are truly representative of all of the data.

GridSearchCV takes in the following arguments:

    - ~estimator~: the machine learning algorithm whose hyperparameters we're tuning (in our case, Lasso() or Ridge())

    - ~param_grid~: a grid of potential values for the hyperparameters that are being tuned. This has to be in the form of a dictionary with the keys representing the parameter inputs to the model and the values, lists of potential parameter values. For instance in the case of our Lasso implementation, we have only one hyperparameter we're tuning, i.e., alpha.

Since we don’t have an idea of what order of magnitude alpha needs to be (i.e., is it between 0.01-0.1 or 0.1-1.0 or 1-10 and so on), using NumPy‘s logspace function can be very useful. It is similar to linspace except that it gets us numbers that are evenly spaced in the logarithmic scale. (It takes in the powers of ten that we’re searching between, so if we wanted to search between 0.01 to 100, we’d set its inputs to -2 and 2 respectively.)

    - ~scoring~: the metric used to evaluate the performance of the model on the test set. GridSearchCV searches for the set of parameters within ~param_grid~ that maximizes this value.

~scikit_learn~'s [[https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter][documentation]] has an exhaustive list of scoring strategies. The argument for mean squared error, our chosen metric, is ~neg_mean_squared_error~. It's a negative value because GridSearchCV /maximizes/ the score it's given.

    - ~cv~: specifies the way the cross-validation is performed. The default here is the 5-fold cross-validation method that's described above. Otherwise one can set a different number of folds (i.e., an alternate positive integer value for k)
    - ~return_train_score~: a Boolean argument specifying whether we would like the output of our fit to return the scores on training data every folds. The default here is False but we're going to set it to True as we're using GridSearchCV to correct for overfitting and would like to be kept in the loop about the training score.

Consider our implementation of Lasso on the student performance dataset. Suppose we wanted to check the result of implementing different values of alpha and wanted to do multiple train-test splits to make sure we're covering the entire sample. We could do this in one line.

The result of the fit object has many attributes, of which we're going to examine the ones that are most important to us:

    1. The ~cv_results_~ object gives us the details of every model fit corresponding to a particular alpha value and the train-test split of each fold.(Our alpha array had 100 values and we did a 5-fold cross-validated search - so this is essentially equivalent to performing 500 model fits!) We’re specifically going to look at the mean train and test scores across the 5 train-test splits. Each of the objects is an array the size of our param_grid, i.e., the number of alpha values we’ve specified.

    2. Additionally, the model fit object also give us: ~best_estimator_~, ~best_score_~ and ~best_params_~. We can get the alpha that is optimal to our scoring strategy.

(Note that the score is the negative of least test mean squared error!) If we were to replicate the previous plot, i.e., plot our new training and test scores as a function of alpha and over plot the line corresponding to the “best” alpha value, we get the following:

[[./Lasso_Grid.png]]

The blue line here is our tuned hyperparameter and this value of alpha (~0.1233) corresponds to the optimal cross-validated test and training error values.

** Script.py

#+begin_src python :results output
  import pandas as pd

  #sneak peek at the data
  df = pd.read_csv('student_math.csv')
  print(df.columns, df.shape)

  #set predictors and outcome variables
  y = df['Final_Grade']
  X = df.drop(columns = ['Final_Grade'])

  #perform a train-test-split
  from sklearn.model_selection import train_test_split

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

  #fit a Lasso regularized regression model
  from sklearn.linear_model import Lasso

  lasso = Lasso(alpha = 0.05)
  lasso.fit(X_train, y_train)

  #Mean Squared Error (MSE)
  from sklearn.metrics import mean_squared_error

  pred_train = lasso.predict(X_train)
  pred_test = lasso.predict(X_test)
  training_mse = mean_squared_error(y_train, pred_train)
  test_mse = mean_squared_error(y_test, pred_test)
  print('Training Error: ', training_mse)
  print('Test Error: ', test_mse)

  #Potential values for alpha are an array between 0.000001 and 1.0
  import numpy as np

  alpha_array = np.logspace(-6, 0, 100)

  #dict with key (alpha) and values being alpha_array
  tuned_parameters = [{'alpha': alpha_array}]

  #implementing different values of alpha and multiple train-test splits
  from sklearn.model_selection import GridSearchCV

  model = GridSearchCV(estimator=Lasso(),
       param_grid = tuned_parameters, scoring =  'neg_mean_squared_error', cv = 5, return_train_score = True)
  model.fit(X, y)

  #mean train and test scores across the 5 train-test splits
  test_scores = model.cv_results_['mean_test_score']
  train_scores = model.cv_results_['mean_train_score']

  #alpha that is optimal to our scoring strategy
  print(model.best_params_, model.best_score_)

#+end_src

#+RESULTS:
#+begin_example
Index(['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel',
       'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2',
       'Final_Grade', 'school_MS', 'sex_M', 'address_U', 'famsize_LE3',
       'Pstatus_T', 'Mjob_health', 'Mjob_other', 'Mjob_services',
       'Mjob_teacher', 'Fjob_health', 'Fjob_other', 'Fjob_services',
       'Fjob_teacher', 'reason_home', 'reason_other', 'reason_reputation',
       'guardian_mother', 'guardian_other', 'schoolsup_yes', 'famsup_yes',
       'paid_yes', 'activities_yes', 'nursery_yes', 'higher_yes',
       'internet_yes', 'romantic_yes'],
      dtype='object') (395, 42)
Training Error:  2.8132075838851405
Test Error:  4.474769444129441
{'alpha': 0.12328467394420659} -3.743294901559115
#+end_example
