
* Wrapper Methods
In this project, you'll analyze data from a survey conducted by Fabio Mendoza Palechor and Alexis de la Hoz Manotas that asked people about their eating habits and weight. The data was obtained from the UCI Machine Learning Repository. Categorical variables were changed to numerical ones in order to facilitate analysis.

First, you'll fit a logistic regression model to try to predict whether survey respondents are obese based on their answers to questions in the survey. After that, you'll use three different wrapper methods to choose a smaller feature subset.

You'll use sequential forward selection, sequential backward floating selection, and recursive feature elimination. After implementing each wrapper method, you'll evaluate the model accuracy on the resulting smaller feature subsets and compare that with the model accuracy using all available features.

** Evaluating a Logistic Regression Model
The data set obesity contains 18 predictor variables. Here's a brief description of them.

    - Gender is 1 if a respondent is male and 0 if a respondent is female.
    - Age is a respondent's age in years.
    - ~family_history_with_overweight~ is 1 if a respondent has family member who is or was overweight, 0 if not.
    - FAVC is 1 if a respondent eats high caloric food frequently, 0 if not.
    - FCVC is 1 if a respondent usually eats vegetables in their meals, 0 if not.
    - NCP represents how many main meals a respondent has daily (0 for 1-2 meals, 1 for 3 meals, and 2 for more than 3 meals).
    - CAEC represents how much food a respondent eats between meals on a scale of 0 to 3.
    - SMOKE is 1 if a respondent smokes, 0 if not.
    - CH2O represents how much water a respondent drinks on a scale of 0 to 2.
    - SCC is 1 if a respondent monitors their caloric intake, 0 if not.
    - FAF represents how much physical activity a respondent does on a scale of 0 to 3.
    - TUE represents how much time a respondent spends looking at devices with screens on a scale of 0 to 2.
    - CALC represents how often a respondent drinks alcohol on a scale of 0 to 3.
    - Automobile, Bike, Motorbike, Public_Transportation, and Walking indicate a respondent's primary mode of transportation. Their primary mode of transportation is indicated by a 1 and the other columns will contain a 0.

The outcome variable ~NObeyesdad~, is 1 if a patient is obese and a 0 if not.

Use the .head() method and inspect the data.

*** Split the data into X and y
In order to use a linear regression model, you'll need to split the data into two parts: the predictor variables and an outcome variable. Do this by splitting the data into a DataFrame of predictor variables called ~X~ and a Series of outcome variables ~y~.

*** Logistic regression model
Create a logistic regresion model called lr. Include the parameter ~max_iter=1000~ to make sure that the model will converge when you try to fit it.

Logistic regression models give a probability that an observation belongs to a category. In the obesity dataset, probabilities greater than 0.5 are considered predictions that the subject will be obese.

*** Fit the model
Use the .fit() method on lr to fit the model to X and y.

*** Model accuracy
A model's accuracy is the proportion of classes that the model correctly predicts. Compute and print the accuracy of lr by using the ~.score()~ method. What percentage of respondents did the model correctly predict as being either obese or not obese?

** Sequential Forward Selection
Sequential forward selection is a wrapper method that builds a feature set by starting with no features and then adding one feature at a time until a desired number of features is reached. In the first step, the algorithm will train and test a model using only one feature at a time. The algorithm keeps the feature that performs best.

In each subsequent step, the algorithm will test the model on each possible new feature addition. Whichever feature improves model performance the most is then added to the feature subset. This process stops once we have the desided number of features.

Sequential forward selection is a greedy algorithm: instead of checking every possible feature set by brute force, it add whichever feature gives the best inmediate performance gain.

Now that you've created a logistic regression model and evaluated its performance, you're ready to do some feature selection.

Our model was 0.7664613927048792 accurate at predicting obesity with all the features. Is it possible that the model could make accurate predictions with fewer features? That would make the model easier to understand, and it could simplify diagnosis.

We will use the SFS class form Python's mlxtend library to implement sequential forward selection and choose a subset of nine features for the logistic regression model.

Create a sequential forward selection model called sfs.

    - Be sure to set the estimator parameter to lr and set the forward and floating parameters to the appropriate values.
    - Also use the parameters ~k_features=9~, scoring='accuracy', and cv=0.
    - ~cv~ allows you to do k-fold cross-validation. We'll leave it at 0 and only evaluate performance on the training set.

*** Fit the model
Use the .fit() method on sfs to fit the model to X and y. This step will take some time (not more than a minute) to run.

*** Inspect the results
The sfs object that you fit in the previous exercise contains information about the sequential forward selection that was applied to your feature set. The subsets_ atribute allows you to see much of that information, including which feature was chosen at each step and the model's accuracy after each feature addition.

The keys in this dictionary are the numbers of features at each step in the sequential forward selection algorithm. The values in the dictionary are dictionaries with information about the feature set at each step. 'avg_score' is the accuracy of the model with the specified number of features.

In this particular example, the model had an accuracy of about 63.5% after family_history_with_overweight was added.

Now that you've run the sequential forward selection algorithm on the logistic regression model with X and y you can see what features were chosen and check the model accuracy on the smaller feature set. Print sfs.subsets_[9] to inspect the results of sequential forward selection.

*** Chosen features and model accuracy
Use the dictionary `sfs.subsets_[9]` to print a tuple of chosen feature names. Then use it to print the accuracy of the model after doing sequential forward selection. How does this compare to the model's accuracy on all available features?

*** Visualize model accuracy
It can be helpful to visualize the results of sequential forward selection and see how accuracy is affected as each feature is added.

The mlxtend library also makes it easy to visualize how the accuracy of a model changes as sequential forward selection adds features. Use the code `plot_sfs(sfs.get_metric_dict())` to plot the model accuracy as a function of the number of features used. Make sure to show your plot as well.

** Sequential Backward Selection
Sequential forward selection was able to find a feature subset that performed marginally better than the full feature set. Let's use a different sequential method and see how it compares.

Sequential backward selection is another wrapper method for feature selection. It is very similar to sequential forward selection, but there is one key difference. Instead of starting with no features and adding one feature at a time, sequential backward selection starts with all of the available features and removes one feature at a time. The algorithm will stop when it arrives at the desired number of features.

To implement sequential backward selection in mlxtend you can use the same SFS class you used for sequential forward selection. The only difference is that you have to set the parameter forward to False.

Create a sequential backward selection model called sbs.

    - Be sure to set the estimator parameter to lr and set the forward and floating parameters to the appropriate values.
    - Also use the parameters k_features=7, scoring='accuracy', and cv=0.

*** Fit the model
Use the .fit() method on sbs to fit the model to X and y.

*** Inspect the results
Now that you've run the sequential backward selection algorithm on the logistic regression model with `X` and `y` you can see what features were chosen and check the model accuracy on the smaller feature set. Print `sbs.subsets_[7]` to inspect the results of sequential backward selection.

*** Chosen features and model accuracy
Use the dictionary `sbs.subsets_[7]` to print a tuple of chosen feature names. Then use it to print the accuracy of the model after doing sequential backward selection. How does this compare to the model's accuracy on all available features?

*** Visualize model accuracy
You can visualize the results of sequential backward floating selection just as you did with sequential forward selection. Use the code `plot_sfs(sbs.get_metric_dict())` to plot the model accuracy as a function of the number of features used.

** Recursive Feature Elimination
So far you've tried two different sequential feature selection methods. Let's try one more: recursive feature elimination.

Recursive feature elimination starts by training a model with all available features. It then ranks each feature according to an importance metric and removes the least important feature. The algorithm then trains the model on the smaller feature set, ranks those features, and removes the least important one. The process stops when the desired number of features is reached.

In regression problems, features are ranked by the size of the absolute value of their coefficients.

It's important to note that you might need to standardize data before doing recursive feature elimination. In regression problems in particular, it's necessary to standardize data so that the scale of features doesn't affect the size of the coefficients.

Note that recursive feature elimination is different from sequential backward selection. Sequential backward selection removes features by training a model on a collection of subsets (one for each possible feature removal) and greedily proceeeding with whatever subset performs best. Recursive feature elimination, on the other hand, only trains a model on one feature subset before deciding which feature to remove next.

This is one advantage of recursive feature elimination. Since it only needs to train and test a model on one feature subset per feature removal, it can be much faster than the sequential selection methods that we've covered.

First you'll standardize the data, then you'll fit the RFE model and inspect the results.

At a later step of this project, you'll need to be able to access feature names. Enter the code features = X.columns for use later.

*** Standardize the data
Before doing applying recursive feature elimination it is necessary to standardize the data. Standardize X and save it as a DataFrame by creating a StandardScaler() object and using the .fit_transform() method.

*** Recursive feature elimination model
Create a RFE() object that selects 8 features. Be sure to set the stimator parameter to lr.

*** Fit the model
Fit the recursive feature elimination model to X and y.

*** Inspect chosen features
Now that you've fit the RFE model you can evaluate the results. Create a list of chosen feature names and call it ~rfe_features~. You can use a list comprehension and filter the features in ~zip(features, rfe.support_)~ based on whether their support is ~True~ (meaning the model kept them) or ~False~ (meaning the model eliminated them).

*** Model Accuracy
Use the .score() method on rfe and print the model accuracy after doing recursive feature elimination. How does this compare to the model's accuracy on all available features?

* SFS.py

#+begin_src python :results output
  import pandas as pd
  from sklearn.linear_model import LogisticRegression
  from mlxtend.feature_selection import SequentialFeatureSelector as SFS
  from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs
  import matplotlib.pyplot as plt
  from sklearn.preprocessing import StandardScaler
  from sklearn.feature_selection import RFE

  # https://archive.ics.uci.edu/ml/datasets/Estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition+

  #Load the data
  obesity = pd.read_csv("obesity.csv")

  #Inspect the data
  print(obesity.head())

  #Split the data into X and y
  X = obesity.drop(['NObeyesdad'], axis=1)
  y = obesity['NObeyesdad']

  #Create a logistic regression model called lr
  lr = LogisticRegression(max_iter=1000)

  #Fit the model
  lr.fit(X, y)

  #Compute and print the accuracy
  print(lr.score(X, y))

  #Set up SFS parameters
  sfs = SFS(lr,
            k_features=9,
            forward=True,
            floating=False,
            scoring='accuracy',
            cv=0)

  #Fit the model
  sfs.fit(X, y)

  #Print sfs.subsets_
  #print(sfs.subsets_)

  #Print(sfs.subsets_[9])
  print(sfs.subsets_[9])

  #Print a tuple of feature names after 9 features are added
  print(sfs.subsets_[9]['feature_names'])
  print(sfs.subsets_[9]['avg_score'])

  #Plot the accuracy of the model as a function of the number of features
  plot_sfs(sfs.get_metric_dict())
  plt.show()

#+end_src

#+RESULTS:
: /home/oldoc/OpenAI/lib/python3.12/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice
:   ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
: /home/oldoc/OpenAI/lib/python3.12/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide
:   ret = ret.dtype.type(ret / rcount)
: [ Babel evaluation exited with code 0 ]

* SBS.py

#+begin_src python :results output
  import pandas as pd
  from sklearn.linear_model import LogisticRegression
  from mlxtend.feature_selection import SequentialFeatureSelector as SFS
  from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs
  import matplotlib.pyplot as plt
  from sklearn.preprocessing import StandardScaler
  from sklearn.feature_selection import RFE

  # https://archive.ics.uci.edu/ml/datasets/Estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition+

  #Load the data
  obesity = pd.read_csv("obesity.csv")

  #Inspect the data
  print(obesity.head())

  #Split the data into X and y
  X = obesity.drop(['NObeyesdad'], axis=1)
  y = obesity['NObeyesdad']

  #Create a logistic regression model called lr
  lr = LogisticRegression(max_iter=1000)

  #Fit the model
  lr.fit(X, y)

  #Compute and print the accuracy
  print(lr.score(X, y))

  ##Sequential Backward Selection
  sbs = SFS(lr,
            k_features=3,
            forward=False,
            floating=False,
            scoring='accuracy',
            cv=0)

  #Fit sbs to X and y
  sbs.fit(X, y)

  #Print(sfs.subsets_[9])
  print(sbs.subsets_[7])

  #Print a tuple of feature names after 9 features are added
  print(sbs.subsets_[9]['feature_names'])
  print(sbs.subsets_[9]['avg_score'])

  #Plot the accuracy of the model as a function of the number of features
  plot_sfs(sbs.get_metric_dict())
  plt.show()

#+end_src

#+RESULTS:
: /home/oldoc/OpenAI/lib/python3.12/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice
:   ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
: /home/oldoc/OpenAI/lib/python3.12/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide
:   ret = ret.dtype.type(ret / rcount)
: [ Babel evaluation exited with code 0 ]

* RFE.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  from sklearn.linear_model import LogisticRegression
  from sklearn.feature_selection import RFE
  from sklearn.preprocessing import StandardScaler

  # https://archive.ics.uci.edu/ml/datasets/Estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition+

  #Load the data
  obesity = pd.read_csv("obesity.csv")

  #Inspect the data
  print(obesity.head())

  #Split the data into X and y
  X = obesity.drop(['NObeyesdad'], axis=1)
  y = obesity['NObeyesdad']

  #Because you'll need to be able to acces feature names
  features = X.columns

  #Standardize the data
  X = StandardScaler().fit_transform(X)

  #Logistic Regression model
  lr = LogisticRegression(max_iter=1000)

  #Create a RFE() object that selects 8 features
  rfe = RFE(lr, n_features_to_select=8)
  rfe.fit(X, y)

  #Inspect chosen features
  rfe_features = [f for (f, support) in zip(features, rfe.support_) if support]

  print(rfe_features)

  #Print the accuracy of the model with features chosen by recursive feature elimination
  print('RFE score: ', rfe.score(X, y))

#+end_src

#+RESULTS:
#+begin_example
   Gender   Age  ...  Walking  NObeyesdad
0       0  21.0  ...        0           0
1       0  21.0  ...        0           0
2       1  23.0  ...        0           0
3       1  27.0  ...        1           0
4       1  22.0  ...        0           0

[5 rows x 19 columns]
['Age', 'family_history_with_overweight', 'FAVC', 'FCVC', 'CAEC', 'SCC', 'Automobile', 'Walking']
RFE score:  0.7678825201326386
#+end_example
