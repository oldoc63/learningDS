
* What is a Decision Tree?
Decision trees are machine learning models that try to find patterns in the features of data points. Take a look at the tree on this page. This tree tries to predict whether a student will get an A on their next test.

By asking questions like "What is the student's average grade in the class" the decision tree tries to get a better understanding of their chances on the next test.

In order to make a classification, this classifier needs a data point with four features:

    - The student's average grade in the class.

    - The number of hours the student plans on studying for the test.

   - The number of hours the student plans on sleeping the night before the test.

   - Whether or not the student plans on cheating.

For example, let's say that somebody has a "B" average in the class, studied for more than 3 hours, slept less than 5 hours before the test, and doesn't plan to cheat. If we start at the top of the tree and take the correct path based on that data, we'll arrive at a leaf node that predicts the person will not get an A on the next test.

If we're given this magic tree, it seems relatively easy to make classifications. But how do these trees get created in the first place? Decision trees are supervised machine learning models, which means that they're created from a training set of labeled data. Creating the tree is where the learning in machine learning happens.

Take a look at the gif on this page. We begin with every point in the training set at the top of the tree. These training points have labels -the red points represent students that didn't get an A on a test and the green points represent students that did get an A on a test.

We then decide to split the data into smaller groups based on a feature. For example, that feature could be something like their average grade in the class. Students with an A average would go into one set, strudents with a B average would go into another subset, and so on.

Once we have these subsets, we repeat the process -we split the data in each subset again on a different feature. Eventually, we reach a point where we decide to stop splitting the data into smaller groups. We've reached a leaf of the tree. We can now count up the labels of the data in that leaf. If an unlabeled point reaches that leaf, it will be classified as the majority label.

We can now make a tree, buy how did we know which features to split the data set with? After all, if we started by splitting the data based on the number of hours they slept the night before the test, we'd end up with a very different tree that would produce very different results. How do we know which tree is best? We'll tackle this question soon!

* Implementing a Decision Tree
To answer the questions posed in the previous exercise, we're going to do things a bit differently in this lesson and work "backwards": we're going to first fit a decision tree to a dataset and visualize this tree using scikit-learn. We're then going to systematically unpack the following: how to interpret the tree visualization, how scikit-learn's implementation works, what is gini impurity,  what are parameters and hyperparameters of the decision tree model, etc.

We’re going to use a dataset about cars with six features:

    - The price of the car, ~buying~, which can be “vhigh”, “high”, “med”, or “low”.

    - The cost of maintaining the car, ~maint~, which can be “vhigh”, “high”, “med”, or “low”.

    - The number of doors, ~doors~, which can be “2”, “3”, “4”, “5more”.
    - The number of people the car can hold, persons, which can be “2”, “4”, or “more”.

    - The size of the trunk, ~lugboot~, which can be “small”, “med”, or “big”.

    - The safety rating of the car, ~safety~, which can be “low”, “med”, or “high”.

** Task 1
We've imported the dataset in the workspace.
    - Take a look at the first five rows of the dataset by uncommenting print(df.head()) and clicking run.

    - We've created dummy features for the categorical values and set the predictor and target variables as X and y respectively. Uncomment the lines pertaining to this and press run.

    - You can examine the new set of features using print(X.columns)

How many features are there now? Do they make sense?

*Hint*
Each categorical  variable is replaced by multiple binary (0 or 1) variables. For instance, instead of the variable ‘buying’ in the original dataset, we now have ‘buying_high’, ‘buying_low’, ‘buying_med’ and ‘buying_vhigh’, each taking a value of 0 or 1 depending on which category the price of the car belongs to.

Running print(len(X.columns)) gives us the total number of features. (There are now 21 new features!)

** Task 2
We can now perform a train-test split and fit a decision tree to our training data. We'll be using scikit-learn's train_test_split function to do the split and the DecisionTreeClassifier() class to fit the data. Uncomment the lines that do the same and press Run.

** Task 3
We're now ready to visualize the decision tree! The tree module within scikit-learn has a plotting functionality that allows us to do this. Uncomment the lines relevant to this and press Run to view the tree visualization.

** Script.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt

  #Import models from scikit learn module:
  from sklearn.model_selection import train_test_split
  from sklearn.tree import DecisionTreeClassifier
  from sklearn import tree

  #Loading the dataset
  df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])

  # 1a. Take a look at the dataset
  print(df.head())

  # 2a. Setting the target and predictor variables
  df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 is not acceptable
  X = pd.get_dummies(df.iloc[:,0:6])
  y = df['accep']

  ## 1c. Examine the new features
  print(X.columns)
  print(len(X.columns))

  # 2a. Performing the train-test split
  x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)

  # 2b. Fitting the decision tree classifier
  dt = DecisionTreeClassifier(max_depth=3, ccp_alpha=0.01, criterion='gini')

  dt.fit(x_train, y_train)

  # 3.Plotting the tree
  plt.figure(figsize=(20, 12))
  tree.plot_tree(dt, feature_names = x_train.columns, max_depth=5, class_names = ['unacc', 'acc'], label='all', filled=True)
  plt.tight_layout()
  plt.show()

#+end_src

#+RESULTS:
#+begin_example
  buying  maint doors persons lug_boot safety  accep
0  vhigh  vhigh     2       2    small    low  unacc
1  vhigh  vhigh     2       2    small    med  unacc
2  vhigh  vhigh     2       2    small   high  unacc
3  vhigh  vhigh     2       2      med    low  unacc
4  vhigh  vhigh     2       2      med    med  unacc
Index(['buying_high', 'buying_low', 'buying_med', 'buying_vhigh', 'maint_high',
       'maint_low', 'maint_med', 'maint_vhigh', 'doors_2', 'doors_3',
       'doors_4', 'doors_5more', 'persons_2', 'persons_4', 'persons_more',
       'lug_boot_big', 'lug_boot_med', 'lug_boot_small', 'safety_high',
       'safety_low', 'safety_med'],
      dtype='object')
21
#+end_example

* Interpreting a Decision Tree
We're now going to examine the decision tree we built for the car dataset. The image generated by the code is the exact plot we created in the previous exercise. Two important concepts to note here are the following:

    - The root node is identified as the top of the tree. This is notated already with the number of samples and the numbers in each class (i.e. unacceptable vs. acceptable) that was used to build the tree.

    - Splits occur with True to the left, False to the right. Note the right split is a leaf node i.e., there are no more branches. Any decision ending here results in the majority class.

(Note that there is a term called gini in each of the boxes that is immensely important for how the split is done)

To interpret the tree, it's useful to keep in mind that the variables we're looking at are categorical variables that correspond to:

    - ~buying~: The price of the car which can be “vhigh”, “high”, “med”, or “low”.

    - ~maint~: The cost of maintaining the car which can be “vhigh”, “high”, “med”, or “low”.

    - ~doors~: The number of doors which can be “2”, “3”, “4”, “5more”.

    - ~persons~: The number of people the car can hold which can be “2”, “4”, or “more”.

    - ~logboot~: The size of the trunk which can be “small”, “med”, or “big”.

    - ~safety~: The safety rating of the car which can be “low”, “med”, or “high”.

** Task 1
The root node is identified as the top of the tree. This is notated already with the number of samples and the number in each class (i.e. unacceptable vs. acceptable) that was used to build the tree. We've created a variable ~frac_acc~ to print the fraction of acceptable cars the tree was trained on. Calculate this form the root node of the tree.

*Hint*
According to the root node, the total number of samples is 1382. The number of cars that are in the class acc is 412.

** Task 2
The first split occurs off the root node based on the boolean ~safety_low <=0.5~. As this is a dummy variable, a value of 1 indicates safety='low'. Splits occur with True to the left, False to the right. Note the right split is a lef node (i.e. there are no more branches), so any decision ending here results in the majority class. What is the predicted class of a vehicle with low safety?

*Hint*
When ~safety_low is > 0.5~, we see that the tree splits off to a leaf on the right resulting in the car belonging to the unacc class. (This intuitively makes sense because we expect a car with a low safety indicator to be less acceptable!)

** Task 3
Identify the final outcome of the decision tree for the sample with ~buying_vhigh=1, persons_2=0, safety_low=0~. We've defined a variable called ~sample_class~ to fill in the correct answer.

*Hint*
Let's traverse the tree from the root node. We have ~safety_low~ = 0, so we move to the left child node. We have ~persons_2~ = 0, which means we move down one more node to the left. Now we have ~buying_vhigh~ = 1 which means we move down one level but to the right now, leaving us in class = unacc.

** Script.py

#+begin_src python :results output
  frac_acc = 412/1382
  print(f'Fraction of acceptable cars: {frac_acc}')

  low_safety_class = 'unacc'
  print(f'Cars with low safety: {low_safety_class}')

  sample_class = 'unacc'
  print(f'The classs of the sample car: {sample_class}')
#+end_src

#+RESULTS:
: Fraction of acceptable cars: 0.2981186685962373
: Cars with low safety: unacc
: The classs of the sample car: unacc

* Gini Impurity
Consider the two trees below. Which tree would be more useful as a model that tries to predict whether someone would get an A in a class?

[[./gini_impurity.png]]

Let's say you use the top tree. You'll end up at a leaf node where the label is up for debate. The training data has labels from both classes! If you use the bottom tree, you'll end up at a leaf where there's only one type of label. There is not debate at all! We'd be much more confident about our classification if we used the bottom tree.

The idea can be quantified by calculating the Gini impurity of a set of data points. For two classes (1 and 2) with probabilities $p_1$ and $p_2$ respectively, the Gini impurity is:

$$
1 - (p_1^2 + p_2^2) = 1 - (p_1^2 + (1 - p_1)^2)
$$

[[./gini_impurity_graph.png]]

The goal of a decision tree model is to separate the classes the best possible, i.e. minimize the impurity (or maximize the purity). Notice that if p_1 is 0 or 1, the Gini impurity is 0, which means there is only one class so there is perfect separation. From the graph, the Gini impurity is maximum at p_1=0.5, which means the two classes are equally balanced, so this is perfectly impure!

In general, the Gini impurity for C classes is defined as:

$$
1 - \sum_1^C p_i^2
$$

** Task 1
Using the same decision tree from the previous exercise, notice that each node is labeled with a gini=, which is the Gini impurity for the samples at that node. For the root node, calculate the Gini impurity using the formula and confirm this matches the value listed on the node.

*Hint*
Use the formula:

$$
1 - (p_1^2 + p_2^2) = 1 - (p_1^2 + (1 - p_1)^2)
$$

$$
1 - \left(\left (\frac{412}{1382} \right )^2 + \left (\frac{970}{1382} \right )^2 \right)
$$

$$
1 -  \left(\left (\frac{412}{1382} \right )^2 + \left (1 - \frac{412}{1382} \right )^2 \right)
$$

** Task 2
Confirm the Gini impurity for the bottom left node. Since this is a non-root node (either a leaf or a split node), the Gini impurity is only calculated for he samples passing through this node, not the entire dataset -hence the number of samples is 449, not 1382.

*Hint*
Use the same formula as above, but observe that the bottom left node has 449 samples with 96 acc and 353 unacc cars. (So p1 = 96/449 and so on.)

#+begin_src python :results output
  gini_root = 1 - ((412/1382)**2 + (1 - (412/1382))**2)
  print(f'Gini impurity root node : {gini_root}')

  gini_bottom_left = 1 - ((96/449)**2 + (1 - (96/449))**2)
  print(f'Gini impurity bottom left node : {gini_bottom_left}')
#+end_src

#+RESULTS:
: Gini impurity root node : 0.41848785606128835
: Gini impurity bottom left node : 0.33618880858725886

* Information Gain
We know that we want to end up with leaves with a low Gini Impurity, but we still need to figure out which features to split on in order to achieve this. To answer this question, we can calculate the /information gain/  of splitting the data on a certain feature. Information gain measures the difference in the impurity of the data before and after the split.

For example, let's start with the root node of our car acceptability tree.

The initial Gini impurity (which we confirmed previously) is 0.418. The first split occurs based on the feature ~safety_low<=0.5~, and as this is a dummy variable with values 0 and 1, this split is pushing higher safety cars to the left (912 samples) and low safety cars to the right (470 samples). Before we discuss how we decided to split on this feature, let's calculate the information again.

The new Gini impurities for these two split nodes are 0.495 and 0 (which is a pure leaf node). All together, the now weighted Gini impurity after the split is:

$$
912/1382*(.495) + 470/1382*(0) = 0.3267
$$

Not bad! (Remember we want our Gini impurity to be lower!) This is lower than our initial Gini impurity, so by splitting the data in that way, we've gained some information about how the data is structured -the datasets after the split are purer than they were before the split.

Then the information gain (or reduction in impurity after the split) is:

$$
0.4185 - 0.3267 = 0.0918
$$

The higher the information gain the better -if information gain is 0, then splitting the data on that feature was useless!

** Task 1
Verify splitting on a pure node will result in an information gain of zero. For example, with the right-hand-side split of the root node, ~safety_low<=0.5~ is false, consider a further split of this node.

The result will be each subsequent split will still have Gini impurity of zero. Fill this in the code editor and uncomment the relevant lines to calculate the gini information gain.

** Task 2
Calculate the information gain on the next split, on ~persons_2~ in the tree but reading the following:

   - The number of samples off the tree to input the split ratio (~r_persons_2~)

   - The gini values at the child nodes (~gini_left_node~ and ~gini_right_node~) Input these, uncomment the relevant lines of code and press Run to obtain the information gain at this split.
** Script.py

#+begin_src python :results output
  #1. Information gain at a pure node (i.e., node with no more branches)
  r = 0.5 #ratio of new split, could be anything
  gini_pure_node = 0
  gini_info_gain = r*gini_pure_node  + (1-r)*gini_pure_node
  print(f'Gini information gain pure node split safety_low >= .5 : {gini_info_gain}')

  #2. Information gain at the 'persons_2 split'
  r_persons_2 =  604/912 #read ratio of the split from the tree
  gini_left_split = 0.434

  gini_right_split = 0

  initial_gini_persons_2 = 0.495

  weighted_gini_persons_2 = r_persons_2 * gini_left_split + (1-r_persons_2) * gini_right_split

  gini_info_gain_persons_2 = initial_gini_persons_2 - weighted_gini_persons_2

  print(f'Gini information gain node persons_2 : {gini_info_gain_persons_2}')

#+end_src

#+RESULTS:
: Gini information gain pure node split safety_low >= .5 : 0.0
: Gini information gain node persons_2 : 0.20757017543859652

* How a decision tree is built (feature split)
We're ready to understand how the decision tree was built by scikit-learn. To recap:

    - The root node in the tree we've been using so far is split on the feature ~safety_low~. When its value is 1, this corresponds to the right split (vehicles with low safety) and when its value is 0, this corresponds to the left split.

    - Information gain is the difference in the weighted gini impurity before and after performing a split at a node. We saw in the previous exercise that information gain for ~safety_low~ as root node was 0.418 - 0.3267 = 0.0918.

We now consider an important question: *How does one know that this is the best node to split on?!* To figure this out we're going to go through the process of calculating information gain for other possible root node choices and calculate the information gain values for each of these. This is precisely what is going on under the hood when one runs a DecisionTreeClassifier() in scikit-learn. By checking information gain values of all possible options at any given split, the algorithm decide on the best feature to split on at every node.

For starters, we will consider a different feature we could have split on first: ~persons_2~. Recall that ~persons_2~ can take a binary value of 0 or 1 as well. Setting ~persons_2~ as the root node means that:

    - the left split will contain data corresponding to a ~persons_2~ value <0.5.

    - the right split will contain data corresponding to a ~persons_2~ value >0.5.

We've defined two functions in the code editor to help us calculate gini impurity (gini) and information gain (~info_gain~) in the code editor. Read through the functions to see if they reflect the formulas we've covered thusfar. Using these functions, we're going to calculate the information gain from splitting on ~persons_2~. We can then follow this procedure for all other possible root node choices and truly check if ~safety_low~ is indeed our best choice for this!

** Task 1
Create two DataFrames left and right that represent the ~y_train~ values that correspond to ~x_train['persons_2']~ being 0 and 1 respectively. Calculate the length of these DataFrames, store them as ~len_left~ and ~len_right~, and print them.

** Task 2
We're now going to calculate the gini impurities corresponding to the overall training data and the left and right split. To do so:

    - Uncomment the line pertaining to calculating the gini impurity in the overall training data.

    - For the gini value of the left node, create a variable ~gini_left~ and use the gini function to calculate the value.

    - For the gini value of the right node, create a variable ~gini_right~ and use the gini function to calculate the value.

** Task 3
Before proceeding to calculate the information gain a this split, let's consolidate what we've calculated in the previous checkpoints:

    - There are 917 cars with a persons_2 value of 0 and 465 cars with ~persons_2~ value of 1.

    - The overall gini impurity of the training data is 0.4185. The gini impurity for the left split was 0.4949 and the gini impurity of the right split is 0.

  This means that the /weighted impurity/ of this split is:

  #+begin_src python
  917/1382 (0.4949) + 465/1382 (0) = 0.3284
  #+end_src

  The /information gain/ for tree whose root node is ~persons_2~ should be:

  #+begin_src python
  0.4185 - 0.3284 =  0.0901
  #+end_src

Use the ~info_gain~ function to calculate the information gain corresponding to this split and store it as ~info_gain_persons_2~. Print it to check if it is indeed the expected value!

** Task 4
We've now verified that splitting at a root node of persons_2 gives us a lesser information gain than splitting at safety_low (0.0901 in comparison to 0.0918). Verify the information gain is the highgest at the root node using the function info_gain and looping through ALL the features. Uncomment the lines that pertain to this calculation to verify if the tree we've been working with so far has the best possible root node.

** Script.py
#+begin_src python :results output
  # The usual libraries, loading the dataset and performing the train-test split.
  import pandas as pd
  from sklearn.model_selection import train_test_split

  df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])

  df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 is not acceptable

  X = pd.get_dummies(df.iloc[:,0:6])
  y = df['accep']

  x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)

  ## Functions to calculate gini impurity and information gain

  def gini(data):
      """calculate the Gini Impurity
      """
      data = pd.Series(data)
      return 1 - sum(data.value_counts(normalize=True)**2)

  def info_gain(left, right, current_impurity):
      """Information Gain associated with creating a node/split data.Input: left, right are data in left branch, right branch, respectively
      current_impurity is the data impurity before splitting into left, right branches
      """
      # weight for gini score of the left branch
      w = float(len(left)) / (len(left) + len(right))
      return current_impurity - w * gini(left) - (1 - w) * gini(right)

  #### ------------------------------------------------

  ## 1. Calculate sample sizes for a split on persons_2
  left = y_train[x_train['persons_2']==0]
  right = y_train[x_train['persons_2']==1]
  len_left = len(left)
  len_right = len(right)
  print ('No. of cars with persons_2 == 0:', len_left)
  print ('No. of cars with persons_2 == 1:', len_right)

  ## 2. Gini impurity calculations
  gi = gini(y_train)
  gini_left = gini(left)
  gini_right = gini(right)

  print(f'Original gini impurity (without splitting): {gi}')
  print(f'Left split gini impurity: {gini_left}')
  print(f'Right split gini impurity: {gini_right}')

  ## 3. Information gain when using feature persons_2
  info_gain_persons_2 = info_gain(left, right, gi)

  print(f'Information gain for persons_2:', info_gain_persons_2)

  ## 4. Which feature split maximizes information gain?
  info_gain_list = []
  for i in x_train.columns:
      left = y_train[x_train[i]==0]
      right = y_train[x_train[i]==1]
      info_gain_list.append([i, info_gain(left, right, gi)])

  info_gain_table = pd.DataFrame(info_gain_list).sort_values(1,ascending=False)
  print(f'Greatest impurity gain at:{info_gain_table.iloc[0,:]}')
  print(info_gain_table)

#+end_src

#+RESULTS:
#+begin_example
No. of cars with persons_2 == 0: 917
No. of cars with persons_2 == 1: 465
Original gini impurity (without splitting): 0.41848785606128835
Left split gini impurity: 0.49485722848081015
Right split gini impurity: 0.0
Information gain for persons_2: 0.09013468781461476
Greatest impurity gain at:0    safety_low
1      0.091603
Name: 19, dtype: object
                 0         1
19      safety_low  0.091603
12       persons_2  0.090135
18     safety_high  0.045116
14    persons_more  0.025261
13       persons_4  0.020254
7      maint_vhigh  0.013622
3     buying_vhigh  0.011001
20      safety_med  0.008480
17  lug_boot_small  0.006758
1       buying_low  0.006519
5        maint_low  0.005343
6        maint_med  0.004197
15    lug_boot_big  0.003913
2       buying_med  0.003338
8          doors_2  0.002021
0      buying_high  0.001094
4       maint_high  0.000530
10         doors_4  0.000423
16    lug_boot_med  0.000386
11     doors_5more  0.000325
9          doors_3  0.000036
#+end_example

* How a decision tree is built (recursion)
Now that we can find the best feature to split the dataset, we can repeat this process again and again to create the full tree. This is a recursive algorithm! We start with every data point from the training set, find the best feature to split the data, split the data based on that feature, and then recusively repeat the process again on each subset that was created from the split.

We'll stop the recursion when we can no longer find a feature that results in any information gain. In other words, we want to create a leaf of the tree when we can't find a way to split the data that makes purer subsets.

** Task 1
Update the variables y_train_sub and x_train_sub with a logical filter to represent the data at the left node after the root node split. Note, this will include only data that has been split at the root node to the left, i.e. safety_low<=.5.

** Task 2
Use the previously defined functions gini and loop for info_gain_list to verify the next split after safety_low<=.5 will be on person_2.

** Script.py
#+begin_src python :results output
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import seaborn as sns
  from sklearn.model_selection import train_test_split
  from sklearn.tree import DecisionTreeClassifier

  def gini(data):
      """calculate the Gini Impurity
      """
      data = pd.Series(data)
      return 1 - sum(data.value_counts(normalize=True)**2)

  def info_gain(left, right, current_impurity):
      """Information Gain associated with creating a node/split data. Input: left, right are data in left branch, right branch, respectively. current_impurity is the data impurity before splitting into left, right branches.
      """
      # weight for gini score of the left branch
      w = float(len(left)) / (len(left) + len(right))
      return current_impurity - w * gini(left) - (1 - w) * gini(right)

  df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])

  df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable

  X = pd.get_dummies(df.iloc[:,0:6])
  y = df['accep']

  x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.2)

  y_train_sub = y_train[x_train['safety_low']==0]
  x_train_sub = x_train[x_train['safety_low']==0]

  gi = gini(y_train_sub)
  print(f'Gini impurity at root: {gi}')

  info_gain_list = []
  for i in x_train.columns:
      left = y_train_sub[x_train_sub[i]==0]
      right = y_train_sub[x_train_sub[i]==1]
      info_gain_list.append([i, info_gain(left, right, gi)])

  info_gain_table = pd.DataFrame(info_gain_list).sort_values(1, ascending=False)
  print(f'Greatest impurity gain at:{info_gain_table.iloc[0,:]}')

#+end_src

#+RESULTS:
: Gini impurity at root: 0.49534472145275465
: Greatest impurity gain at:0    persons_2
: 1     0.208137
: Name: 12, dtype: object

* Train and Predict using scikit-learn
Now we'll finally build a decision tree ourselves! We will use scikit-learn's tree module to create, train, predict, and visualize a decision tree classifier. The syntax is the same as other models in scikit-learn, so it should look very familiar. First, an instance of the model class is instantiated with DecisionTreeClassifier(). To use non-default hyperparameter values, you can pass them at this stage, such as ~DecisionTreeClassifier(max_depth=5)~.

Then .fit() takes a list of data points followed by a list of the labels associated with that data and builds the decision tree model.

Finally, once we've made our tree, we can use it to classify new data points. The .predict() method takes an array of data points and will return an array of classifications for those data points. ~predic_proba()~ can also be used to return class probabilities instead. Last, .score() can be used to generate the accuracy score for a new set of data and labels.

As with other sklearn models, only numeric data can be used (categorical variables and nulls must be handled prior to model fitting).

** Task 1
Create a decision tree classification model defined as dtree with default parameters. Print the parameters of the tree using ~.get_params()~ method.

** Task 2
Fit dtree using the training data set and labels. After the tree is fit, print the depth of the tree using ~.get_depth()~.

** Task 3
Predict the classes of the test data set (~x_test~) and save this as an array ~y_pred~. Print the accuracy of model on the test set (either using .score() or ~accuracy_score()~).

** Script.py
#+begin_src python :results output
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import seaborn as sns
  from sklearn.model_selection import train_test_split
  from sklearn.tree import DecisionTreeClassifier
  from sklearn import tree
  from sklearn.metrics import accuracy_score

  df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])

  df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable

  X = pd.get_dummies(df.iloc[:,0:6])
  y = df['accep']

  x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)

  ## 1. Create a decision tree and print the parameters
  dtree = DecisionTreeClassifier()
  print(f'Decision Tree Parameters: ', dtree.get_params())

  ## 2. Fit decision tree on training set and print the depth of the tree
  dtree.fit(x_train, y_train)
  print(f'Decision Tree Depth: {dtree.get_depth()}')

  ## 3. Predict on test data and accuracy of model on test set
  y_pred = dtree.predict(x_test)
  print(f'Test set accuracy: {dtree.score(x_test, y_test)}')

#+end_src

#+RESULTS:
: Decision Tree Parameters:  {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}
: Decision Tree Depth: 11
: Test set accuracy: 0.9826589595375722

* Visualizing Decision Trees
We built a decision tree using scikit-learn and predicted new values with it. But what does the tree look like? What features are used to split? Two methods using only scikit-learn/matplotlib can help visualize the tree, the first using tree_plot, the second listing the rules as text. There are other libraries available with more advanced visualization (graphviz and dtreeviz, for example, but may require additional instalation and won't be covered here).

** Task 1
Press Run on the existing code and inspect the current output of ~tree.plot_tree()~ (Use the arrows in the browser to zoom out for a closer wiew). What seems to be missing? The features are labeled only by index rather than name. Pass the additional parameters ~feature_names~ = ~x_train.columns~, ~class_names~ = ['unacc', 'acc'], filled=True to the plotting function and note the differences.

** Task 2
Maybe you want a more simplistic method of visualizing the tree in a text-based fashion. This can be done by printing the result of ~tree.export_text()~. Fix the same issue as above by passing the parameter feature_names with a list of the columns to print out the names rather than index.

** Script.py
#+begin_src python :results output
  import pandas as pd
  import matplotlib.pyplot as plt
  from sklearn.model_selection import train_test_split
  from sklearn.tree import DecisionTreeClassifier
  from sklearn import tree

  ## Loading the data and setting target and predictor variables
  df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])
  df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable
  X = pd.get_dummies(df.iloc[:,0:6])
  y = df['accep']

  ## Train-test split and fitting the tree
  x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.3)
  dtree = DecisionTreeClassifier(max_depth=3)
  dtree.fit(x_train, y_train)

  ## Visualizing the tree
  plt.figure(figsize=(27,12))
  tree.plot_tree(dtree, feature_names = x_train.columns, class_names = ['unacc','acc'], filled=True)
  plt.tight_layout()
  plt.show()

  ## Text-based visualization of the tree (View this in the Output terminal!)
  print(tree.export_text(dtree, feature_names = x_train.columns.tolist()))

#+end_src

#+RESULTS:
#+begin_example
|--- safety_low <= 0.50
|   |--- persons_2 <= 0.50
|   |   |--- maint_vhigh <= 0.50
|   |   |   |--- class: True
|   |   |--- maint_vhigh >  0.50
|   |   |   |--- class: False
|   |--- persons_2 >  0.50
|   |   |--- class: False
|--- safety_low >  0.50
|   |--- class: False

#+end_example

* Advantages and Disadvantages
As we have seen already, decision trees are easy to understand, fully explainable, and have a natural way to visualize the decision making process. In addition, often little modification needs to be made to the data prior to modeling (such as scaling, normalization, removing outliers) and decision trees are relatively quick to train and predict. However, now let's talk about some of their limitations.

One problem with the way we're currently making our decision trees is that our trees aren't always /globally optimal./ This means that there might be a better tree out there somewhere that produces better results. But wait, why did we go through all that work of finding information gain if it's not producing the best possible tree?

Our current strategy of creating trees is greedy. We assume that the best way to create a tree is to find the feature that will result in the largest information gain right now and split on that feature. We never consider the ramifications of that split further down the tree. It's possible that if we split on a suboptimal feature right now, we would find even better splits later on. Unfortunately, finding a globally optimal tree is an extremely difficult task, and finding a tree using our greedy approach is a reasonable substitute.

Another problem wiht our trees is that they are prone to /overfit/ the data. This means that the structure of the tree is too dependent on the training data and may not generalize well to new data. In general, larger trees tend to overfit the data more. As the tree gets bigger, it becomes more tuned to the training data and it loses a more generalized understanding of the real world data.

** Task 1
Fit two decision trees to the dataset we've been working with:

    - dtree1 with no restriction on the max_depth

    - dtree2 with a ~max_depth~ of 7, on the training data

Print the ~max_depth~ of each tree.

** Task 2
Evaluate the accuracy of each tree on the test set.

Does the tree with the greater depth or the tree with the lower depth have a higher accuracy?

Remember, a larger depth does not always mean better accuracy, as it can lead to overfitting.

** Script.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  from sklearn.model_selection import train_test_split
  from sklearn.tree import DecisionTreeClassifier
  from sklearn import tree

  df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])
  df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable

  X = pd.get_dummies(df.iloc[:,0:6])
  y = df['accep']

  x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.3)

  ## 1. Two decision trees
  dtree1 = DecisionTreeClassifier()
  dtree1.fit(x_train, y_train)
  dtree1_depth = dtree1.get_depth()
  print(dtree1_depth)

  dtree2 = DecisionTreeClassifier(max_depth=7)
  dtree2.fit(x_train, y_train)
  dtree2_depth = dtree2.get_depth()
  print(dtree2_depth)

  ## 2. Calculate accuracy scores on test data for both trees
  dtree1_score = dtree1.score(x_test, y_test)
  print(f'Test set accuracy tree no max depth: {dtree1_score}') # or accuracy_score(y_test, y_pred)

  dtree2_score = dtree2.score(x_test, y_test)
  print(f'Test set accuracy tree max depth 7: {dtree2_score}') # or accuracy_score(y_test, y_pred)

#+end_src

#+RESULTS:
: 11
: 7
: Test set accuracy tree no max depth: 0.9788053949903661
: Test set accuracy tree max depth 7: 0.9614643545279383
