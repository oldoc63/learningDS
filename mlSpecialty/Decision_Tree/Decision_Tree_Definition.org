
* What is a Decision Tree?
Decision trees are machine learning models that try to find patterns in the features of data points. Take a look at the tree on this page. This tree tries to predict whether a student will get an A on their next test.

By asking questions like "What is the student's average grade in the class" the decision tree tries to get a better understanding of their chances on the next test.

In order to make a classification, this classifier needs a data point with four features:

    - The student's average grade in the class.

    - The number of hours the student plans on studying for the test.

   - The number of hours the student plans on sleeping the night before the test.

   - Whether or not the student plans on cheating.

For example, let's say that somebody has a "B" average in the class, studied for more than 3 hours, slept less than 5 hours before the test, and doesn't plan to cheat. If we start at the top of the tree and take the correct path based on that data, we'll arrive at a leaf node that predicts the person will not get an A on the next test.

If we're given this magic tree, it seems relatively easy to make classifications. But how do these trees get created in the first place? Decision trees are supervised machine learning models, which means that they're created from a training set of labeled data. Creating the tree is where the learning in machine learning happens.

Take a look at the gif on this page. We begin with every point in the training set at the top of the tree. These training points have labels -the red points represent students that didn't get an A on a test and the green points represent students that did get an A on a test.

We then decide to split the data into smaller groups based on a feature. For example, that feature could be something like their average grade in the class. Students with an A average would go into one set, strudents with a B average would go into another subset, and so on.

Once we have these subsets, we repeat the process -we split the data in each subset again on a different feature. Eventually, we reach a point where we decide to stop splitting the data into smaller groups. We've reached a leaf of the tree. We can now count up the labels of the data in that leaf. If an unlabeled point reaches that leaf, it will be classified as the majority label.

We can now make a tree, buy how did we know which features to split the data set with? After all, if we started by splitting the data based on the number of hours they slept the night before the test, we'd end up with a very different tree that would produce very different results. How do we know which tree is best? We'll tackle this question soon!

* Implementing a Decision Tree
To answer the questions posed in the previous exercise, we're going to do things a bit differently in this lesson and work "backwards": we're going to first fit a decision tree to a dataset and visualize this tree using scikit-learn. We're then going to systematically unpack the following: how to interpret the tree visualization, how scikit-learn's implementation works, what is gini impurity,  what are parameters and hyperparameters of the decision tree model, etc.

We’re going to use a dataset about cars with six features:

    - The price of the car, ~buying~, which can be “vhigh”, “high”, “med”, or “low”.

    - The cost of maintaining the car, ~maint~, which can be “vhigh”, “high”, “med”, or “low”.

    - The number of doors, ~doors~, which can be “2”, “3”, “4”, “5more”.
    - The number of people the car can hold, persons, which can be “2”, “4”, or “more”.

    - The size of the trunk, ~lugboot~, which can be “small”, “med”, or “big”.

    - The safety rating of the car, ~safety~, which can be “low”, “med”, or “high”.

** Task 1
We've imported the dataset in the workspace.
    - Take a look at the first five rows of the dataset by uncommenting print(df.head()) and clicking run.

    - We've created dummy features for the categorical values and set the predictor and target variables as X and y respectively. Uncomment the lines pertaining to this and press run.

    - You can examine the new set of features using print(X.columns)

How many features are there now? Do they make sense?

*Hint*
Each categorical  variable is replaced by multiple binary (0 or 1) variables. For instance, instead of the variable ‘buying’ in the original dataset, we now have ‘buying_high’, ‘buying_low’, ‘buying_med’ and ‘buying_vhigh’, each taking a value of 0 or 1 depending on which category the price of the car belongs to.

Running print(len(X.columns)) gives us the total number of features. (There are now 21 new features!)

** Task 2
We can now perform a train-test split and fit a decision tree to our training data. We'll be using scikit-learn's train_test_split function to do the split and the DecisionTreeClassifier() class to fit the data. Uncomment the lines that do the same and press Run.

** Task 3
We're now ready to visualize the decision tree! The tree module within scikit-learn has a plotting functionality that allows us to do this. Uncomment the lines relevant to this and press Run to view the tree visualization.

** Script.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt

  #Import models from scikit learn module:
  from sklearn.model_selection import train_test_split
  from sklearn.tree import DecisionTreeClassifier
  from sklearn import tree

  #Loading the dataset
  df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])

  # 1a. Take a look at the dataset
  print(df.head())

  # 2a. Setting the target and predictor variables
  df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 is not acceptable
  X = pd.get_dummies(df.iloc[:,0:6])
  y = df['accep']

  ## 1c. Examine the new features
  print(X.columns)
  print(len(X.columns))

  # 2a. Performing the train-test split
  x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)

  # 2b. Fitting the decision tree classifier
  dt = DecisionTreeClassifier(max_depth=3, ccp_alpha=0.01, criterion='gini')

  dt.fit(x_train, y_train)

  # 3.Plotting the tree
  plt.figure(figsize=(20, 12))
  tree.plot_tree(dt, feature_names = x_train.columns, max_depth=5, class_names = ['unacc', 'acc'], label='all', filled=True)
  plt.tight_layout()
  plt.show()

#+end_src

#+RESULTS:
#+begin_example
  buying  maint doors persons lug_boot safety  accep
0  vhigh  vhigh     2       2    small    low  unacc
1  vhigh  vhigh     2       2    small    med  unacc
2  vhigh  vhigh     2       2    small   high  unacc
3  vhigh  vhigh     2       2      med    low  unacc
4  vhigh  vhigh     2       2      med    med  unacc
Index(['buying_high', 'buying_low', 'buying_med', 'buying_vhigh', 'maint_high',
       'maint_low', 'maint_med', 'maint_vhigh', 'doors_2', 'doors_3',
       'doors_4', 'doors_5more', 'persons_2', 'persons_4', 'persons_more',
       'lug_boot_big', 'lug_boot_med', 'lug_boot_small', 'safety_high',
       'safety_low', 'safety_med'],
      dtype='object')
21
#+end_example

* Interpreting a Decision Tree
We're now going to examine the decision tree we built for the car dataset. The image generated by the code is the exact plot we created in the previous exercise. Two important concepts to note here are the following:

    - The root node is identified as the top of the tree. This is notated already with the number of samples and the numbers in each class (i.e. unacceptable vs. acceptable) that was used to build the tree.

    - Splits occur with True to the left, False to the right. Note the right split is a leaf node i.e., there are no more branches. Any decision ending here results in the majority class.

(Note that there is a term called gini in each of the boxes that is immensely important for how the split is done)

To interpret the tree, it's useful to keep in mind that the variables we're looking at are categorical variables that correspond to:

    - ~buying~: The price of the car which can be “vhigh”, “high”, “med”, or “low”.

    - ~maint~: The cost of maintaining the car which can be “vhigh”, “high”, “med”, or “low”.

    - ~doors~: The number of doors which can be “2”, “3”, “4”, “5more”.

    - ~persons~: The number of people the car can hold which can be “2”, “4”, or “more”.

    - ~logboot~: The size of the trunk which can be “small”, “med”, or “big”.

    - ~safety~: The safety rating of the car which can be “low”, “med”, or “high”.

** Task 1
The root node is identified as the top of the tree. This is notated already with the number of samples and the number in each class (i.e. unacceptable vs. acceptable) that was used to build the tree. We've created a variable ~frac_acc~ to print the fraction of acceptable cars the tree was trained on. Calculate this form the root node of the tree.

*Hint*
According to the root node, the total number of samples is 1382. The number of cars that are in the class acc is 412.

** Task 2
The first split occurs off the root node based on the boolean ~safety_low <=0.5~. As this is a dummy variable, a value of 1 indicates safety='low'. Splits occur with True to the left, False to the right. Note the right split is a lef node (i.e. there are no more branches), so any decision ending here results in the majority class. What is the predicted class of a vehicle with low safety?

*Hint*
When ~safety_low is > 0.5~, we see that the tree splits off to a leaf on the right resulting in the car belonging to the unacc class. (This intuitively makes sense because we expect a car with a low safety indicator to be less acceptable!)

** Task 3
Identify the final outcome of the decision tree for the sample with ~buying_vhigh=1, persons_2=0, safety_low=0~. We've defined a variable called ~sample_class~ to fill in the correct answer.

*Hint*
Let's traverse the tree from the root node. We have ~safety_low~ = 0, so we move to the left child node. We have ~persons_2~ = 0, which means we move down one more node to the left. Now we have ~buying_vhigh~ = 1 which means we move down one level but to the right now, leaving us in class = unacc.

** Script.py

#+begin_src python :results output
  frac_acc = 412/1382
  print(f'Fraction of acceptable cars: {frac_acc}')

  low_safety_class = 'unacc'
  print(f'Cars with low safety: {low_safety_class}')

  sample_class = 'unacc'
  print(f'The classs of the sample car: {sample_class}')
#+end_src

#+RESULTS:
: Fraction of acceptable cars: 0.2981186685962373
: Cars with low safety: unacc
: The classs of the sample car: unacc

* Gini Impurity
Consider the two trees below. Which tree would be more useful as a model that tries to predict whether someone would get an A in a class?

[[./gini_impurity.png]]

Let's say you use the top tree. You'll end up at a leaf node where the label is up for debate. The training data has labels from both classes! If you use the bottom tree, you'll end up at a leaf where there's only one type of label. There is not debate at all! We'd be much more confident about our classification if we used the bottom tree.

The idea can be quantified by calculating the Gini impurity of a set of data points. For two classes (1 and 2) with probabilities $p_1$ and $p_2$ respectively, the Gini impurity is:

$$
1 - (p_1^2 + p_2^2) = 1 - (p_1^2 + (1 - p_1)^2)
$$

[[./gini_impurity_graph.png]]

The goal of a decision tree model is to separate the classes the best possible, i.e. minimize the impurity (or maximize the purity). Notice that if p_1 is 0 or 1, the Gini impurity is 0, which means there is only one class so there is perfect separation. From the graph, the Gini impurity is maximum at p_1=0.5, which means the two classes are equally balanced, so this is perfectly impure!

In general, the Gini impurity for C classes is defined as:

$$
1 - \sum_1^C p_i^2
$$

** Task 1
Using the same decision tree from the previous exercise, notice that each node is labeled with a gini=, which is the Gini impurity for the samples at that node. For the root node, calculate the Gini impurity using the formula and confirm this matches the value listed on the node.

*Hint*
Use the formula:

$$
1 - (p_1^2 + p_2^2) = 1 - (p_1^2 + (1 - p_1)^2)
$$

$$
1 - \left(\left (\frac{412}{1382} \right )^2 + \left (\frac{970}{1382} \right )^2 \right)
$$

$$
1 -  \left(\left (\frac{412}{1382} \right )^2 + \left (1 - \frac{412}{1382} \right )^2 \right)
$$

** Task 2
Confirm the Gini impurity for the bottom left node. Since this is a non-root node (either a leaf or a split node), the Gini impurity is only calculated for he samples passing through this node, not the entire dataset -hence the number of samples is 449, not 1382.

*Hint*
Use the same formula as above, but observe that the bottom left node has 449 samples with 96 acc and 353 unacc cars. (So p1 = 96/449 and so on.)

#+begin_src python :results output
  gini_root = 1 - ((412/1382)**2 + (1 - (412/1382))**2)
  print(f'Gini impurity root node : {gini_root}')

  gini_bottom_left = 1 - ((96/449)**2 + (1 - (96/449))**2)
  print(f'Gini impurity bottom left node : {gini_bottom_left}')
#+end_src

#+RESULTS:
: Gini impurity root node : 0.41848785606128835
: Gini impurity bottom left node : 0.33618880858725886
