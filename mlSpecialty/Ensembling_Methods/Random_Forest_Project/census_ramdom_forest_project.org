
* Random Forest Project
In this project, we will be using a dataset containing census information from UCI's Machine Learning Repository.

By using this census data with a random forest, we will try to predict whether or not a person makes more than $50,000.

** Datasets
The original data set is available at the UCI Machine Learning Repository:

    - https://archive.ics.uci.edu/ml/datasets/census+income

The dataset has been loaded in scritp.py and save as a dataframe named ~df~. Some of the imput and output features of interest are:

    - ~age~: continuous

    - ~workclass~: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked

    - ~education~: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool

    - ~race~: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black

    - ~sex~: Female, Male

    - ~capital-gain~: continuous

    - ~capital-loss~: continuous

    - ~hours-per-week~: continuous

    - ~native country:~ discrete

    - ~income~: discrete, >50K, <=50K

** Investigate the data

*** Task 1
We will build a *random forest classifier* to predict the income category. First, take a look at the distribution of income values -what percentage of samples have incomes less than 50k and greater than 50k?

*Hint*
Use
#+begin_src python
df['income'].value_counts(normalize=True)
#+end_src

*** Task 2
There's a small problem with our data that is a little hard to catch -every string has an extra space at the start. For example, the first row's native-country is "  United-States". One way to fix this is to select all columns of type ~object~ and use the string method ~.str.strip()~.

*Hint*
Update each object columns with ~df[c].str.strip()~.

*** Task 3
Create a features dataframe X. This should include only features in the list ~feature_cols~ and convert categorical features to dummy variables using ~pd.get_dummies()~. Include the parameter ~drop_first=True~ to eliminate redundant features.

*Hint*
Use ~pd.get_dummies(df[feature_cols], drop_first=True)~.

*** Task 4
Create the output variable ~y~, which is binary. It should be 0 when income is less than 50k and 1 when it is greater than 50k.

*Hint*
Try using np.where()

*** Task 5
Split the data into a train and test set with a test size of 20%.

** Build and Tune Random Forest Classifiers by Depth

*** Task 6
Instantiate an instance of a RandomForestClassifier() (with default parameters). Fit the model on the train data and print the score (accuracy) on the test data. This will act as a baseline to compare other model performances.

*Hint*
~rf~ should be the instance of RandomForestClassifier(), trained using ~x_train, y_train~ and scored on ~x_test, y_test~.

*** Task 7
We will explore tuning the random forest classifier model by testing the performance over a range o ~max_depth~ values. Fit a random forest classifier for max_depth values from 1-25. Save the accuracy score for the train and test sets in the lists ~accuracy_train, accuracy_test~.

*Hint*
Inside the loop skeleton, each random forest should be fit on the trainin data, i.e. ~x_train, y_train~. The accuracy score is then computed using the predictions from the model (on the relevant dataset) and the actual values (y_train or y_test).

*** Task 8
Find the largest accuracy and the depth this occurs on the test data.

*Hint*
The accuracy values are saved in ~accuracy_test~, so the maximum can be found with ~np.max(accuracy_test)~. The depth at which that occurs is at the index of the depths array ~np.argmax(accuracy_test)~.

*** Task 9
Plot the training and test accuracy of the models versus the ~max_depth~.

*Hint*
The data was saved in the previous loop, use ~plt.plot(depths, accuracy_test)~, etc.

*** Task 10
Refit the random forest model using the ~max_depth~ from above; save the feature importances in a dataframe. Sort the results and print the top five features.

*Hint*
Update the ~max_depth=best_depth~ in the random forest classifier.

** Create Additional Features and ReTune

*** Task 11
Looking at the education feature, there are 16 unique values -from preschool to profesional school. Rather than adding dummy variables for each value, it makes sense to bin some of these values together. While there are many ways to do this, we will take the approach of combining the values into 3 groups: ~High school and less~, ~Collegue to Bachelors~ and ~Masters and more~. Create a new column in df for this new features called ~education_bin~.

*** Task 12
Like we did previously, we will now add this new feature into our feature list and recreate ~X~.

*** Task 13
As we did before, we will tune the random forest classifier model by testing the performance over a range of ~max_depth~ values. Fit a random forest classifier for ~max_depth~ from 1-25. Save the accuracy score for the train and test sets in the lists ~accuracy_train, accuracy_test~.

*** Task 14
Find the largest accuracy and the depth this occurs on the test data. Compare the results from the previous model tuned.

*** Task 15
Plot the training and test accuracy of the models versus the max_depth. Compared the results from the previous model tuned.

*** Task 16
Refit the random forest model using the ~max_depth~ from above; save the feature importances in a dataframe. Sort the results and print the top five features. Compare the results from the previous model tuned.

*** Task 17
Not that the accuracy of our final model increased and one of our added features is now in the top 5 based on importance!

There are a few different ways to extend this project:

    - Are there other features that may lead to an even better performance? Consider creating new ones or adding additional features not part of the original feature list
    - Consider tuning hyperparameters based on a different evaluation metric -our classes are fairly imbalanced, AUC of F1 may lead to a different result
    - Tune more parameters of the model. You can find a description of all the parameters you can tune in the Random Forest Classifier documentation. For example, see what happens if you tune ~max_features~ or ~n_estimators~.

** Script.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import seaborn as sns

  # Import models from scikit learn module:
  from sklearn.model_selection import train_test_split
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, RandomForestRegressor
  from sklearn import tree
  from sklearn.linear_model import LogisticRegression
  from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

  col_names = ['age', 'workclass', 'fnlwgt','education', 'education-num',
  'marital-status', 'occupation', 'relationship', 'race', 'sex',
  'capital-gain','capital-loss', 'hours-per-week','native-country', 'income']

  df = pd.read_csv('adult.data', header=None, names=col_names)

  # Distribution of income
  print(df['income'].value_counts(normalize=True))

  # Clean columns by stripping extra whitespace for columns of type "object"
  for c in df.select_dtypes(include=['object']).columns:
      df[c] = df[c].str.strip()

  feature_cols = ['age', 'capital-gain', 'capital-loss', 'hours-per-week', 'sex', 'race']

  # Create feature dataframe X with feature columns and dummy variables for categorical features
  X = pd.get_dummies(df[feature_cols], drop_first=True)

  # Create output variable y which is binary, 0 when income is less than 50k, 1 when it is greater than 50k
  y = np.where(df['income']=='<=50K', 0, 1)

  # Split the data into train and test set
  x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.20)

  # Instantiate random forest classifier, fit and score with default parameters
  rf = RandomForestClassifier()
  rf.fit(x_train, y_train)
  rf.score(x_test, y_test)
  print(f'Accuracy score for default random forest: {round(rf.score(x_test, y_test)*100,3)}%')

  # Tune the hyperparameter max_depth over a range 1-25, save the scores for test and train set
  np.random.seed(0)
  accuracy_train = []
  accuracy_test = []
  depths = range(1, 26)
  for i in depths:
      rf = RandomForestClassifier(max_depth=i)
      rf.fit(x_train, y_train)
      y_pred = rf.predict(x_test)
      accuracy_test.append(accuracy_score(y_test, rf.predict(x_test)))
      accuracy_train.append(accuracy_score(y_train, rf.predict(x_train)))

  # Find the best accuracy and at what depth that occurs
  best_acc = np.max(accuracy_test)
  best_depth = depths[np.argmax(accuracy_test)]

  print(f'The highest accuracy on the test is achieved when depth: {best_depth}')

  print(f'The highest accuracy on the test set is: {round(best_acc*100,3)}%')

  # Plot the accuracy scores for the test and train set over the range of depth values
  plt.plot(depths, accuracy_test, 'bo--', depths, accuracy_train, 'r*:')
  plt.legend(['test accuracy', 'train accuracy'])
  plt.xlabel('max depth')
  plt.ylabel('accuracy')
  plt.show()

  # Save the best random forest model and save the feature importances in a dataframe
  best_rf = RandomForestClassifier(max_depth=best_depth)
  best_rf.fit(x_train, y_train)
  feature_imp_df = pd.DataFrame(zip(x_train.columns, best_rf.feature_importances_), columns=['feature', 'importance'])
  print('Top 5 random forest features:')
  print(feature_imp_df.sort_values('importance', ascending=False).iloc[0:5])

  # Create two new features, based on education and native country
  df['education_bin'] = pd.cut(df['education-num'], [0,9,13,16], labels=['HS or less', 'College to Bachelors', 'Masters or more'])

  feature_cols = ['age', 'capital-gain', 'capital-loss', 'hours-per-week', 'sex', 'race', 'education_bin']

  # Use these two new additional features and recreate X and test/train split
  X = pd.get_dummies(df[feature_cols], drop_first=True)

  x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.2)

  # Find the best max depth now with the additional two features
  np.random.seed(0)
  accuracy_train = []
  accuracy_test = []
  depths = range(1,10)
  for i in depths:
      rf = RandomForestClassifier(max_depth=i)
      rf.fit(x_train, y_train)
      y_pred = rf.predict(x_test)
      accuracy_test.append(accuracy_score(y_test, rf.predict(x_test)))
      accuracy_train.append(accuracy_score(y_train, rf.predict(x_train)))

  best_acc = np.max(accuracy_test)
  best_depth = depths[np.argmax(accuracy_test)]
  print(f'The highest accuracy on the test is achieved when depth: {best_depth}')
  print(f'The highest accuracy on the test set is: {round(best_acc*100,3)}%')

  plt.figure(2)
  plt.plot(depths, accuracy_test, 'bo--', depths, accuracy_train, 'r*:')
  plt.legend(['test accuracy', 'train accuracy'])
  plt.xlabel('max depth')
  plt.ylabel('accuracy')
  plt.show()

  # Save the best model and print the two features with the new feature set
  best_rf = RandomForestClassifier(max_depth=best_depth)
  best_rf.fit(x_train, y_train)
  feature_imp_df = pd.DataFrame(zip(x_train.columns, best_rf.feature_importances_), columns=['feature', 'importance'])
  print('Top 5 random forest features:')
  print(feature_imp_df.sort_values('importance', ascending=False).iloc[0:5])

#+end_src

#+RESULTS:
#+begin_example
income
 <=50K    0.75919
 >50K     0.24081
Name: proportion, dtype: float64
Accuracy score for default random forest: 82.067%
The highest accuracy on the test is achieved when depth: 12
The highest accuracy on the test set is: 83.464%
Top 5 random forest features:
          feature  importance
1    capital-gain    0.370119
0             age    0.248969
3  hours-per-week    0.140480
2    capital-loss    0.140439
4        sex_Male    0.078258
The highest accuracy on the test is achieved when depth: 9
The highest accuracy on the test set is: 84.431%
Top 5 random forest features:
                          feature  importance
1                    capital-gain    0.326836
0                             age    0.201973
10  education_bin_Masters or more    0.130849
3                  hours-per-week    0.102162
2                    capital-loss    0.095188
#+end_example
