
* Basics of Random Forest
In this lesson, you'll learn what random forest are, how the random forest algorithm works and how to implement it in ~scikit-learn~.

We've seen that decision trees can be powerful supervised machine learning models. However, they're not without their weaknesses -decision trees are often prone to overfitting. We've discussed some strategies to minimize this problem, like pruning, but sometimes that isn't enough. We need to find another way to generalize our trees. This is where the concept of a random forest comes in handy.

A random forest is an /ensemble machine learning technique./ A random forest contains many decision trees that all work together to classify new points. When a randon forest is asked to classify a new point, the random forest gives that point to each of the decision trees. Each of those trees reports their classification and the random forest returns the most popular classification. It's like every tree gets a vote, and the most popular classification wins. Some of the trees in the random forest may be overfit, but by making the prediction based on a large number of trees, overfitting will have less of an impact.

The ilustration below depicts a random forest used to predict if a student will get an A un a test or not based on an anonymous survey. The survey fields (the features here!) include hours the student slept, whether they expressed an intention to chear or not, hours studied and their average grade so far. Here, the random forest is made up of eleven decision trees -the prediction outcome A is outvoted 6:5!

[[./tree_election.png]]

* Bootstrapping
You might be wondering how the trees in the random forest get created. After all, right now, our algorithm for creating a decision tree is deterministic -given a trainin set, the same tree will be made every time. To make a random forest, we use a technique callled /bagging/ which is short for /bootstrap aggregating/. This exercise will explain bootstrapping, which is a type of sampling method done with *replacement*.

How it works is as follows: every time a decision tree is made, it is created using a different subset of the points in the training set. For example, if our training set had ~1000~ rows in it, we could make a decision tree by picking ~100~ of those rows at random to build the tree. This way, every tree is different, but all trees will still be created from a portion of the training data.

In bootstrapping, we're doing this process /with replacement./ Picture putting all 100 rows in a bag and reaching in and grabbing one row at random. After writing down what row we picked, we put that row back in our bag. This means that when we're picking our 100 random rows, we could pick the same row more than once. In fact, it's very unlikely, but all 100 randomly picked rows could all be the same row! Because we're picking these rows with replacement, there's no need tho shrink our bagged training set from 1000 rows to 100. We can pick 1000 at random, and because we can get the same row more than once, we'll still end up with a unique data set.

We've loaded a dataset about cars here. An important field within the dataset is the safety rating, which tells us how crash/rollover resistant a car is, in other words, how safe the car is. The safety variable can be either "low", "med", or "high". We're going to implement bootstrapping and estimate the average safety rating across the different bootstrapped samples.

Here a list of the variables in the car evaluation dataset.

*Variable*            *Description*
~safety~ 	     estimated safety of the car (low, med, or high)
~buying~ 	     buying price
~maint~ 	             price of the maintenance
~doors~ 	             number of doors
~persons~          capacity in terms of persons to carry
~lug_boot~       the size of luggage boot
~accep~ 	             evaluation level (unacceptable, acceptable, good, very good)

** Task 1
We've written some code to print the number of rows in the data set and the distribution of safety ratings of the entire database.

*Hint*
The safety variable has the same number of observations for each of the three classes.

** Task 2
Now that we know the safety variable classes are equally distributed, we're going to create a bootstrapped sample using ~.sample()~.

~.sample()~ takes two arguments:

    - number of rows: which is the same size as the dataset

    - ~replace~: set to True because bootstrapping is sampling with replacement

Print the distribution of safety ratings of the new sampled dataset.

What is the safety ratings distribution of the bootstrapped data?

*Hint*
Use ~.value_counts()~ with the parameter ~normalize=True~ to get the distribution.

The new ~safety~ ratings distribution should not be perfectly equal this time!

** Task 3
Note that the distribution has now shifted! Using the same process, write a for loop to create 1000 bootstrapped samples of the same size as the original dataset. Save the percentage of "low" ratings into an array called ~low_perc~.

** Task 4
We've written some code to plot a histogram of the low percentage values.

We see that the average value of the low safety proportion of vehicles spans a range centered around the true mean.

** Task 5
Now print the average low percentage and the 95% confidence range.

*Hint*
We are 95% confident that the “low” ratings will make up between 31.08% and 35.65% of the total observations. NOTE: Your range may vary depending on your model.

** Script.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import seaborn as sns

  # Models from scikit learn module:
  from sklearn.model_selection import train_test_split
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.ensemble import RandomForestClassifier

  df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])
  df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable
  X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)
  y = df['accep']

  x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.25)
  nrows = df.shape[0]

  ## 1. Print number of rows and distribution of safety ratings
  print(nrows)
  print(f'Distribution of safety ratings in (nrows) of data:')
  print(df.safety.value_counts(normalize=True))

  ## 2. Create bootstrapped sample
  boot_sample = df.sample(nrows, replace=True)
  print(f'Distribution of safety ratings in bootstrapped sample data:')
  print(boot_sample.safety.value_counts(normalize=True))

  ## 3. Create 1000 bootstrapped samples
  low_perc = []
  for i in range(1000):
      boot_sample = df.sample(nrows, replace=True)
      low_perc.append(boot_sample.safety.value_counts(normalize=True)['low'])

  ## 4. Plot a histogram of the low percentage values
  mean_lp = np.mean(low_perc)
  print(mean_lp)
  plt.hist(low_perc, bins=20)
  plt.xlabel('Low Percentage')
  plt.show()

  ## 5. What are the 2.5 and 97.5 percentiles?
  print(f'Average low percentage: {np.mean(low_perc).round(4)}')

  low_perc.sort()
  print(f'95% Confidence Interval for low percengage: ({low_perc[25].round(4)},{low_perc[975].round(4)})')

#+end_src

#+RESULTS:
#+begin_example
1728
Distribution of safety ratings in (nrows) of data:
safety
low     0.333333
med     0.333333
high    0.333333
Name: proportion, dtype: float64
Distribution of safety ratings in bootstrapped sample data:
safety
low     0.355903
med     0.335069
high    0.309028
Name: proportion, dtype: float64
0.33347685185185183
Average low percentage: 0.3335
95% Confidence Interval for low percengage: (0.3108,0.3565)
#+end_example

* Bagging
Random forest create different trees using a process known as bagging, which is short for bootstrapped aggregating. As we already covered bootstrapping, the process starts with creating a single decision tree on a bootstrapped sample of data points in the training set. Then after many trees have been made, the results are "aggregated" together. In the case of a classification task, often the aggregation is taking the majority vote of the individual classifiers. For regression tasks, often the aggregation is the average of the individual regressors.

We will dive into this process for the cars dataset we used in the previous exercise. The dataset has six features:

    - ~buying~ : car price as a categorical variable: “vhigh”, “high”, “med”, or “low”

    - ~maint~ : cost of maintaining the car; can be “vhigh”, “high”, “med”, or “low”.

    - ~doors~ : number of doors; can be “2”, “3”, “4”, “5more”.

    - ~persons~ : number of people the car can hold; can be “2”, “4”, or “more”.

    - ~lugboot~ : size of the trunk; can be “small”, “med”, or “big”.

    - ~safety~ : safety rating of the car; can be “low”, “med”, or “high”

Load the dataset and do the train-test split. Our target variable for prediction is an acceptability rating, accep, that's either ~True~ or ~False~.

** Task 1
Train a decision tree with ~max_depth~  set to 5. Evaluate the ~accuracy_score~ on the ~test~ data.

** Task 2
We've written some code to get a new set of indices, ~ids~, to generate a bootstrapped set of row indices. We've set the ~random_state~ argument to ~0~ for reproducibility. Using these indices, fit another decision tree to training data pertaining to these rows. What is the accuracy score on the test set for the new classifier?

** Task 3
Repeat a decision tree build on 10 different bootstrapped samples using a for loop. Save the results, ~y_pred~ of all 10 predictions on the test set in an array,  ~preds~. Take the average of the 10 results and save it as ~ba_pred~.

** Task 4
We have just performed bagging! Calculate the accuracy score on the bagged predictions and save it as ~ba_accuracy~. (Note that the predictions are averaged and will no longer be binary as a bunch of zeroes and ones have been averaged.)

*Hint*
Threshold the ~ba_pred~ array to get a binary classification and then compute the accuracy.

** Script.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  from sklearn.model_selection import train_test_split
  from sklearn.tree import DecisionTreeClassifier
  from sklearn import tree
  from sklearn.metrics import accuracy_score

  df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])

  df['accep'] = ~(df['accep'] == 'unacc') #1 is acceptable, 0 is unacceptable

  X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)
  y = df['accep']

  x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.25)

  # 1. Decision tree trained on training set
  dtree = DecisionTreeClassifier(max_depth=5)
  dtree.fit(x_train, y_train)
  print(f'Accuracy score of DT on test set (trained using full set): {dtree.score(x_test, y_test).round(4)}')

  # 2. New decision tree trained on bootstrapped sample
  dt2 = DecisionTreeClassifier(max_depth=5)

  # ids are the indices of the bootstrapped sample
  ids = x_train.sample(x_train.shape[0], replace=True, random_state=0).index
  dt2.fit(x_train.loc[ids], y_train[ids])

  print(f'Accuracy score of DT on test set (trained using bootstrapped sample): {dt2.score(x_test, y_test).round(4)}')

  # 3.Bootstrapping ten samples and aggregating the results:
  preds = []
  random_state = 0

  for i in range(10):
      ids = x_train.sample(x_train.shape[0], replace=True, random_state=random_state+i).index
      dt2.fit(x_train.loc[ids], y_train[ids])
      preds.append(dt2.predict(x_test))

  ba_pred = np.array(preds).mean(0)

  # 4. Calculate accuracy of the bagged sample
  ba_accuracy = accuracy_score(ba_pred>=0.5, y_test)

  print(f'Accuracy score of aggregated 10 bootstrapped samples: {ba_accuracy.round(4)}')

#+end_src

#+RESULTS:
: Accuracy score of DT on test set (trained using full set): 0.8588
: Accuracy score of DT on test set (trained using bootstrapped sample): 0.8912
: Accuracy score of aggregated 10 bootstrapped samples: 0.9097

* Randon Feature Selection
In addition to using bootstrapped samples of our dataset, we can continue to add variety to the ways our trees are created by randomly selecting the features that are used.

Recall that for our car data set, the original features were the following:

    - The price of the car which can be “vhigh”, “high”, “med”, or “low”.

    - The cost of maintaining the car which can be “vhigh”, “high”, “med”, or “low”.

    - The number of doors which can be “2”, “3”, “4”, “5more”.

    - The number of people the car can hold which can be “2”, “4”, or “more”.

    - The size of the trunk which can be “small”, “med”, or “big”.

    - The safety rating of the car which can be “low”, “med”, or “high”

Our target variable for prediction is an acceptability rating, ~accep~, that's either ~True~ or ~False~. For our final features sets, ~x_train~ and ~x_test~, the categorical features have been dummy encoded, giving us 15 features in total.

When we use a decision tree, all the features are used and the split is chosen as the one that increases the information gain the most. While it may seem counter-intuitive, selecting a random subset of features can help in the performance of an ensemble model. In the following example, we will use a random selection of features prior to model building to add additional variance to the individual trees. While an individual tree may perform worse, sometimes the increases in variance can help model performance of the ensemble model as a whole.

** Task 1
We've written some code that create a random sample of 10 features, ~rand_features~ from the 15 original features by using NumPy's ~random.choice~ method and defined a decision tree classifier ~dt2~.

Train the new decision tree model (without specifying any pre-selected parameters!), ~dt2~ on training data /that contains only these ten columns./ Calculate the new accuracy on the test set, store it as ~accuracy_dt2~ and print it to compare it to the tree built using the entire training set.

*Hint*
Remember to calculate accuracy score only on ~x_test[rand_features]~ and not the whole test data.

** Task 2
Every time we use ~np.random.choice~, we can generate a new subset of random features. We're now going to obtain predictions from ten decision tree classifiers by doing the following. We've initialized an empty array called ~predictions~ and created a ~for~ loop with ten iterations with the following steps:

    - The first line selects ten random features using ~rand_features = np.random.choice(x_train.columns, 10)~

    - Fit the decision tree ~dt2~ to training data pertaining to these features

    - Append the predictions on test data (with only rand_features columns!) to the array ~predictions~

** Task 3
We have ter decision trees' worth of predictions now! To meningfully combine their predictions, let's use the following system. If more than 5 classifiers predict that a datapoint belong to a certain class, we assign an aggregate prediction to that class. To do this:

    - Create an array ~prob_predictions~ that is the mean value of the predictions stored in ~predictions~. This is similar to a probability on each datapoint since this is a binary classification problem.

    - Create an array ~agg_predictions~ that assigns a value of False or True to each element in ~prob_predictions~ based on whether the value is <= 0.5 or >0.5. This gives us the aggregate prediction from the 10 decision tree classifiers.

    - Calculate the accuracy score on this aggregate prediction array. Store it as ~agg_accuracy~ and print it to compare it to the previous accuracy scores.

      *Hint*

      Use ~np.array.mean()~ to calculate the mean of the array predictions. Remember to specify the right axis! (We want the average in each column and not the rows!)

      To get ~agg_predictions~, you can apply the inequality ~(prob_predictions>0.5)~ to obtain True/False values.

** Script.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  from sklearn.model_selection import train_test_split
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.metrics import accuracy_score

  df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])

  df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable

  X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)
  y = df['accep']

  x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.25)
  dt = DecisionTreeClassifier()
  dt.fit(x_train, y_train)
  print("Accuracy score of DT on test set (trained using full feature set):")
  accuracy_dt = dt.score(x_test, y_test)
  print(accuracy_dt)

  # 1. Create rand_features, random samples from the set of features
  rand_features = np.random.choice(x_train.columns, 10)

  # Make new decision tree trained on random sample of 10 features and calculate the new accuracy score
  dt2 = DecisionTreeClassifier()
  dt2.fit(x_train[rand_features], y_train)

  print("Accuracy score of DT on test set (trained using random feature sample):")
  print(dt2.score(x_test[rand_features], y_test))

  # 2. Build decision trees on 10 differents random samples
  predictions = []
  for i in range(10):
      rand_features = np.random.choice(x_train.columns, 10)
      dt2.fit(x_train[rand_features], y_train)
      predictions.append(dt2.predict(x_test[rand_features]))

  # 3. Get aggregate predictions and accuracy score
  prob_predictions = np.array(predictions).mean(0)
  agg_predictions = (prob_predictions>0.5)
  agg_accuracy = accuracy_score(agg_predictions, y_test)
  print('Accuracy score of aggregated 10 samples:')
  print(agg_accuracy)

#+end_src

#+RESULTS:
: Accuracy score of DT on test set (trained using full feature set):
: 0.9490740740740741
: Accuracy score of DT on test set (trained using random feature sample):
: 0.6805555555555556
: Accuracy score of aggregated 10 samples:
: 0.7546296296296297

* Baggin in scikit-learn
The two steps we walked through above created trees on bootstrapped samples and randomly selecting features. These can be combined together and implemented at the same time! Combining them adds an additional variation to the base learners for the ensemble model. This in turn increases the ability of the model to generalize to new and unseen data, i.e., it minimizes bias and increases variance. Rather than re-doing this process manually, we will use ~scikit-learn~'s bagging implementation, ~BaggingClassifier()~, to do so.

Much like other models we have used in ~scikit-learn~, we instantiate a instance of ~BaggingClassifier()~ and specify the parameters. The first parameter, ~base_estimator~ refers to the machine learning model that is being bagged. In the case of random forest, the base estimator would be a decision tree. We are going to use a decision tree classifier *with* a ~max_depth~ of 5, this will be instantiated with ~BaggingClassifier(DecisionTreeClassifier(max_depth=5))~.

After the model has been defined, methods ~.fit()~, ~.predict()~, ~.score()~ can be used as expected. Additional hyperparameters specific to bagging include the number of estimators (~n_estimators~) we want to use and the maximum number of features we'd like to keep (~max_features~).

*Note*:
While we have focused on decision tree classifiers (as this is the base learner for a random forest classifier), this procedure of bagging is not specific to decision trees, and in fact can be used for any base classifier or regression model. The ~scikit-learn~ implementation is generalizable and can be used for other base models!

** Task 1
Create an instance of ~BaggingClassifier~, ~bag_dt~ with the following arguments:

    - ~DecisionTreeClassifier~ (with ~max_depth=5~) base estimator

    - ~n_estimators=10~

Fit the model on the training set and evaluate the model on the test set by calculating the accuracy score. Save the score as ~bag_accuracy~ and print it.

*Hint*
You can use the ~.score()~ method on ~bag_dt~ to get the accuracy. Alternately you can also use ~accuracy_score~ from ~scikit-learn~'s ~metrics~ module.

** Task 2
Create a different bagging classifier, ~bag_dt_10~ that includes the parameter ~max_features=10~ over and above the parameters in ~bag_dt~. Fit the model on the training set and evaluate the model on the test set by calculating the accuracy score. Save the score as ~bag_accuracy_10~ and print it.

** Task 3
Change the base estimator of the bagged classifier to be logistic regression and call this instance ~bag_lr~ as follows:

    - Set ~base_estimator~ to be ~LogisticRegression()~

    - Set ~n_estimators~ and ~max_features~ to be 10 like in the previous checkpoint

Refit on the training set and calculate the accuracy on the test set. Store this as ~bag_accuracy_lr~ and print it.

** Script.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  from sklearn.model_selection import train_test_split
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.ensemble import RandomForestClassifier, BaggingClassifier,RandomForestRegressor
  from sklearn.metrics import accuracy_score

  df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])
  df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable
  X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)
  y = df['accep']
  x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.25)

  # 1. Bagging classifier with 10 Decision Tree base estimators
  bag_dt = BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=5), n_estimators=10)
  bag_dt.fit(x_train, y_train)

  print('Accuracy score of Bagged Classifier, 10 estimators:')
  bag_accuracy = bag_dt.score(x_test, y_test)
  print(bag_accuracy)

  # 2. Set max_features to 10
  bag_dt_10 = BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=5), n_estimators=10, max_features=10)
  bag_dt_10.fit(x_train, y_train)

  print('Accuracy score of Bagged Classifier, 10 estimators, 10 max features:')
  bag_accuracy_10 = bag_dt_10.score(x_test, y_test)
  print(bag_accuracy_10)

  # 3. Change base estimator to Logistic Regression
  from sklearn.linear_model import LogisticRegression

  bag_lr = BaggingClassifier(estimator=LogisticRegression(), n_estimators=10, max_features=10)
  bag_lr.fit(x_train, y_train)

  print('Accuracy score of Logistic Regression, 10 estimators:')
  bag_accuracy_lr = bag_lr.score(x_test, y_test)
  print(bag_accuracy_lr)

#+end_src

#+RESULTS:
: Accuracy score of Bagged Classifier, 10 estimators:
: 0.9328703703703703
: Accuracy score of Bagged Classifier, 10 estimators, 10 max features:
: 0.9375
: Accuracy score of Logistic Regression, 10 estimators:
: 0.8981481481481481

* Train and Predict using scikit-learn
Now that we have covered two major ways to combine trees, both in terms of samples and features, we are ready to get to the implementation of random forest! This will be similar to what we covered in the previous exercises, but the random forest algorithm has a slightly different way of randomly choosing features. Rather than choosing a single random set at the onset, each split chooses a different random set.

For example, when finding which feature to split the data on the first time, we might randomly choose to only consider the price of the car, the number of doors, and the safety rating. After splitting the data on the best feature from that subset, we'll likely want to split again. For this next split, we'll randomly select three features again to consider. This time those features might be the cost of maintenance, the number of doors, and the size of the trunk. We'll continue this process until the tree is complete.

One question to consider is how to choose the number of features to randomly select. Why did we choose 3 in this example? A good rule of thumb is select as many features as /the square root of the total number of features./ Our car dataset doesn't have a lot of features, so in this example, it's difficult to follow this rule. But if we had a dataset with 25 features, we'd want to randomly select 5 features to consider at every split point.

You now have the ability to make a random forest using your own decision trees. However, ~scikit-learn~ has a ~RandomForestClassifier()~ class that will do all this work for you! RandomForestClassifier is in the ~sklearn.ensemble~ module.

~RandomForestClassifier()~ works almost identically to DecisionTreeClassifier() -the ~.fit()~, ~.predict()~, and ~.score()~ methods work in the exact same way.

** Task 1
Create a random forest classification model defined as ~rf~ with default parameters. Use the ~.get_params()~ method to get the parameters of the random forest. Store it as ~rf_params~ and print it.

** Task 2
Fit ~rf~ using the training data set and labels. Predict the classes of the test data set (~x_test~) and save this as an array ~y_pred~. Calculate the accuracy of model on the test set (either using ~.score()~ or ~accuracy_score()~) and save it as ~rf_accuracy~.

** Task 3
Implement additional classification evaluation metrics -calculate and print the precision, recall, and confusion matrix on the test set. Store them as ~rf_precision~, ~rf_recall~ and ~rf_confusion_matrix~ respectively.

** Scrip.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  from sklearn.model_selection import train_test_split
  from sklearn.ensemble import RandomForestClassifier
  from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score

  df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])
  df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable
  X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)
  y = df['accep']
  x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)

  # 1. Create a Random Forest Classifier and print its parameters
  rf = RandomForestClassifier()
  rf_params = rf.get_params()

  print('Random Forest parameters:')
  #print(rf_params)

  # 2. Fit the Random Forest Classifier training data and calculate accuracy score on the test data
  rf.fit(x_train, y_train)
  y_pred = rf.predict(x_test)
  rf_accuracy = rf.score(x_test, y_test)

  print('Test set accuracy:')
  print(rf_accuracy)

  # 3. Calculate Precision and Recall scores and the Confusion Matrix
  rf_precision = precision_score(y_test, y_pred)
  print(f'Test set precision: {rf_precision}')

  rf_recall = recall_score(y_test, y_pred)
  print(f'Test set recall: {rf_recall}')

  rf_confusion_matrix = confusion_matrix(y_test, y_pred)
  print(f'Test set confusion matrix:\n {rf_confusion_matrix}')

#+end_src

#+RESULTS:
: Random Forest parameters:
: Test set accuracy:
: 0.9537037037037037
: Test set precision: 0.953125
: Test set recall: 0.8970588235294118
: Test set confusion matrix:
:  [[290   6]
:  [ 14 122]]

* Randon Forest Regressor
Just like in decision trees, we can use random forest for regression as well. It is important to know when to use regression or classification -this usually comes down to what type of variable your target is. Previously, we were using a binary categorical variable (acceptable versus not), so a classification model was used.

We will now consider a hypothetical new target variable, price, for this data set, which is a continous variable. We've generated some fake prices in the dataset so that we have numerical values instead of the previous categorical variables. (Please note that these are not reflective of the previous categories of high and low prices - we just wanted some numeric values so we can perform regression! :) )

Now, instead of a classification task, we will use ~scikit-learn~'s ~RandomForestRegressor()~ to carry out a regression task.

*Note*
Recall that the default evaluation score for regressors in scikit-learn is the R-squared score.

** Task 1
Initialize and fit a RandomForestRegressor() model named ~rfr~ on the training data. Calculate the default scores (the ~R^2~ values here) on the train and test sets, store them as ~r_squared_train~ and ~r_squared_test~ respectively, and print them.

** Task 2
Calculate the average price of a car, store it as ~avg_price~. Calculate the MAE (Mean Absolute Error) for the train and test sets and store the values as ~mae_train~ and ~mae_test~. Print all three values to see how the errors compare to the mean.

** Script.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np

  from sklearn.model_selection import train_test_split
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.ensemble import RandomForestRegressor
  from sklearn.metrics import mean_absolute_error

  df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])
  df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable
  X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)

  ## Generating some fake prices for regression! :)
  fake_prices = (15000 + 25*df.index.values) + np.random.normal(size=df.shape[0])*5000
  df['price'] = fake_prices
  print(df.price.describe())
  y = df['price']

  x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.25)

  # 1. Create a Random Regressor and print R^2 scores on training and test data
  rfr = RandomForestRegressor()
  rfr.fit(x_train, y_train)
  r_squared_train = rfr.score(x_train, y_train)
  r_squared_test = rfr.score(x_test, y_test)

  print(f'Train set R^2: {r_squared_train}')
  print(f'Test set R^2: {r_squared_test}')

  # 2. Print Mean Absolute Error on training and test data
  avg_price = y.mean()
  print(f'Avg Price Train/Test: {avg_price}')

  y_pred_train = rfr.predict(x_train)
  y_pred_test = rfr.predict(x_test)

  mae_train = mean_absolute_error(y_train, y_pred_train)
  print(f'Train Set MAE: {mae_train}')

  mae_test = mean_absolute_error(y_test, y_pred_test)
  print(f'Test Set MAE: {mae_test}')

#+end_src

#+RESULTS:
#+begin_example
count     1728.000000
mean     36702.550871
std      13458.824713
min       5537.045255
25%      26142.533383
50%      36402.770260
75%      47412.013446
max      71126.772771
Name: price, dtype: float64
Train set R^2: 0.9751293337158969
Test set R^2: 0.8257021485249421
Avg Price Train/Test: 36702.550871382286
Train Set MAE: 1663.9768046983374
Test Set MAE: 4588.38588177549
#+end_example

* Review
Here are some of the major takeaways about random forests:

    - A random forest is an *ensemble machine learning model*. It makes a classification by aggregating the classifications of many decision trees.

    - Random forest are used to *avoid overfitting*. By aggregating the classification of multiple trees, having overfitted trees in a random forest is less impactful.

    - Every decision tree in a random forest is created by using a *different subset* of data points from the training set. Those data points are chosen at random with replacement, which means a single data point can be chosen more than once. This process is known as bagging.

    - When creating a tree in random forest, a *randomly selected subset of features* are considered as candidates for the best splitting feature. If your dataset has n features, it is common practice to randomly select the square root of n features.

* Quiz

** Question 1
How does a random forest regressor make predictions?

    - A random forest regressor uses the average prediction from its decision trees as the final predictions.

      Correct! For regression, the average value of all the decision trees is used.

** Question 2
What does ~scikit-learn~'s ~RandonForestClassifier~'s ~.fit()~ method do?

    - The ~.fit()~ method creates the model according to the training data and training labels

      Correct! This is where the model gets created based on the training set.

** Question 3
How does a random forest classifier make predictions?

    - A random forest classifier uses the most common classification from its decision trees as the final classification

      Correct! The classification wiht the most votes wins!

** Question 4
Fill in the code to fit a ~RandonForestClassifier~ with ~n_estimators=50~ on the training data (x_train, y_train) and score on the test data (x_test, y_test).

#+begin_src python :results output
  from sklearn.ensemble import RandomForestClassifier

  rf = RandomForestClassifier(n_estimators=50)

  rf.fit(x_train, y_train)

  rf.score(x_test, y_test)

#+end_src

#+RESULTS:

** Question 5
How is a random forest model related to a decision tree model?

     - A random forest is an ensemble model that makes a classification by aggregating the results of multiple decision trees.

** Question 6
When creating a RandomForestClassifier from ~scikit-learn~, what does the ~n_estimators~ parameter represent and what is it's default value?

The number of trees in the forest, ant the default value is ~100~.

Correct! If you don't include this parameter, the default is 100.

** Question 7
A decision tree regressor was built to predict the selling price of a house and was shown to be overfit. The data has 100 features and 10,000 rows. Fill in the code to train the best new model with appropriate hyperparameters to reduce the overfitting and evaluate the model on the test set.

#+begin_src python
  from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
  from sklearn import tree
  from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

  model = RandomForestRegressor(n_estimators=100, max_features=10)
  model.fit(x_train, y_train)
  model.score(x_test, y_test)
#+end_src

** Question 8
Which statement best describes a bootstrapped sample and how is it used in creating a random forest model?

    - Bootstrapped samples are obtained by taking samples with replacement. Each decision tree is built on a bootstrapped sample.

** Question 9
If your dataset has n features, what is the default value for the number of features considered at each split in scikit-learn's RandomForestClassifier?

    - sqrt(n)

      Correct! This number can be tuned, but ~sqrt(n)~ is the default value.
