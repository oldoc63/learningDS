
* Boosting
In this module we will cover a powerful ensemble method called Boosting. Boosted ensemble methods use weak learners as base models that are simple and tend to suffer from high bias. The weak learners underfit the data.

Boosting is a sequential learning technique where each of the base models builds off of the previous models. Each subsequent model aims to improve the performance of the final ensemble model by attempting to fix the errors in the previous stage.

There are two important decisions that need to be made to perform boosted ensembling:

    1. Sequential Fitting Method

    2. Aggregation Method

Two boosting algorithms that will be covered in detail in this module are *Adaptive Boosting* and *Gradient Boosting*.

While boosting can be applied to any base machine learning algorithm, we will demonstrate with and extremely popular choice as a base estimator, the decision tree. Recall that Decision Trees are a commonly used and powerful machine learning algorithm because they are easy to interpret. Additionally, the training data requires very little manipulation (no need standardization, removal of collinearity, etc).

The major limitation to decision trees is that they tend to suffer from high variance and are therefore prone to overfitting. They are good at making a series of decisions which cause them to memorize the training data, so they do not generalize well to unseen data. In the following exercises we will explore how to work past these limitations while using decision trees for boosting.

[[./base_models_bagging_boosting.png]]

* Adaptive Boosting Overview
*Adaptive Boosting* (or AdaBoost) is a sequential ensembling method that can be used for both classification and regression. It can use any base machine learning model, though it is most commonly used with decision trees.

For AdaBoost, the *Sequential Fitting Method* is accomplished by updating the weight attached to each of the training dataset observations as we proceed from one base model to the next. The *Aggregation Method* is a weighted sum of those base models where the model weight is dependent on the error of that particular estimator.

The training of an AdaBoost model is the process of determining the training dataset observation weights at each step as well as the final weight for each base model for aggregation.

In the next exercise we will dive into the details of AdaBoost!

[[./AdaBoost.png]]

* Adaptive Boosting
Let's take a deeper look at how AdaBoost works! AdaBoost can be used for both regression and classification, but in this example we will be solving a classification problem. We begin with the full Training Dataset. You will see that it consists of green circles and red triangles. The goal of our AdaBoost classifier will be to form a decision boundary that separates these two classes. Initially, the training data instances are all given the same weight. This is indicated by the size of shapes all being the same.

Our first step is to fit an estimator, the 1st Base Model. While boosting can be applied to any base machine learning model, we will use decision trees.  But aren't decision trees prone to overfitting? We already said that the base model for boosting are supposed to be very simple and tend to underfit. That is correct, and for this reason we use the simplest version of a decision tree, known as a decision stump. A decision stump only makes a single decision, so the resultant estimator only has two leaf nodes.

Taking a look at the Result of the 1st Base Model, we see that the decision boundary, that is the border between the lighter green and lighter red regions, does a decent job of separating the green circles from the red triangles. However we do notice that there are two red triangles in the light green region. This indicates that they have been classified incorrectly by the decision stump.

Each of the base models will contribute a different amount to the final ensemble model. The influence that a particular base model contributes is going to be dependent on the *number* of errors it makes, or for regression, the *magnitude* of the errors it makes. We don't want a decision stump that does a terrible job of classifying the data to have the same say as a decision stump that does a great job. Once we are able to evaluate the Result of the 1st Base Model, we can Weight the Model and assign it a value, here indicated by ~alpha_1~.

To prepare for the next stage of the sequential learning process, we need to Reweight the Data. The instances of the training data that were classified incorrectly by the 1st Base Model, the two red triangles in the middle right, are given a larger weight than the other data instances indicated by their larger size. By assigning those misclassified points a larger weight, we are asking the 2nd Base Model to give them preferential treatment during the Model Fitting.

Taking a look at the Result of the 2nd Base Model, we see that is exactly what happens. The two larger red triangles are classified correctly by the 2nd Base Model. Once again we assign the base model a weight, ~alpha_2~ proportional to the errors it makes and prepare for the next stage of the sequential learning by reweighting the training data. The instances that were incorrectly classified by the 2nd Base Model, the two green circles in on the center right, are given a larger weight.

Once we have reached the predefined number of estimators for our AdaBoost model, the base models are ready to aggregate. In this example we have choosen ~n_estimators = 3~. The influence of each base model in the final ensemble model will be proportional to the ~alpha~ it was assigned during the training process.

* Adaptive Boosting Implementation
Let's take this opportunity to implement AdaBoost on a real dataset and solve a classification problem.

We will be using a dataset from [[https://archive.ics.uci.edu/dataset/19/car+evaluation][UCI's Machine Learning Repository]] to evaluate the acceptability of a car based on a set of features that encompasses their price and technical characteristics.

** Task 1
Create the base estimator for the AdaBoost classifier in the form a decision stump using ~DecisionTreeClassifier()~ and store it in a variable named ~decision_stump~. Recall, that a decision stump is a decision tree with only two leaf nodes.

Print the parameters of the decision stump using the ~.get_params()~ method.

*Hint*
A decision stump is a decision tree with ~max_depth=1~.

** Task 2
Using ~AdaBoostClassifier()~, create an AdaBoost classification model with the ~base_estimator~ parameter set to ~decision_stump~ and ~n_estimators~ set to ~5~. Store the model in a variable named ~ada_classifier~.

Print the parameters of the AdaBoost model using the ~.get_params()~ method.

** Task 3
Fit ~ada_classifier~ using the training features (~X_train~) and corresponding labels (~y_train~).

Predict the classes of the testing dataset (~X_test~) and store them as an array in a variable named ~y_pred~.

** Task 4
Now we will explore some of the most common evaluation metrics for classification on our trained AdaBoost model.

    - Calculate the accuracy and store it in a variable named ~accuracy~.

    - Calculate the precision and store it in a variable named ~precision~.

    - Calculate the recall and store it in a variable named ~recall~.

    - Calculate the f1-score and store it in a variable named ~f1~.

*Hint*
For each of the scoring metrics the relevant ~scikit-learn~ sintax looks as follows: ~metric_score~. The inputs to the function are the true values and predicted values in that order.

** Task 5
Take a look at the confusion matrix.

** Script.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  from sklearn.model_selection import train_test_split
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.ensemble import AdaBoostClassifier
  from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

  # Load dataset to a pandas DataFrame
  path_to_data = 'https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data'
  column_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep']
  df = pd.read_csv(path_to_data, names=column_names)

  target_column = 'accep'
  raw_feature_columns = [col for col in column_names if col != target_column]

  # Create dummy variables from the feature columns
  X = pd.get_dummies(df[raw_feature_columns], drop_first=True)

  # Convert target column to binary variable; 0 if 'unacc', 1 otherwise
  df[target_column] = np.where(df[target_column] == 'unacc', 0, 1)
  y = df[target_column]

  # Split the full dataset into training and testing sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.3)

  # 1. Create a decision stump base model using the Decision Tree Classifier and print its parameters
  decision_stump = DecisionTreeClassifier(max_depth=1)
  print(decision_stump.get_params())

  # 2. Create an Adaptive Boost Classifier and print its parameters
  ada_classifier = AdaBoostClassifier(estimator=decision_stump, n_estimators=5)
  print(ada_classifier.get_params())

  # 3. Fit the Adaptive Boost Classifier to the training data and get the list of predictions
  ada_classifier.fit(X_train, y_train)
  y_pred = ada_classifier.predict(X_test)

  # 4. Calculate the accuracy, precision, recall, and f1-score on the testing data
  accuracy = accuracy_score(y_test, y_pred)
  precision = precision_score(y_test, y_pred)
  recall = recall_score(y_test, y_pred)
  f1 = f1_score(y_test, y_pred)

  print(f'Test set accuracy:\t{accuracy}')
  print(f'Test set precision:\t{precision}')
  print(f'Test set recall:\t{recall}')
  print(f'Test set f1-score:\t{f1}')

  # 5. Print the confusion matrix
  test_conf_matrix = pd.DataFrame(
      confusion_matrix(y_test, y_pred, labels=[1, 0]),
      index=['actual yes', 'actual no'],
      columns=['predicted yes', 'predicted no']
  )
  print(f'Confusion Matrix:\n{test_conf_matrix.to_string()}')

#+end_src

#+RESULTS:
#+begin_example
{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 1, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}
{'algorithm': 'SAMME.R', 'base_estimator': 'deprecated', 'estimator__ccp_alpha': 0.0, 'estimator__class_weight': None, 'estimator__criterion': 'gini', 'estimator__max_depth': 1, 'estimator__max_features': None, 'estimator__max_leaf_nodes': None, 'estimator__min_impurity_decrease': 0.0, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'estimator__min_weight_fraction_leaf': 0.0, 'estimator__random_state': None, 'estimator__splitter': 'best', 'estimator': DecisionTreeClassifier(max_depth=1), 'learning_rate': 1.0, 'n_estimators': 5, 'random_state': None}
Test set accuracy:	0.8574181117533719
Test set precision:	0.7247191011235955
Test set recall:	0.8376623376623377
Test set f1-score:	0.7771084337349398
Confusion Matrix:
            predicted yes  predicted no
actual yes            129            25
actual no              49           316
#+end_example

* Gradient Boosting Overview
*Gradient Boosting* is a sequential ensembling method that can be used for both classification and regression. It can use any base machine learning model, though it is most commonly used with decision trees, known as Gradient Boosted Trees.

For Gradient Boost, the *Sequential Fitting Method* is accomplished by fitting a base model to the negative gradient of the error in the previous stage. The *Aggregation Method* is a weighted sum of those base models where the model weight is constant.

The training of a Gradient Boosted model is the process of determining the base model error at each step and using those to determine how to best formulate the subsequent base model.

[[./GradientBoostedTreesTheory.png]]

* Gradient Boosting Implementation
Now that we have taken a look at what is going on under the hood, we are ready to implement Gradient Boosting on a real dataset and solve a classification problem.

We will be using a dataset from [[https://archive.ics.uci.edu/dataset/19/car+evaluation][UCI's Machine Learning Repository]] to evaluate the acceptability of a car based on a set of features that encompasses their price and technical characteristics.

** Task 1
Create a Gradient Boosted Trees classification model using ~GradientBoostingClassifier()~ with ~n_estimators~ set to ~15~. Leave all other parameters to their default values. Store the model in a variable named ~grad_classifier~.

Print the parameters of the GradientBoostedTrees model using the ~.get_params()~ method.

** Task 2
Fit ~grad_classifier~ using the training features (~X_train~) and corresponding labels (~y_train~).

Predict the classes of the testing dataset (~X_test~) and store them as an array in a variable named ~y_pred~.

** Task 3
Now we will explore some of the most common evaluation metrics for classification on our trained Gradient Boosted Trees model.

    - Calculate the accuracy and store it in a variable named ~accuracy~.

    - Calculate the precision and store it in a variable named ~precision~.

    - Calculate the recall and store it in a variable named ~recall~.

    - Calculate the f1-score and store it in a variable named ~f1~.

Print the evaluation metrics you just stored.

** Script.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  from sklearn.model_selection import train_test_split
  from sklearn.ensemble import GradientBoostingClassifier
  from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

  # Load dataset to a pandas DataFrame
  path_to_data ='https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data'

  column_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep']

  df = pd.read_csv(path_to_data, names=column_names)
  target_column = 'accep'
  raw_feature_columns = [col for col in column_names if col != target_column]

  # Create dummy variables from the feature columns
  X = pd.get_dummies(df[raw_feature_columns], drop_first=True)

  # Convert target column to binary variable; 0 if 'unacc', 1 otherwise
  df[target_column] = np.where(df[target_column] == 'unacc', 0, 1)
  y = df[target_column]

  # Split the full dataset into training and testing sets
  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.30)

  # 1. Create a Gradient Boosting Classifier adn print its parameters
  grad_classifier = GradientBoostingClassifier(n_estimators=15)
  print(grad_classifier.get_params())

  # 2. Fit the Gradient Boosted Trees Classifier to the training data and get a list of predictions
  grad_classifier.fit(X_train, y_train)
  y_pred = grad_classifier.predict(X_test)

  # 3. Calculate the accuracy, precision, recall, and f1-score on the testin data
  accuracy = accuracy_score(y_test, y_pred)
  precision = precision_score(y_test, y_pred)
  recall = recall_score(y_test, y_pred)
  f1 = f1_score(y_test, y_pred)

  print(f'Test set accuracy:\t{accuracy}')
  print(f'Test set precision:\t{precision}')
  print(f'Test set recall:\t{recall}')
  print(f'Test set f1:\t{f1}')

  # 4. Code and print the confusion matrix
  test_conf_matrix = pd.DataFrame(
      confusion_matrix(y_test, y_pred, labels=[1, 0]),
      index=['actual yes', 'actual no'],
      columns=['predicted yes', 'predicted no']
  )

  print(f'Confusion Matrix:\n{test_conf_matrix.to_string()}')

#+end_src

#+RESULTS:
: {'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 15, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
: Test set accuracy:	0.8978805394990366
: Test set precision:	0.7885714285714286
: Test set recall:	0.8961038961038961
: Test set f1:	0.8389057750759877
: Confusion Matrix:
:             predicted yes  predicted no
: actual yes            138            16
: actual no              37           328
