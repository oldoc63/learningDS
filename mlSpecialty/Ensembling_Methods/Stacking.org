
* Stacking
Learn about the stacking ensemble learning method

** Introduction
We've seen how ensembling methods such as bagging or boosting can be used to manage bias and variance of a machine learning model and improve performance. In this article, we're going to learn about another ensemble method: Stacking! Stacking fundamentally operates differently from bagging and boosting in two ways:

    - The base estimators used in the ensemble need not be weak learners that improve performance each iteration.

    - Unlike bagging, there are no subsampling processes used. The stacking model effectively uses a full training set.

Stacking is a proven technique for improving prediction accuracy of standalone models. How does this work? Let's take a look!

** How does stacking work?
Consider the scenario in which we are handling a classification problem where we are exploring different models. When using a decision tree we find that oru model has poor generalization performance as a result of overfitting. Additionally, when using a logistic regression classification model, we discover our model parameters are hard to tune due to highly-correlated features. We might discover that while each model has a unique advantage over the others, each may also have a distinct drawback such as poor generalization accuracy or the inability to predict a specific class. As it turns out, there's no rule stating we can't take the best of both worlds!

Stacking can be thought of as a *democracy* of machine learning models, where different models are trained and subsequently cast their vote through their predictions. A majority-rules approach can be used for determining the final model prediction if we weighted each estimator equally. In practice, each base estimator may need to be weighted differently, so we have a later-stage model to learn how to appropiately weigh the predictions of all the prior base estimators.

Let's talk about creating a stacking model and how to set up our training data for it.

*** Training Base Estimators
We can select from combinations of different base estimators, such as a logistic regression model in combination with a decision tree. We could additionally select models of the same learning algorithms, but with different parameters, such as multiple decision trees with varying depths. The number of estimators is arbitrary, so it's good practice to explore how different combinations behave.

This introduces a problem, however. The estimators would be making predictions on data used in training. This puts our model at risk of overfitting. To avoid this, we use *K-Fold Cross Validation* as described next.

[[./k_fold_cross_validation.png]]

*** K-Fold Cross Validation
Consider 10 segments (or folds) and a stacking model that uses a logistic regression model and a decision tree model. Each estimator can be trained using data from 9 of the segments, and make predictions on the excluded 10th segment. We then append the predictions as new features to that 10th segment. Now 1/10th of the training data has two new features: one is the prediction made by the logistic regression model and the other is the prediction made by the decision tree model.

We want to do the same with the other 9 segments, so we rotate the excluded segment and repeat this process until all training data points are augmented with new features. The end result is a prediction made on each training sample, without having seen the sample during the training process.

*** Feature Augmentation
In our stacking setup, the base estimators need to be trained to make predictions on our training data. The prediction of each estimator will be appended to the corresponding data sample as a new feature. We thus augment the training data set with this additional information. The augmented training set is used by our later-stage stacking model to make the final prediction (see stacking.gif).

So, in summary, say our training dataset has 10,000 samples, 10 features, and we select 3 base estimators. We would train each base estimator on the training set and make predictions on the training set. Each estimator would make a prediction on each sample, therefore each sample will have 3 predictions. These 3 predictions are appended to the pre-existing 10 featrures. This leaves us with 10,000 training samples with 13 features each.


*** Training the stacking model
With our augmented training set, we're ready to prepare the final model that will make the official prediction from our stacking setup.

This later-stage model is a learning algorithm that we select just like the base estimators. We may even reuse an algorithm from an earlier stage here. The purpose of this model is to learn the proper weighting of the earlier estimator given our training samples now include a data point from each estimator. Some estimators may perform better than others, so our overall model should account for this.

The only difference in the training process is that the model will be designed to accept samples of the augmented size rather than the given size. This means if the given data set has n features and m base estimators, this model will require n + m features on each sample.

Once the later-stage model is trained, we feed testing data samples into the base estimators, which will append their predictions to the data sample. This sample is then given tho the later-stage model for our final prediction.

** Example
In the associated code for this article, we'll be referencing a water quality dataset. The features of this dataset are ones that can be used to determine how safe a supply of water is to drink or its "potability" represented by a 1 for drinkable and a 0 for not drinkable. For simplicity, let's take a look at a subset of the data with three of its features:

[[./three_features.png]]

For this example, we'll select a Logistic Regression model and a Decision Tree model as our base estimators. The table below shows how the training dataset expands with new features provided by the predictions of the trained base estimators.

[[./five_features.png]]

** Implementing stacking with ~scikit-learn~

#+begin_src python
  import pandas as pd
  df = pd.read_csv('water_potability')
  print(df.columns, df.shape)
#+end_src

Output:
#+begin_example
Index(['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity',
       'Organic_carbon', 'Trihalomethanes', 'Turbidity', 'Potability'],
      dtype='object') (2011, 10)
#+end_example

We'll take a train-test split of our data to handle the training process.

#+begin_src python
  X = water_potability.drop(['Potability'], axis=1)
  y = water_potability['Potability']

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=rand_state)
#+end_src

To assemble our ensemble, we'll make a dictionary of base estimators. This will be contained within the ~level_0_estimators~ dict. Also, our final estimator will be a Random Forest as represented with ~level_1_estimator~. Notice also how we prepare to add new features to our training dataset (as columns) in ~level_0_columns~.

#+begin_src python
  level_0_estimators = dict()
  level_0_estimators["logreg"] = LogisticRegression(random_state=rand_state)
  level_0_estimators["forest"] = RandomForestClassifier(random_state=rand_state)

  level_0_columns = [f"{name}_prediction" for name in level_0_estimators.keys()]

  level_1_estimator = RandomForestClassifier(random_state=rand_state)
#+end_src

Handling our k-fold cross-validation is fairly straightforward using ~sklearn.model_selection.StratifiedKFold~ from the ~scikit-learn~ library. The kfold is then given to the instantiated ~StackingClassifier~.

#+begin_src python
  kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=rand_state)
  stacking_clf = StakingClassifier(estimators=list(level_0_estimators.items()), final_estimator=level_1_estimator, passthrough=True, cv=kfold, stack_method="predict_proba")
#+end_src

Calling fit_transform on our classifier manages a lot of the heavy lifting. It will handle the training of our ~level_0~ base estimators along with the ~level_1_estimator~, make cross-validated predictions on the training set, and augment the training set with predictions from each estimator. Let's see how the resulting training dataset looks:

#+begin_src python
  df = pd.DataFrame(stacking_clf.fit_transform(X_train, y_train), columns=level_0_columns + list(X_train.columns))
#+end_src

Finally, with our full model trained, we can make predictions. Let's compare how our Stacking classifier performed with how a lone linear model and a lone decision tree model perform!

#+begin_src python
  y_val_pred = stacking_clf.predict(X_test)
  stacking_accuracy = accuracy_score(y_test, y_val_pred)

  vanilla_logistic_regression = LogisticRegression(random_state=rand_state).fit(X_train, y_train)
  lr_accuracy = accuracy_score(y_test, vanilla_logistic_regression.predict(X_test))

  vanilla_decision_tree = RandomForestClassifier(random_state=rand_state).fit(X_train, y_train)
  dt_accuracy = accuracy_score(y_test, vanilla_decision_tree.predict(X_test))

  print(f'Stacking accuracy: {staking_accuracy:.4f}')
  print(f'Logistic Regression accuracy: {lr_accuracy:.4f}')
  print(f'Decision Tree accuracy: {dt_accuracy:.4f}')
#+end_src

Output:

#+begin_example
Stacking accuracy: 0.6576
Logistic Regression accuracy: 0.5732
Decision Tree accuracy: 0.6526
#+end_example

** Limitations of stacking
Stacking is very powerful in that we remove the occasionally difficult choice of which learning algorithm to use for our problem. Depending on the use case, this benefit does come with some limitations worth noting:

    - Because we have an arbitrary number of learning algorithms in use, training an entire stacking model is computationally expensive. This is also true for deployed inference models.

    - Such a large model with many parameters means that a plethora of data is needed for proper training. Small datasets won't see significant gains with stacking. Stacking models typically yields marginal gains over the best single estimator used for the same problem. When successful, a stacked model may reduce error by 2% or less.

** Additional Considerations
How many layers can a stacking model have?

Stacking offers some creativity and openendedness in how we want to build our model. A vanilla stacking model may have one tier of models to contribute to the final prediction. We could alternatively construct a multi-tier approach in which one level of models feeds into a later stage of models before making a final prediction.

As with other machine learning models, we also have many parameters to tune and select from. We can enhance model diversity by using different training algorithms, different training sets, different feature subsets, and different hyperparameters. In the next section, we’ll use a k-fold cross-validation to preprocess the training data, where k=10. In reality, the value for k is arbitrary, and a different k folds may lead to performance improvement or decline.

** Conclusion
We have covered in-depth how stacking can combine the predictive power of various machine learning models. This property distinguishes stacking from other ensemble methods like bagging or boosting, while not being mutually exclusive in application. Further, we discussed how a k-fold cross-validation setup of the training data can help our stacking model avoid overfitting and augment the data to leverage all of its predictive strengths. In a coded example, we see this in action usint the scikit-learn libray where we assembled a stacking model with logistic regression estimators and decision tree estimators. We found tha using these estimators in a combination yielded higher prediction accuracy compared to using them independently.

** Practice

*** Question 1
How is stacking different from other ensemble methods like bagging or boosting?

A Stacking model can combine the predictive strengths of different types of estimators whereas bagging and boosting models tipically use one estimator.

Bagging and boosting are homogenous techniques  in that they reuse the same learning algorithms in the ensemble. Stacking creates an ensemble of an arbitrary number of learning algorithms and estimators.

*** Question 2
In what way(s) can we fine-tune a Stacking model?

All techniques mentioned can be used in fine tuning a Stacking model.

A stacking model setup provides a lot of open-endedness in the pursuit of finding the best performance. We have the freedom to make adjustments on any model in our setup as well as the data being used.
