
* An Introduction to Unsupervised Learning
Unsupervised Learning describes a class of algorithms that find patterns from /unlabelled or untagged/ data. In supervised learning, we deal with data that is labelled or tagged. For example, we predict continuous outcomes in /regression/ (for instance, predictin housing prices) or categorical outcomes in /classification/ (spam versus not spam, for example). However, often training data isn't labelled in this manner and this is where unsupervised learning comes in! It relies on using the *underlying distributions of features within the data* to figure out clusters of similarity.

Unsupervised learning methods are extremely important to the functioning of many real-world algorithms -think image recognition, ride-shares anticipating demands, snapchat filters and many more! There are three primary ways they're used:

    - *Clustering*: Identifying clusters within a dataset like identifying disease outbreak clusters or in natural language processing, in creating word clouds that are semantically related, etc.

    - *Dimensionality Reduction/Feature Extraction*: They can be used to condense the number of features in a dataset with a high number of features before applying a supervised learning algorithm.

    - *Automated Labelling/Tagging*: Unsupervised learning algorithms are immensely useful in categorizing uncategorized data and one can then perform the familiar classification/regression tasks using supervised learning.

In this module we will focus on two of the most commonly used unsupervised learning techniques -Principal Component Analysis (PCA) and K-Means Clustering. The former is most often used for dimensionality reduction and the latter is used in clustering problems primarily. After this module you will be able to:

    - Perform dimensionality reduction using PCA

    - Classify images using PCA

    - Find clusters within data using K-Means

    - Extract features using PCA and K-Means

** Introduction to Clustering
Often, the data you encounter in the real world won't be sorted into categories and won't have labeled answers to your question. Finding patterns in this type of data, unlabeled data, is a common theme in many machine learning applications. /Unsupervised Learning/ is how we find patterns and structure in these data.

*Clustering* is the most well-known unsupervised learning technique. It finds structure in unlabeled data by identifying similar groups, or /clusters./ Examples of clustering applications are:

    - *Recommendation engines:* group products to personalize the user experience

    - *Search engines:* group news topics and search results

    - *Market segmentation:* group customers based on geography, demography, and behaviors

    - *Image segmentation:* medical imaging or road scene segmentation on self-driving cars

    - *Text clustering:* group similar texts together based on word usage

The /Iris/ data set is a famous example of unlabeled data. It consist of measurements of sepals and petals on 50 different iris flowers. Here you can see a visualization of this data set that shows how the flowers naturally form three distinct clusters. We'll learn how to find those clusters in this lesson.

      [[./k_means_clustering.gif]]

** K-Means Clustering
The goal of clustering is to separate data so that data similar to one another are in the same group, while data different from one another are in different groups. So two question arise:

    - How many groups do we choose?

    - How do we define similarity?

/k-means/ is the most popular and well-known clustering algorithm, and it tries to address these two questions:

    - The 'k' refers to the number of clusters (groups) we expect to find in a dataset.

    - The 'means' refers to the average distance of data to each cluster center, also known as the /centroid/, which we are trying to minimize.

It is an iterative approach:

    1. Place ~k~ random centroids for the initial clusters.

    2. Assign data samples to the nearest centroid.

    3. Calculate new centroids based on the above-assigned data samples.

    4. Repeat Steps 2 and 3 until convergence.

/Convergence/ occurs when points don't move between clusters and centroids stabilize. This iterative process of updating clusters and centroids is called /training./

Once we are happy with our clusters, we can take a new unlabeled datapoint and quickly assign it to the appropriate cluster. This is called /inference./

In practice it can be tricky to know how many clusters to look for. In the example here, the algorithm is sorting the data into k=2 clusters.

We will first implement k-means the hard way (to help you understand the algorithm) and then the easy way using the sklearn library!

** Iris Dataset
Before we implement the k-means algorithm, let's find a dataset. The sklearn package embeds some datasets and sample images. One of them is the Iris dataset.

The Iris dataset consist of measurements of sepals and petals of 3 different plant species:

    - /Iris setosa/

    - /Iris versicolor/

    - /Iris virginica/

The sepal is the part that encases and protects the flower when it is in the bud stage. A petal is a leaflike part that is often colorful.

From sklearn library, import the datasets module:

#+begin_src python
from sklearn import datasets
#+end_src

To load the Iris dataset:

#+begin_src python
iris = datasets.load_iris()
#+end_src

The Iris dataset looks like:

#+begin_src python
[[ 5.1  3.5  1.4  0.2 ]
 [ 4.9  3.   1.4  0.2 ]
 [ 4.7  3.2  1.3  0.2 ]
 [ 4.6  3.1  1.5  0.2 ]
   . . .
 [ 5.9  3.   5.1  1.8 ]]
#+end_src

We call each row of data a /sample/. For example, each flower is one sample.

Each characteristic we are interested in is a /feature/. For example, petal length is a feature of this dataset.

The features of the dataset are:

    - *Column 0*: Sepal length

    - *Column 1*: Sepal width

    - *Column 2*: Petal length

    - *Column 3*: Petal width

The 3 species of Iris plants are what we are going to cluster later in this lesson.

*** Task 1
Import the datasets module and load the Iris data.

*** Task 2
Every dataset from sklearn comes with a bunch of different information (not just the data) and is stored in a similar fashion.

First, let's take a look at the most important thing, the sample data:

Each row is a plant!

*** Task 3
The iris dataset comes with target values. The target values indicate which cluster each flower belongs to. In real life clustering problems, you will work with unlabeled data sets that don't come with targets. For the sake of practice, we can ignore the targets while we are clustering. After we have clustered the data the targets can be used to chech our work.

Take a look at the target values:

The ~iris.target~ values give the /ground truth/ for the Iris dataset. Ground truth, in this case, is the number corresponding to the flower that we are trying to learn.

*** Task 4
Let's take a look at one single row of data and the corresponding target.

*** Task 5
It is always a good idea to read the descriptions of the data.

*** Script.py

#+begin_src python :results output
  import matplotlib.pyplot as plt
  from sklearn import datasets

  iris = datasets.load_iris()
  #print(iris.data)
  #print(iris.target)

  #print(iris.data[0, :], iris.target[0])

  #print(iris.DESCR)

#+end_src

#+RESULTS:

** Visualize Before K-Means
To get a better sense of the data in the iris.data matrix, let's visualize it!

With Matplotlib, we can create 2D scatter plot of the Iris dataset using two of its features (sepal length vs. petal length). Of course there are four different features that we could plot, but it's much easier to visualize only two dimension.

The sepal length measurements are stored in column 0 of the matrix, and the petal length measurements are stored in column 2 of the matrix.

But how do we get these values?

Suppose we only want to retrieve the values that are in column ~0~ of a matrix, we can use the Numpy/Pandas notation ~[:, 0]~ like so:

#+begin_src python
matrix[:, 0]
#+end_src

[:, 0] can be translated to [all_rows, column_0]

Once you have the measurements we need, we can make a scatter plot like this:

#+begin_src python
plt.scatter(x, y)
#+end_src

To show the plot:

#+begin_src python
plt.show()
#+end_src

Let's try this! But this time, plot the sepal length (column 0) vs. sepal width (column 1) instead.

*** Task 1
Store ~iris.data~ in a variable named ~samples~.

*** Task 2
Create a list named ~x~ that contains the column ~0~ values of ~samples~.

Create a list named ~y~ that contains the column ~1~ values of ~samples~.

*** Task 3
Use the ~.scatter()~ function to create a scatter plot of ~x~ and ~y~.

Because some of the data samples have the exact same features, let's add ~alpha=0.5~.

*** Task 4
Call the .show() function to display the graph.

We've also included x-axis label and y-axis label.

Adding alpha=0.5 makes some points look darker than others. The darker spots are where there is overlap.

*** Script.py

#+begin_src python :results output
  import matplotlib.pyplot as plt
  from sklearn import datasets

  iris = datasets.load_iris()

  # Store iris.data
  samples = iris.data

  # Create x and y
  x = samples[:, 0]
  y = samples[:, 1]

  # Plot x and y
  plt.scatter(x, y, alpha=0.5)

  plt.xlabel('sepal length (cm)')
  plt.ylabel('sepal width (cm)')

  plt.show()

#+end_src

#+RESULTS:

** Implementing K-Means: Step 1
The K-Means algorithm:

    1. *Place ~k~ random centroids for the initial clusters.*

    2. Assign data samples to the nearest centroid.

    3. Update centroids based on the above-assigned data samples.

    4. Repeat Steps 2 and 3 until convergence.

After looking at the scatter plot and having a better understanding of the Iris data, let's start implementing the k-means algorithm.

In this exercise, we will implement Step 1.

Because we expect there to be three clusters (for the three species of flowers), let's implement k-means where the ~k~ is 3. In real-life situations you won't always know how many clusters to look for. We'll learn more about how to choose k later.

Using the NumPy library, we will create three /random/ initial centroids and plot them along with our samples.

*** Task 1
First, create a variable named k and set it to 3.

*** Task 2
Then, use NumPy's ~random.uniform()~ function to generate random values in two lists:

    - a ~centroids_x~ list that will have ~k~ random values between ~min(x)~ and ~max(x)~

    - a ~centroids_y~ list that will have ~k~ random values between ~min(y)~ and ~max(y)~

The random.uniform() function looks like:

#+begin_src python
  np.random.uniform(low, high, size)
#+end_src

The ~centroids_x~ will have the x-values for our initial random centroids and the ~centroids_y~ will have the y-values for our initial random centroids.

*Hint*
Use the lower and upper  bounds of the x and y values.

You can also use NumPy's ~random.randint()~ function, but it will generate random ~int~ s instead of ~float~ s.

*** Task 3
Create an array named centroids and use the ~zip()~ function to add ~centroids_x~ and ~centroids_y~ to it.

Then print centroids.

The ~centroids~ list should now have all the initial centroids.

*** Task 4
Make a scatter plot of y vs x. (Remember that you’re only looking at two out of four dimensions of the data set. Based on that it might not look like there are three distinct clusters, but since we are missing two dimensions this plot won’t tell the whole story.)

Make a scatter plot of centroids_y vs centroids_x.

Show the plots to see your centroids!

Adding alpha=0.5 makes the points look darker than others. This is because some of the points might have the exact the same values. The dots are darker because they are stacked!

*** Script.py

#+begin_src python :results output
  import matplotlib.pyplot as plt
  import numpy as np
  from sklearn import datasets

  iris = datasets.load_iris()

  samples = iris.data

  x = samples[:, 0]
  y = samples[:, 1]

  # Number of clusters
  k = 3

  # X coordinates of random centroids
  centroids_x = np.random.uniform(min(x), max(x), size=k)

  centroids_y = np.random.uniform(min(y), max(y), size=k)

  # Create centroids array
  centroids = np.array(list(zip(centroids_x, centroids_y)))

  print(centroids)

  # Make a scatter plot of y and x
  plt.scatter(x, y, alpha=0.5)

  # Make a scatter plot of centroids_y, centroid_x
  plt.scatter(centroids_x, centroids_y)

  plt.xlabel('sepal length (cm)')
  plt.ylabel('sepal width (cm)')

  plt.show()

#+end_src

#+RESULTS:
: [[4.89566089 3.01716756]
:  [6.97357791 2.47696023]
:  [6.23522749 2.62739094]]
