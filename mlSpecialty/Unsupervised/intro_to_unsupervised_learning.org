
* An Introduction to Unsupervised Learning
Unsupervised Learning describes a class of algorithms that find patterns from /unlabelled or untagged/ data. In supervised learning, we deal with data that is labelled or tagged. For example, we predict continuous outcomes in /regression/ (for instance, predictin housing prices) or categorical outcomes in /classification/ (spam versus not spam, for example). However, often training data isn't labelled in this manner and this is where unsupervised learning comes in! It relies on using the *underlying distributions of features within the data* to figure out clusters of similarity.

Unsupervised learning methods are extremely important to the functioning of many real-world algorithms -think image recognition, ride-shares anticipating demands, snapchat filters and many more! There are three primary ways they're used:

    - *Clustering*: Identifying clusters within a dataset like identifying disease outbreak clusters or in natural language processing, in creating word clouds that are semantically related, etc.

    - *Dimensionality Reduction/Feature Extraction*: They can be used to condense the number of features in a dataset with a high number of features before applying a supervised learning algorithm.

    - *Automated Labelling/Tagging*: Unsupervised learning algorithms are immensely useful in categorizing uncategorized data and one can then perform the familiar classification/regression tasks using supervised learning.

In this module we will focus on two of the most commonly used unsupervised learning techniques -Principal Component Analysis (PCA) and K-Means Clustering. The former is most often used for dimensionality reduction and the latter is used in clustering problems primarily. After this module you will be able to:

    - Perform dimensionality reduction using PCA

    - Classify images using PCA

    - Find clusters within data using K-Means

    - Extract features using PCA and K-Means

** Introduction to Clustering
Often, the data you encounter in the real world won't be sorted into categories and won't have labeled answers to your question. Finding patterns in this type of data, unlabeled data, is a common theme in many machine learning applications. /Unsupervised Learning/ is how we find patterns and structure in these data.

*Clustering* is the most well-known unsupervised learning technique. It finds structure in unlabeled data by identifying similar groups, or /clusters./ Examples of clustering applications are:

    - *Recommendation engines:* group products to personalize the user experience

    - *Search engines:* group news topics and search results

    - *Market segmentation:* group customers based on geography, demography, and behaviors

    - *Image segmentation:* medical imaging or road scene segmentation on self-driving cars

    - *Text clustering:* group similar texts together based on word usage

The /Iris/ data set is a famous example of unlabeled data. It consist of measurements of sepals and petals on 50 different iris flowers. Here you can see a visualization of this data set that shows how the flowers naturally form three distinct clusters. We'll learn how to find those clusters in this lesson.

      [[./k_means_clustering.gif]]

** K-Means Clustering
The goal of clustering is to separate data so that data similar to one another are in the same group, while data different from one another are in different groups. So two question arise:

    - How many groups do we choose?

    - How do we define similarity?

/k-means/ is the most popular and well-known clustering algorithm, and it tries to address these two questions:

    - The 'k' refers to the number of clusters (groups) we expect to find in a dataset.

    - The 'means' refers to the average distance of data to each cluster center, also known as the /centroid/, which we are trying to minimize.

It is an iterative approach:

    1. Place ~k~ random centroids for the initial clusters.

    2. Assign data samples to the nearest centroid.

    3. Calculate new centroids based on the above-assigned data samples.

    4. Repeat Steps 2 and 3 until convergence.

/Convergence/ occurs when points don't move between clusters and centroids stabilize. This iterative process of updating clusters and centroids is called /training./

Once we are happy with our clusters, we can take a new unlabeled datapoint and quickly assign it to the appropriate cluster. This is called /inference./

In practice it can be tricky to know how many clusters to look for. In the example here, the algorithm is sorting the data into k=2 clusters.

We will first implement k-means the hard way (to help you understand the algorithm) and then the easy way using the sklearn library!
