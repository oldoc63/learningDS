
* Introduction to Implementing PCA
In this lesson, we will be implementing Principal Component Analysis (PCA) using the Python libraries NumPy and scikit-learn.

The motivation of Principal Component Analysis (PCA) is to find a new set of features that are oredered by the amount of variation (and therefore, information) they contain. We can then select a subset of these PCA features. This leaves us with lower-dimensional data that still retains most of the information contained in the larger dataset.

In this lesson, we will:

    - Implement PCA in Numpy step-by-step

    - Implement PCA in scikit-learn using only a few lines of code

    - Use principal components to train a machine learning model

    - Visualize principal components using image data

For the next few exercises, we will use a dataset that describes several types of dry beans separated into seven categories.

We will begin by taking a look at the features that describe different categories of beans.

** Task 1
A file named ~Dry_Bean.csv~ is loaded for you as a Pandas DataFrame named ~df~. Print the head of the DataFrame. Note that all the columns are numerical except for the ~Class~ column.

** Task 2
Extract the numerical features from ~df~ by dropping the ~'Class'~ column, and save them as a new DataFrame called ~data_matrix~.

** Script.py

#+begin_src python :results output
  import pandas as pd

  # Read de csv data as a DataFrame
  df = pd.read_csv('Dry_Bean.csv')

  # Remove null and na values
  df.dropna()

  # Print the DataFrame head
  print(df.head())

  # Extract the numerical columns
  data_matrix = df.drop(columns='Class')

  # Extract the classes
  classes = df['Class']

  # Print data_matrix
  print(data_matrix)

#+end_src

#+RESULTS:
#+begin_example
    Area  Perimeter  MajorAxisLength  ...  ShapeFactor3  ShapeFactor4  Class
0  28395    610.291       208.178117  ...      0.834222      0.998724  SEKER
1  28734    638.018       200.524796  ...      0.909851      0.998430  SEKER
2  29380    624.110       212.826130  ...      0.825871      0.999066  SEKER
3  30008    645.884       210.557999  ...      0.861794      0.994199  SEKER
4  30140    620.134       201.847882  ...      0.941900      0.999166  SEKER

[5 rows x 17 columns]
        Area  Perimeter  ...  ShapeFactor3  ShapeFactor4
0      28395    610.291  ...      0.834222      0.998724
1      28734    638.018  ...      0.909851      0.998430
2      29380    624.110  ...      0.825871      0.999066
3      30008    645.884  ...      0.861794      0.994199
4      30140    620.134  ...      0.941900      0.999166
...      ...        ...  ...           ...           ...
13606  42097    759.696  ...      0.642988      0.998385
13607  42101    757.499  ...      0.676099      0.998219
13608  42139    759.321  ...      0.676884      0.996767
13609  42147    763.779  ...      0.668237      0.995222
13610  42159    772.237  ...      0.616221      0.998180

[13611 rows x 16 columns]
#+end_example

* Implementing PCA
** Numpy
In this exercise, we will perform PCA using the NumPy method ~np.linalg.eig~, which performs eigendecomposition and outputs the eigenvalues and eigenvectors.

The *eigenvalues* are related to the relative variation described by each principal component. The *eigenvectors* are also known as the principal axes. They tell us how to transform (rotate) our data into new features that capture this variation.

To implement this in Python:

#+begin_src python
  correlation_matrix = data_matrix.corr()
  eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)
#+end_src

    1. First, we generate a correlation matrix using ~.corr()~

    2. Next, we use ~np.linalg.eig()~ to perform eigendecomposition on the correlation matrix. This gives us two outputs -the eigenvalues and eigenvectors.

*** Task 1
The DataFrame that you created in the previous exercise, ~data_matrix~, is loaded for you. Find the correlation matrix for the features in ~data_matrix~ and save the result as ~correlation_matrix~.

Uncomment the lines that show the heatmap and notice that there are pairs of features with very high correlations.

*** Task 2
Using the ~correlation_matrix~, find the eigenvalues and eigenvectors using the NumPy method ~np.linalg.eig()~. Save them as ~eigenvalues~ and ~eigenvectors~, respectively, then print them out.

You should see that ~eigenvalues~ contains 16 numbers, while ~eigenvectors~ contains 16 vectors with 16 values each.

*Hint*
Use the method ~np.linalg.eig(data_matrix)~ to retrieve the eigenvalues and eigenvectors.

** Analysis
After performing PCA, we generally want to know how useful the new features are. One way to visualize this is to create a scree plot. which shows the proportion of information described by each principal component.

The proportion of information explained is equal to the relative size of each eigenvalue.

To create a scree plot, we can then plot these relative proportions.

From this plot, we see thath the first principal component explains about 50% of the variation in the data, the second explains about 30%, and so on.

Another way to view this is to see how many principal axes it takes to reach around 95% of the total amount of information. Ideally, we'd like to retain as few features as possible while still reaching this threshold.

To do this, we need to calculate the cumulative sum of the ~info_prop~ vector we created earlier and plot it using matplotlib.

From this plot, we see that four principal axes account for 95% of the variation in the data.

** Script.py

#+begin_src python :results output
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  import seaborn as sns

  # Read de csv data as a DataFrame
  df = pd.read_csv('Dry_Bean.csv')

  # Remove null and na values
  df.dropna()

  # Extract the numerical columns
  data_matrix = df.drop(columns='Class')

  # Extract the classes
  classes = df['Class']

  # Use the .corr() method on data_matrix to get the correlation matrix
  correlation_matrix = data_matrix.corr()

  # Heatmap code:
  red_blue = sns.diverging_palette(220, 20, as_cmap=True)
  sns.heatmap(correlation_matrix, vmin = -1, vmax = 1, cmap = red_blue)
  plt.show()

  # Perform decomposition using np.linalg.eig
  eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)

  #print('eigenvectors: ')
  #print(eigenvectors)

  print('eigenvalues: ')
  print(eigenvalues)

  # Proportion of information explained
  info_prop = eigenvalues / eigenvalues.sum()
  print('info_prop: ')
  print(info_prop)

  # Plot the relative proportions
  plt.plot(np.arange(1, len(info_prop) + 1), info_prop, 'bo-')
  plt.show()

  # Calculate the cumulative sum of info_prop
  cum_info_prop = np.cumsum(info_prop)

  # Plot cum_info_prop using matplotlib
  plt.plot(np.arange(1,len(info_prop)+1),
         cum_info_prop,
         'bo-')
  plt.hlines(y=.95, xmin=0, xmax=15)
  plt.vlines(x=4, ymin=0, ymax=1)
  plt.show()

#+end_src

#+RESULTS:
#+begin_example
eigenvalues:
[8.87463018e+00 4.22895571e+00 1.28105028e+00 8.18252847e-01
 4.38286865e-01 1.83961749e-01 1.11624116e-01 5.20132000e-02
 8.26026072e-03 1.45388993e-03 1.05418870e-03 2.93982938e-04
 1.48794566e-04 1.00102669e-05 1.78479175e-06 2.14611337e-06]
info_prop:
[5.54664386e-01 2.64309732e-01 8.00656422e-02 5.11408029e-02
 2.73929290e-02 1.14976093e-02 6.97650724e-03 3.25082500e-03
 5.16266295e-04 9.08681206e-05 6.58867938e-05 1.83739336e-05
 9.29966038e-06 6.25641679e-07 1.11549485e-07 1.34132086e-07]
#+end_example

** Exercise

*** Task 1
The array of ~eigenvalues~ is loaded for you. Find the proportion of each eigen value compared to the sum of the eigenvalues. Save the results as an array named ~info_prop~.

Then, generate a scree plot. Notice that the first principal axes contain nearly 50% of the information.

*Hint*
Use the method ~eigenvalues.sum()~ to find the sum of all the elements of the eigenvalues.

*** Task 2
Using ~info_prop~, calculate the cumulative sum of the information proportions. Save the resulting array as ~cum_info_prop~.

Then, generate a plot of the cumulative proportion of information explained. Notice how four principal components describe about 95% of the information in the data.

*** Script.py

#+begin_src python :results output
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt

  eigenvalues = pd.read_csv('eigenvalues.csv')['eigenvalues'].values

  # 1. Find the proportion of information for each eigenvector, which is equal to the eigenvalues divided by the sum of all eigenvalues
  info_prop = eigenvalues / eigenvalues.sum()

  ## Plot the principal axes vs the information proportions for each principal axis
  plt.plot(np.arange(1,len(info_prop)+1), info_prop, 'bo-', linewidth=2)
  plt.title('Scree Plot')
  plt.xlabel('Principal Axes')
  plt.xticks(np.arange(1,len(info_prop)+1))
  plt.ylabel('Percent of Information Explained')
  plt.show()
  plt.clf()

  # 2. Find the cumulative sum of the proportions
  cum_info_prop = np.cumsum(info_prop)

  ## Plot the cumulative proportions array

  plt.plot(cum_info_prop, 'bo-', linewidth=2)
  plt.hlines(y=.95, xmin=0, xmax=15)
  plt.vlines(x=3, ymin=0, ymax=1)
  plt.title('Cumulative Information percentages')
  plt.xlabel('Principal Axes')
  plt.xticks(np.arange(1,len(info_prop)+1))
  plt.ylabel('Cumulative Proportion of Variance Explained')
  plt.show()


#+end_src

#+RESULTS:
