
* Introduction to Implementing PCA
In this lesson, we will be implementing Principal Component Analysis (PCA) using the Python libraries NumPy and scikit-learn.

The motivation of Principal Component Analysis (PCA) is to find a new set of features that are oredered by the amount of variation (and therefore, information) they contain. We can then select a subset of these PCA features. This leaves us with lower-dimensional data that still retains most of the information contained in the larger dataset.

In this lesson, we will:

    - Implement PCA in Numpy step-by-step

    - Implement PCA in scikit-learn using only a few lines of code

    - Use principal components to train a machine learning model

    - Visualize principal components using image data

For the next few exercises, we will use a dataset that describes several types of dry beans separated into seven categories.

We will begin by taking a look at the features that describe different categories of beans.

** Task 1
A file named ~Dry_Bean.csv~ is loaded for you as a Pandas DataFrame named ~df~. Print the head of the DataFrame. Note that all the columns are numerical except for the ~Class~ column.

** Task 2
Extract the numerical features from ~df~ by dropping the ~'Class'~ column, and save them as a new DataFrame called ~data_matrix~.

** Script.py

#+begin_src python :results output
  import pandas as pd

  # Read de csv data as a DataFrame
  df = pd.read_csv('Dry_Bean.csv')

  # Remove null and na values
  df.dropna()

  # Print the DataFrame head
  print(df.head())

  # Extract the numerical columns
  data_matrix = df.drop(columns='Class')

  # Extract the classes
  classes = df['Class']

  # Print data_matrix
  print(data_matrix)

#+end_src

#+RESULTS:
#+begin_example
    Area  Perimeter  MajorAxisLength  ...  ShapeFactor3  ShapeFactor4  Class
0  28395    610.291       208.178117  ...      0.834222      0.998724  SEKER
1  28734    638.018       200.524796  ...      0.909851      0.998430  SEKER
2  29380    624.110       212.826130  ...      0.825871      0.999066  SEKER
3  30008    645.884       210.557999  ...      0.861794      0.994199  SEKER
4  30140    620.134       201.847882  ...      0.941900      0.999166  SEKER

[5 rows x 17 columns]
        Area  Perimeter  ...  ShapeFactor3  ShapeFactor4
0      28395    610.291  ...      0.834222      0.998724
1      28734    638.018  ...      0.909851      0.998430
2      29380    624.110  ...      0.825871      0.999066
3      30008    645.884  ...      0.861794      0.994199
4      30140    620.134  ...      0.941900      0.999166
...      ...        ...  ...           ...           ...
13606  42097    759.696  ...      0.642988      0.998385
13607  42101    757.499  ...      0.676099      0.998219
13608  42139    759.321  ...      0.676884      0.996767
13609  42147    763.779  ...      0.668237      0.995222
13610  42159    772.237  ...      0.616221      0.998180

[13611 rows x 16 columns]
#+end_example

* Implementing PCA
** Numpy
In this exercise, we will perform PCA using the NumPy method ~np.linalg.eig~, which performs eigendecomposition and outputs the eigenvalues and eigenvectors.

The *eigenvalues* are related to the relative variation described by each principal component. The *eigenvectors* are also known as the principal axes. They tell us how to transform (rotate) our data into new features that capture this variation.

To implement this in Python:

#+begin_src python
  correlation_matrix = data_matrix.corr()
  eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)
#+end_src

    1. First, we generate a correlation matrix using ~.corr()~

    2. Next, we use ~np.linalg.eig()~ to perform eigendecomposition on the correlation matrix. This gives us two outputs -the eigenvalues and eigenvectors.

*** Task 1
The DataFrame that you created in the previous exercise, ~data_matrix~, is loaded for you. Find the correlation matrix for the features in ~data_matrix~ and save the result as ~correlation_matrix~.

Uncomment the lines that show the heatmap and notice that there are pairs of features with very high correlations.

*** Task 2
Using the ~correlation_matrix~, find the eigenvalues and eigenvectors using the NumPy method ~np.linalg.eig()~. Save them as ~eigenvalues~ and ~eigenvectors~, respectively, then print them out.

You should see that ~eigenvalues~ contains 16 numbers, while ~eigenvectors~ contains 16 vectors with 16 values each.

*Hint*
Use the method ~np.linalg.eig(data_matrix)~ to retrieve the eigenvalues and eigenvectors.

** Analysis
After performing PCA, we generally want to know how useful the new features are. One way to visualize this is to create a scree plot. which shows the proportion of information described by each principal component.

The proportion of information explained is equal to the relative size of each eigenvalue.

To create a scree plot, we can then plot these relative proportions.

From this plot, we see thath the first principal component explains about 50% of the variation in the data, the second explains about 30%, and so on.

Another way to view this is to see how many principal axes it takes to reach around 95% of the total amount of information. Ideally, we'd like to retain as few features as possible while still reaching this threshold.

To do this, we need to calculate the cumulative sum of the ~info_prop~ vector we created earlier and plot it using matplotlib.

From this plot, we see that four principal axes account for 95% of the variation in the data.

** Script.py

#+begin_src python :results output
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  import seaborn as sns

  # Read de csv data as a DataFrame
  df = pd.read_csv('Dry_Bean.csv')

  # Remove null and na values
  df.dropna()

  # Extract the numerical columns
  data_matrix = df.drop(columns='Class')

  # Extract the classes
  classes = df['Class']

  # Use the .corr() method on data_matrix to get the correlation matrix
  correlation_matrix = data_matrix.corr()

  # Heatmap code:
  red_blue = sns.diverging_palette(220, 20, as_cmap=True)
  sns.heatmap(correlation_matrix, vmin = -1, vmax = 1, cmap = red_blue)
  plt.show()

  # Perform decomposition using np.linalg.eig
  eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)

  #print('eigenvectors: ')
  #print(eigenvectors)

  print('eigenvalues: ')
  print(eigenvalues)

  # Proportion of information explained
  info_prop = eigenvalues / eigenvalues.sum()
  print('info_prop: ')
  print(info_prop)

  # Plot the relative proportions
  plt.plot(np.arange(1, len(info_prop) + 1), info_prop, 'bo-')
  plt.show()

  # Calculate the cumulative sum of info_prop
  cum_info_prop = np.cumsum(info_prop)

  # Plot cum_info_prop using matplotlib
  plt.plot(np.arange(1,len(info_prop)+1),
         cum_info_prop,
         'bo-')
  plt.hlines(y=.95, xmin=0, xmax=15)
  plt.vlines(x=4, ymin=0, ymax=1)
  plt.show()

#+end_src

#+RESULTS:
#+begin_example
eigenvalues:
[8.87463018e+00 4.22895571e+00 1.28105028e+00 8.18252847e-01
 4.38286865e-01 1.83961749e-01 1.11624116e-01 5.20132000e-02
 8.26026072e-03 1.45388993e-03 1.05418870e-03 2.93982938e-04
 1.48794566e-04 1.00102669e-05 1.78479175e-06 2.14611337e-06]
info_prop:
[5.54664386e-01 2.64309732e-01 8.00656422e-02 5.11408029e-02
 2.73929290e-02 1.14976093e-02 6.97650724e-03 3.25082500e-03
 5.16266295e-04 9.08681206e-05 6.58867938e-05 1.83739336e-05
 9.29966038e-06 6.25641679e-07 1.11549485e-07 1.34132086e-07]
#+end_example

** Exercise

*** Task 1
The array of ~eigenvalues~ is loaded for you. Find the proportion of each eigen value compared to the sum of the eigenvalues. Save the results as an array named ~info_prop~.

Then, generate a scree plot. Notice that the first principal axes contain nearly 50% of the information.

*Hint*
Use the method ~eigenvalues.sum()~ to find the sum of all the elements of the eigenvalues.

*** Task 2
Using ~info_prop~, calculate the cumulative sum of the information proportions. Save the resulting array as ~cum_info_prop~.

Then, generate a plot of the cumulative proportion of information explained. Notice how four principal components describe about 95% of the information in the data.

*** Script.py

#+begin_src python :results output
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt

  eigenvalues = pd.read_csv('eigenvalues.csv')['eigenvalues'].values

  # 1. Find the proportion of information for each eigenvector, which is equal to the eigenvalues divided by the sum of all eigenvalues
  info_prop = eigenvalues / eigenvalues.sum()

  ## Plot the principal axes vs the information proportions for each principal axis
  plt.plot(np.arange(1,len(info_prop)+1), info_prop, 'bo-', linewidth=2)
  plt.title('Scree Plot')
  plt.xlabel('Principal Axes')
  plt.xticks(np.arange(1,len(info_prop)+1))
  plt.ylabel('Percent of Information Explained')
  plt.show()
  plt.clf()

  # 2. Find the cumulative sum of the proportions
  cum_info_prop = np.cumsum(info_prop)

  ## Plot the cumulative proportions array

  plt.plot(cum_info_prop, 'bo-', linewidth=2)
  plt.hlines(y=.95, xmin=0, xmax=15)
  plt.vlines(x=3, ymin=0, ymax=1)
  plt.title('Cumulative Information percentages')
  plt.xlabel('Principal Axes')
  plt.xticks(np.range(1,len(info_prop)+1))
  plt.ylabel('Cumulative Proportion of Variance Explained')
  plt.show()

#+end_src

#+RESULTS:

* Implementing PCA using Scikit-Learn
Another way to perform PCA is using the scikit-learn module ~sklearn.decomposition.PCA~.

The steps to perform PCA using this method are:

    - Standardize the data matrix. This is done by subtracting the mean and dividing by the standard deviation of each column vector.

    #+begin_src python
      mean = data.mean(axis=0)
      sttd = data.std(axis=0)
      data_standardized = (data - mean) / sttd
    #+end_src

    - Perform eigendecomposition by fitting the standardized data. We can access the eigenvectors using the ~components_~ attribute an the proportional sizes of the eigenvalues using the ~explained_variance_ratio_~ attribute.

    #+begin_src python
      pca = PCA()

      components = pca.fit(data_standardized).components_

      components = pd.DataFrame(components).transpose()

      components.index = data_matrix.columns

      print(components)

    #+end_src

    #+begin_src python
      var_ratio = pca.explained_variance_ratio_

      var_ratio = pd.DataFrame(var_ratio).transpose()

      print(var_ratio)

    #+end_src

This module has many advantages over the NumPy method, including a number of different solvers to calculate the principal axes. This can greatly improve the quality of the results.

** Task 1
The data matrix is loaded for you as ~data_matrix~. Standardize all of the columns in the data matrix asn save  the result as ~data_matrix_standardized~, then print out the first 5 rows to inspect it.

*Hint*
Standardize the ~data_matrix~ by subtracting the mean and dividing by the standard deviation, (~data_matrix - mean / sttd~).

** Task 2
Will we now use the ~PCA~ module from ~sklearn.decomposition.PCA~ to calculate the principal components (eigenvectors) for the standardized data matrix.

Fit and calculate the principal components using the ~fit()~ method and ~components_~ attribute. Save the results as components and then uncomment the code to transpose, index, and print the components/eigenvectors.

You should see 16 eigenvectors. Each one should have a value (or weight) for the each of the 16 original features.

*Hint*
Use ~pca.fit(data_matrix_standardized)~ to calculate principal components for the standardized data. Then use the ~.components_~ attribute to extract the components.

** Task 3
Using the same ~pca~ object from the previous checkpoint, we can retrieve the variance (information) ratios for each principal component using the ~explained_variance_ratio_~ property.

Save the variance ratios as ~var_ratio~ and then save the values as DataFrame and print them out.

Are they consistent with our original finding that the first four components account for about 95% of the information un the data?

*Hint*
The ~explained_variance_ratio~ is the percentage of variance explained by each of the selected components.

** Script.py

#+begin_src python :results output
  import numpy as np
  import pandas as pd
  from sklearn.decomposition import PCA

  data_matrix = pd.read_csv('data_matrix.csv')

  # 1. Standardize the data matrix
  mean = data_matrix.mean(axis=0)
  sttd = data_matrix.std(axis=0)
  data_matrix_standardized = (data_matrix - mean) / sttd
  print(data_matrix_standardized.head())

  # 2. Find the principal components
  pca = PCA()
  components = pca.fit(data_matrix_standardized).components_
  components = pd.DataFrame(components).transpose()
  components.index = data_matrix.columns
  print(components)

  # 3. Calculate the variance/info ratios
  var_ratio = pca.explained_variance_ratio_
  var_ratio = pd.DataFrame(var_ratio).transpose()
  print(var_ratio)

#+end_src

#+RESULTS:
#+begin_example
       Area  Perimeter  ...  ShapeFactor3  ShapeFactor4
0 -0.840718  -1.143277  ...      1.925653      0.838340
1 -0.829157  -1.013887  ...      2.689603      0.771110
2 -0.807128  -1.078789  ...      1.841288      0.916721
3 -0.785712  -0.977179  ...      2.204169     -0.197978
4 -0.781210  -1.097344  ...      3.013352      0.939605

[5 rows x 16 columns]
                       0         1         2   ...        13        14        15
Area             0.282458  0.245882 -0.061447  ... -0.655728  0.133190  0.231436
Perimeter        0.310891  0.179303 -0.018853  ... -0.081390  0.012658  0.014614
MajorAxisLength  0.325824  0.100757 -0.084692  ...  0.186251  0.174432  0.346019
MinorAxisLength  0.236199  0.343461  0.007500  ...  0.183096  0.155445  0.331749
AspectRation     0.229298 -0.330844 -0.169058  ... -0.026666  0.102810 -0.123576
Eccentricity     0.231526 -0.319434 -0.163042  ... -0.000046 -0.042306  0.014793
ConvexArea       0.283200  0.244630 -0.053649  ...  0.648622 -0.130974 -0.224753
EquivDiameter    0.297484  0.222802 -0.049914  ... -0.266409 -0.332488 -0.675589
Extent          -0.059808  0.220619 -0.085258  ...  0.000057 -0.000001 -0.000005
Solidity        -0.143016  0.103322 -0.738670  ...  0.007671 -0.001387 -0.002115
roundness       -0.248165  0.214805 -0.163325  ... -0.014669  0.002375  0.001894
Compactness     -0.238378  0.328914  0.149701  ... -0.002026  0.652516 -0.372516
ShapeFactor1    -0.221319 -0.332549 -0.032623  ... -0.004011 -0.005468 -0.011285
ShapeFactor2    -0.314625  0.129419  0.120077  ...  0.018504  0.005545  0.014668
ShapeFactor3    -0.238983  0.327522  0.149570  ... -0.046184 -0.601335  0.234065
ShapeFactor4    -0.198009  0.100061 -0.536903  ...  0.004297  0.000616  0.010377

[16 rows x 16 columns]
         0        1         2   ...            13            14            15
0  0.554664  0.26431  0.080066  ...  6.256417e-07  1.341321e-07  1.115495e-07

[1 rows x 16 columns]
#+end_example

* Projecting the data onto the principal axes
Once we have performed PCA and obtained the eigenvectors, we can use them to project the data onto the first principal axes. We can do this by taking the dot product of the data and eigenvectors, or by using the ~sklearn.decomposition.PCA~ module as follows:

#+begin_src python
  from sklearn.decomposition import PCA

  # only keep 3 PCs
  pca = PCA(n_components = 3)

  # transform the data using the first 3 PCs
  data_pcomp = pca.fit_transform(data_standardized)

  # transform into a dataframe
  data_pcomp = pd.DataFrame(data_pcomp)

  # rename columns
  data_pcomp.columns = ['PC1', 'PC2', 'PC3']

  # print the transformed data
  print(data_pcomp.head())
#+end_src

[[./PCs.png]]

Once we have the transformed data, we can look at a scatter plot of the first two transformed features using seaborn or matplotlib. This allows us to view relationships between multiple features at once in 2D or 3D space. Often, the first 2-3 principal components result in clustering of the data.

Below, we've plotted the first two principal components for a dataset of measurements for three different penguin species:

#+begin_src python
  sns.lmplot(x='PC1', y='PC2', data=data_pcomp, hue='species', fit_reg=False)
  plt.show()
#+end_src

[[./PCs_penguin_species.png]]

** Task 1
Let's only keep the first four principal components because they account for 95% of the information in the data.

Use the ~.fit_transform()~ method to transform the standardized data matrix into the new features and save the result as ~data_pcomp~. Then save as a dataframe, re-label, and print the transformed data.

*Hint*
To specify the number of principal axes, you can use the ~n_components~ argument when initializing the ~sklearn.decomposition.PCA~ class, as in ~PCA(n_components=2)~. Then use the ~.fit_transform()~ method to transform the data.

** Task 2
Create a seaborn scatter plot of the first two components. Do you notice any clustering occurring?

** Script.py

#+begin_src python :results output
  import numpy as np
  import pandas as pd
  from sklearn.decomposition import PCA
  import matplotlib.pyplot as plt
  import seaborn as sns

  data_matrix_standardized = pd.read_csv('./data_matrix_standardized.csv')

  classes = pd.read_csv('./classes.csv')['Class']

  # 1. Transform the data into 4 new features using the first PCs
  pca = PCA(n_components=4)
  data_pcomp = pca.fit_transform(data_matrix_standardized)
  data_pcomp = pd.DataFrame(data_pcomp)
  data_pcomp.columns = ['PC1', 'PC2', 'PC3', 'PC4']
  print(data_pcomp.head())

  # 2. Plot the first two principal components by the bean classes
  data_pcomp['bean_classes'] = classes
  sns.lmplot(x='PC1', y='PC2', data=data_pcomp, hue='bean_classes', fit_reg=False)
  plt.show()

#+end_src

#+RESULTS:
:         PC1       PC2       PC3       PC4
: 0 -4.981378  1.824630  0.748993 -0.390797
: 1 -5.436593  2.932257  2.182294 -0.431944
: 2 -4.757913  1.826817  0.514019 -0.125849
: 3 -4.300383  2.003587  3.554316  0.082961
: 4 -6.349107  4.088055  1.179156 -0.830327

* PCA as Features
So far we have used PCA to find principal axes and project the data onto them. We can use a subset of the projected data for modeling, while retaining most of the information in the original (and higher-dimensional) dataset.

For example, recall in the previous exercise that the first four principal axes already contained 95% of the total amount of variance (or information) in the original data. We can use the first four components to train a model, just like we would on the original 16 features.

Because of the lower dimensionality, we should expect training times to be faster. Furthermore, the principal axes ensure that each new feature has no correlation with any other, which can result in better model performance.

In this checkpoint, we will be using the first four principal components as our training data for a Support Vector Classifier (SVC). We will compare this to a model fit with the entire dataset (16 features) using the average likelihood score. Average likelihood is a model evaluation metric; the higher the average likelihood, the better the fit.

** Task 1
Read through the code to make sure that you understand what's happening. Here are the steps:

    - Transform the original data by projecting it onto the first four principal axes. We chose four PCs because we previously found that they contain 95% of the variance in the original data

    - Split the data into 67% training and 33% testing sets

    - Use the transformed training data to fit an SVM model

    - Print out the average likelihood score for the testing data

    - Re-split the original 16 standardized features into training and test sets

    - Fit the same SVM model on the training set with all 16 features

    - Print out the average likelihood score for the test data

Notice that the score for the model using the first 4 principal components is higher than for the model that was fit with te 16 original features. We only needed 1/4 of the data to get even better model performance!

** Script.py

#+begin_src python :results output
  import pandas as pd
  from sklearn.decomposition import PCA
  from sklearn.svm import LinearSVC
  from sklearn.model_selection import train_test_split

  data_matrix_standardized = pd.read_csv('./data_matrix_standardized.csv')
  classes = pd.read_csv('./classes.csv')

  # We will use the classes as y
  y = classes.Class.astype('category').cat.codes

  # Get principal components with 4 features and save as X
  pca_1 = PCA(n_components=4)
  X = pca_1.fit_transform(data_matrix_standardized)

  # Split the data into 33% testing and the rest training
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

  # Create a linear support vector classifier
  svc_1 = LinearSVC(random_state=0, tol=1e-5, dual='auto')
  svc_1.fit(X_train, y_train)

  # Generate a score for the testing data
  score_1 = svc_1.score(X_test, y_test)
  print(f'Score for model with 4 PCA features: {score_1}')

  # Split the original data into 33% testing and the rest training
  X_train, X_test, y_train, y_test = train_test_split(data_matrix_standardized, y, test_size=0.33, random_state=42)

  # Create a Linear Support Vector Classifier
  svc_2 = LinearSVC(random_state=0, dual='auto')
  svc_2.fit(X_train, y_train)

  # Generate a score for the testing data
  score_2 = svc_2.score(X_test, y_test)
  print(f'Score for the model with original features: {score_2}')

#+end_src

#+RESULTS:
: Score for model with 4 PCA features: 0.8472840605520926
: Score for the model with original features: 0.9171861086375779

* PCA for images 1
Another way to show the inner workings of PCA is to use an image dataset. An image can be represented as a row in a data matrix, where each feature corresponds to the intensity of a pixel.

In this and the following exercise, we will be using the Olivetti Faces image dataset. We will begin by standardizing the images, and then observing the images of faces themselves.

In the next exercise, we will then transform the original data using PCA and re-plot the images using a subset of the principal components. This will allow us to visualize the mechanism by which PCA retains information in the data while reducing the dimensionality.

** Task 1
Let's start by downloading the Olivetti Faces dataset from scikit-learn. Standardize the image dataset by subtracting the mean and dividing by the standard deviation.

*Hint*
Standardize a matrix or DataFrame by subtracting the mean and dividing by the standard deviation, (a_matrix - mean) / sttd.

** Task 2
Each row from the faces dataset is a flattened image. Each feature is a pixel in the image. Find the side length of each square image by taking the square root of the number of features (pixels). Print the number of pixels per image and per side-length.

*Hint*
Use the NumPy method of ~np.sqrt~ to find the square root of a number.

** Task 3
Plot the faces

** Script.py

#+begin_src python :results output
  import numpy as np
  from sklearn import datasets
  import matplotlib.pyplot as plt

  # Download the data from sklearn's datasets
  faces = datasets.fetch_olivetti_faces()['data']

  # 1. Standardize the images using the mean and standard deviation
  faces_mean = faces.mean(axis=0)
  faces_std = faces.std(axis=0)
  faces_standardized = (faces - faces_mean) / faces_std

  # 2. Find the side length of a square image
  n_images, n_features = faces_standardized.shape
  side_length = int(np.sqrt(n_features))
  print(f'Number of features(pixels) per image: {n_features}')
  print(f'Square image side length: {side_length}')

  # Create an empty 10x8 plot
  fig = plt.figure(figsize=(10, 8))

  # Observe the first 15 images
  for i in range(15):
      # Create subplot, remove x and y ticks, and add title
      ax = fig.add_subplot(3, 5, i + 1, xticks=[], yticks=[])
      ax.set_title(f'Image of Face: #{i}')
      # Get an image from a row based on the current value of i
      face_image = faces_standardized[i]
      # Reshape this image into side_length x side_length
      face_image_reshaped = face_image.reshape(side_length, side_length)
      # Show the image
      ax.imshow(face_image_reshaped, cmap=plt.cm.bone)
  plt.show()

#+end_src

#+RESULTS:
: Number of features(pixels) per image: 4096
: Square image side length: 64

* PCA for images 2
Now that we have cleaned up the data, we can perform PCA to retrieve the eigenvalues and eigenvectors.

This can be useful in understanding how PCA works! We can visualize the eigenvectors by plotting them. They actually have a name: ~eigenfaces~. The eigenfaces are the building blocks for all the other faces in te data.

We can also visualize the dimensionality reduction that occurs when we transform the original data using a smaller number of principal components. In the code editor, we've provided you with code to:

    - Plot the eigenfaces

    - Plot the reconstructed faces using a smaller number of transformed features. To start, we've used 400 principal components -only 0.9% of the original number of features (pixels)!

** Task 1
First, inspect the eigenfaces. Notice how they each depict some of the common distintive features in the various faces.

Now, scroll down in the plotting output to view the reconstructed faces. These were constructed using only 400 transformed features. Notice that they look almost identical to the original faces, depite requiring only .9% as much data to store them!

** Task 2
Now, change the number of components to 40 instead of 400 and press "Run". Scroll down to vier the reconstructed faces. Notice how we get a decent reconstruction, but lose some detail and clarity by reducing the feature space this much (only 0.09% of the original).

** Script.py

#+begin_src python :results output
  import numpy as np
  import pandas as pd
  from sklearn.decomposition import PCA
  import matplotlib.pyplot as plt

  faces_standardized = pd.read_csv('./faces_standardized.csv').values

  # 1. Instantiate a PCA object and fit the standardized faces dataset
  pca = PCA(n_components=400)
  pca.fit(faces_standardized)

  # 2. Retrieve and plot eigenvectors (eigenfaces)
  eigenfaces = pca.components_

  fig = plt.figure(figsize=(10, 8))
  fig.suptitle('Eigenvectors of Images (Eigenfaces)')

  for i in range(15):
      # Create subplot, remove x and y ticks, and add title
      ax = fig.add_subplot(3, 5, i + 1, xticks=[], yticks=[])
      ax.set_title(f'Eigenface: #{i}')

      # Get an eigenvector from the curren value of i
      eigenface = eigenfaces[i]

      # Reshape this image into 64x64 since flattened shape was 4096
      eigenface_reshaped = eigenface.reshape(64, 64)

      # Show the image
      ax.imshow(eigenface_reshaped, cmap=plt.cm.bone)
  plt.show()

  # 3. Reconstruct images from the compressed principal components
  # The principal components are usually calculated using `faces_standardized @ principal_axes` or the `.transform` method
  principal_components = pca.transform(faces_standardized)

  # The `inverse_transform` method allows for reconstruction of images in the original size
  faces_reconstructed = pca.inverse_transform(principal_components)

  # Plot the reconstructed images
  fig = plt.figure(figsize=(10, 8))
  fig.suptitle('Reconstructed Images from Principal Components')
  for i in range(15):
      ax = fig.add_subplot(3, 5, i + 1, xticks=[], yticks=[])
      ax.set_title(f'Reconstructed: {i}')

      reconstructed_face = faces_reconstructed[i]
      reconstructed_face_reshaped = reconstructed_face.reshape(64, 64)
      ax.imshow(reconstructed_face_reshaped, cmap=plt.cm.bone)
  plt.show()

#+end_src

#+RESULTS:
