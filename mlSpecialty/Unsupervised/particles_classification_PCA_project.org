
* Particles Classification with PCA Project
In this project, you will classify particles into gamma(signal) or hadrons(background). Given that the features are correlated, you will perform PCA to get a new set of features, and select the features that contain the most information. The data set was generated by a Monte Carlo program, Corsika, described in D. Heck et al., CORSIKA, A Monte Carlo code to simulate extensive air showers, Forschungszentrum Karlsruhe FZKA 6019 (1998).

The first part of the project ‘Observing the Dataset’ is in script_1.py. The second part of the project ‘Performing PCA’ is in script_2.py.

** Observing the Dataset

*** Task 1
Remove any nulls from the dataset.

*Hint*
Use the Pandas method ~df.dropna()~ to drop NAN values.

*** Task 2
Extract the numerical columns onto a variable named ~data_matrix~ and the classes to a variable named ~classes~

*Hint*
Drop the class label using the pandas method of ~df.drop~, as in ~df.drop(columns='class')~

*** Task 3
Create a correlation matrix of the data matrix and show it as a heatmap

*Hint*
Create a correlation matrix using the Pandas method ~data_matrix.corr()~

*** Task 4
Find the eigenvectors and eigenvalues using the NumPy function ~np.linalg.eig~. Here we also order the eigenvalues from greatest to smallest by ordering its indices first, and use these indices to also order the eigenvalues.

*Hint*
Use the NumPy function ~np.linalg.eig(correlation_matrix)~ to retrieve eigenvalues and eigenvectors.

*** Task 5
Find the proportions of each eigenvalue to the total sum of the eigenvalues. These proportions represent the percentages of information that each eigenvalue's associated eigenvector contains.

*Hint*
Divide the eigenvalues by the sum of the eigenvalues to get the normalized proportions, ~eigenvalues / eigenvalues.sum~.

*** Task 6
Find the cumulative percentages of the ordered eigenvectors.

*** Script_1.py

#+begin_src python :results output
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  import seaborn as sns

  ## Task 1: Drop nan values

  # Read the csv data as a DataFrame
  df = pd.read_csv('./telescope_data.csv', index_col=0)

  # Remove null and na values
  df.dropna()

  # Print the DataFrame head
  print('Task 1:')
  print(df.head())

  ## Task 2: Extract class column

  # Extract the class classes
  classes = df['class']
  data_matrix = df.drop(columns='class')

  print('Task 2:')
  print(data_matrix)

  ## Task 3: Create a correlation matrix

  # Use the .corr() method on data_matrix to get the correlation matrix
  correlation_matrix = data_matrix.corr()

  ax = plt.axes()
  sns.heatmap(correlation_matrix, cmap='Greens', ax=ax)
  ax.set_title('Task 3:')
  plt.show()

  ## Task 4: Perform eigendecomposition
  print('Task 4:')

  # Perform eigen decomposition using np.linalg.eig
  eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)

  print(f'Eigenvalues length: {eigenvalues.size}, Original Number of Features: {data_matrix.shape[1]}')

  # Order the eigenvalues by ordering the indices of the eigenvalues using `argsort`, and use [::-1] to order them from greatest to smallest
  indices = eigenvalues.argsort()[::-1]
  eigenvalues = eigenvalues[indices]
  eigenvectors = eigenvectors[:, indices]

  print(eigenvalues.shape, eigenvectors.shape)

  ## Task 5: Find the variance / information percentages for each eigenvalue.

  # Find the percentages of information for each eigenvector, which is generated by the proportion of its eigenvalues to the sum of all eigenvalues
  information_proportions = eigenvalues / eigenvalues.sum()
  information_percents = information_proportions * 100

  # Plot the principal axes vs the information proportions for each principal axis
  plt.figure()
  plt.plot(information_percents, 'ro-', linewidth=2)
  plt.title('Task 5: Scree Plot')
  plt.xlabel('Principal Axes')
  plt.ylabel('Percent of Information Explained')
  plt.show()

  ## Task 6: Find the cumulative variance / information percentage for each eigenvalue

  # Find the cumulative sum of the percentages
  cumulative_information_percents = np.cumsum(information_percents)

  # Plot the cumulative percentages array
  plt.figure()
  plt.plot(cumulative_information_percents, 'ro-', linewidth=2)

  # Also plot a horizontal line indicating the 95% mark, and a vertical line for the third principal axis
  plt.hlines(y=95, xmin=0, xmax=15)
  plt.vlines(x=3, ymin=0, ymax=100)
  plt.title('Task 6: Cumulative Information Percentages')
  plt.xlabel('Principal Axes')
  plt.ylabel('Cumulative Proportion of Variance Explained')
  plt.show()

#+end_src

#+RESULTS:
#+begin_example
Task 1:
    fLength    fWidth   fSize   fConc  ...  fM3Trans   fAlpha     fDist  class
0   28.7967   16.0021  2.6449  0.3918  ...   -8.2027  40.0920   81.8828      g
1   31.6036   11.7235  2.5185  0.5303  ...   -9.9574   6.3609  205.2610      g
2  162.0520  136.0310  4.0612  0.0374  ...  -45.2160  76.9600  256.7880      g
3   23.8172    9.5728  2.3385  0.6147  ...   -7.1513  10.4490  116.7370      g
4   75.1362   30.9205  3.1611  0.3168  ...   21.8393   4.6480  356.4620      g

[5 rows x 11 columns]
Task 2:
        fLength    fWidth   fSize  ...  fM3Trans   fAlpha     fDist
0       28.7967   16.0021  2.6449  ...   -8.2027  40.0920   81.8828
1       31.6036   11.7235  2.5185  ...   -9.9574   6.3609  205.2610
2      162.0520  136.0310  4.0612  ...  -45.2160  76.9600  256.7880
3       23.8172    9.5728  2.3385  ...   -7.1513  10.4490  116.7370
4       75.1362   30.9205  3.1611  ...   21.8393   4.6480  356.4620
...         ...       ...     ...  ...       ...      ...       ...
19015   21.3846   10.9170  2.6161  ...    2.8766   2.4229  106.8258
19016   28.9452    6.7020  2.2672  ...   -2.9632  86.7975  247.4560
19017   75.4455   47.5305  3.4483  ...   -9.4662  30.2987  256.5166
19018  120.5135   76.9018  3.9939  ...  -63.8389  84.6874  408.3166
19019  187.1814   53.0014  3.2093  ...   31.4755  52.7310  272.3174

[19020 rows x 10 columns]
Task 4:
Eigenvalues length: 10, Original Number of Features: 10
(10,) (10, 10)
#+end_example
