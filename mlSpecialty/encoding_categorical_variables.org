
* Encoding Categorical Variables
### Encoding categorical variables with Python

** Introduction
Categorical data is data that has more than one category. When working with that type of data we have two types, nominal and ordinal. Nominal data is data that has no particular order or hierarchy to it, and ordinal data is categorical data where the categories have order, but the differences between the categories are not important or unclear.

We will be working with a dataset of used cars for this article to truly understand and demonstrate how to work with categorical data. Let's explore it and see what type of data we are working with.

#+begin_src python :results output
  import pandas as pd

  #import data
  cars = pd.read_csv('cars.csv')

  #check variable types
  print(cars.dtypes)

#+end_src

#+RESULTS:
#+begin_example
Unnamed: 0        int64
year              int64
make             object
model            object
trim             object
body             object
transmission     object
vin              object
state            object
condition       float64
odometer        float64
color            object
interior         object
seller           object
mmr               int64
sellingprice      int64
saledate         object
Unnamed: 16     float64
dtype: object
#+end_example

We can see from the output that we have a lot of features that are ~dtype = object~ and that tells us those features could be text or a mix of text and numerical values. For our encoding examples, we will explore a few of those object features and transform those values so we can have a data frame ready form machine learning.

The reason we put our time into this level of encoding is that there are many machine learning models that cannot handle text and will only work with numbers. Our data must be encoded into numbers before we even begin to train, test or evaluate a model.

** Ordinal encoding
We mentioned already that ordinal data is data that does have order and a hierarchy between its values. Let us take a look at the ~condition~ feature from our data frame and perform a value_counts to see how many times each labels is listed in our feature.

#+begin_src python :results output
  import pandas as pd

  #import data
  cars = pd.read_csv('cars.csv')

  #check variable types
  print(cars.dtypes)

  #howmany times each label is listed in our feature
  print(cars['condition'].value_counts())
#+end_src

#+RESULTS:
#+begin_example
Unnamed: 0        int64
year              int64
make             object
model            object
trim             object
body             object
transmission     object
vin              object
state            object
condition       float64
odometer        float64
color            object
interior         object
seller           object
mmr               int64
sellingprice      int64
saledate         object
Unnamed: 16     float64
dtype: object

# #OUTPUT
# New          2881
# Like New     2860
# Good         2027
# Fair          753
# Excellent     18
#+end_example

This is definitely an example of ordinal data: the condition of the used cars can easily be put in order of those in the “best” condition to the cars in the “worst” condition. The output printed the labels with the highest counts, but we can assume the following hierarchy:

    Excellent
    New
    Like New
    Good
    Fair

We need to convert these labels into numbers, and we can do this with two different approaches. First, we can do this by creating a dictionary where every label is the key and the new numeric number is the value. 'Excellent' will get the highest score and 'Fair' will be our lowest score. Then we will map each label from the condition column to the numeric value and create a new column called condition_rating.

#+begin_src python
  #create dictionary of label:values in order
  rating_dict = {'Excellent':5, 'New':4, 'Like New':3, 'Good':2, 'Fair':1}

  #create a new column
  cars['condition_rating'] = cars['condition'].map(rating_dict)
#+end_src

The second approach we will show is how to utilize the sklearn.preprocessing library OrdinalEncoder. We follow a similar approach: we set our categories as a list, and then we will .fit_transform the values in our feature condition. We need to make sure we adhere to the shape requirements of a 2-D array, so you'll notice the method .reshape(-1,1).

We'll also note, this method will not work if your feature has NaN values. Those need to be addressed prior to running .fit_transform.

#+begin_src python
  #using scikit-learn
  from sklearn.preprocessing import OrdinalEncoder

  #create encoder and set category order
  encoder = OrdinalEncoder(categories=[['Excellent', 'New', 'Like New', 'Good', 'Fair']])

  #reshape our feature
  condition_reshaped = cars['condition'].values.reshape(-1,1)

  #create a new variable with assigned numbers
  cars['condition_rating'] = encoder.fit_transform(condition_reshaped)
#+end_src

** Label encoding
Now, we can talk about nominal data, and we have to approach this type of data differently than what we did with ordinal data. Our color feature has a lot of different labels, but here are the top five colors that appear in our data frame.

#+begin_src python :results output
  import pandas as pd

  cars = pd.read_csv('cars.csv')

  print(cars['color'].nunique())

  print(cars['color'].value_counts()[:5])
#+end_src

#+RESULTS:
: 19
: color
: black     2015
: white     1931
: gray      1506
: silver    1503
: blue       869
: Name: count, dtype: int64

To prepare this feature, we still need to convert our text to numbers, so let's do just that. We will demonstrate two different approaches, with the firs one showing how to convert the feature from an object type to a categories type.

#+begin_src python :results output
  import pandas as pd

  cars = pd.read_csv('cars.csv')

  #convert feature to category type
  cars['color'] = cars['color'].astype('category')

  #save a new version of category codes
  cars['color'] = cars['color'].cat.codes

  #print to see transformation
  print(cars['color'].value_counts()[:5])
#+end_src

#+RESULTS:
: color
: 1     2015
: 16    1931
: 7     1506
: 14    1503
: 2      869
: Name: count, dtype: int64

Comparing our newly transformed data to the original top 5, we can see Black was transformed to 1, White as transformed to 16, and so on.

However, we have created a problem for ourselves and potentially our model. We can see that 'Blue' cars now have a value of 2, and 'White' cars = 18, our model could actually give 'White' cars 8 times more weight than a 'Blue' car simply because of the way we encoded this feature. To combat this ordinal assumption our model will make, we should one-hot encode our nominal data, which we will cover shortly.

One more way we can transform this feature is by using sklearn.preprocessing and the LabelEncoder library. This method will not work if your feature has NaN values. Those need to be addressed prior to running .fit_transform.

#+begin_src python
  from sklearn.preprocessing import LabelEncoder

  #create encoder
  encoder = LabelEncoder()

  #create new variable with assigned numbers
  cars['color'] = encoder.fit_transform(cars['color'])
#+end_src

** One-hot Encoding
One-hot encoding is when we create a *dummy* variable for each value of our categorical feature, and a dummy variable is defined as a numeric variable with two values: 1 and 0. We will continue to talk about our color feature from our used car dataset.

Looking at this visual below, we can see we have ten cars in four different colors. In place of the single color column, we create four dummy variables -one new column for each color. Then the values that go into that column are binary, indicating if the car in that row is the color of the column name (1) or not (0).

[[./dummy_variables.png]]

This approach is great for our color feature and will allow the model to see each category as its own feature and not try to create order between a "Black car" and a "Red Car". Here is how we can implement this in Python:

#+begin_src python
  import pandas as pd
  #use pandas .get_dummies method to create one new column for each color
  ohe = pd.get_dummies(cars['color'])
  #join the new columns back onto our cars dataframe
  cars = cars.join(ohe)
#+end_src

A downside to this approach is that it can create a lot of features which can then create a very sparse matrix.

One-hot encode the feature body from our cars dataset.

Remember  to use pd.get_dummies(feature) when building your new feature.

#+begin_src python :results output
  import pandas as pd

  cars = pd.read_csv('cars.csv')

  ## one hot encode the feature
  ## label this variable ohe
  ohe = pd.get_dummies(cars['body'])

  ## join our new columns back
  cars = cars.join(ohe)

  ## print the column names
  print(cars.columns)

  ## iterate the columns
  for col in cars.columns:
      print(col)

  ## print the 'suv' column
  print(cars[['suv']])

#+end_src

#+RESULTS:
#+begin_example
Index(['Unnamed: 0', 'year', 'make', 'model', 'trim', 'body', 'transmission',
       'vin', 'state', 'condition', 'odometer', 'color', 'interior', 'seller',
       'mmr', 'sellingprice', 'saledate', 'Unnamed: 16', 'Access Cab',
       'CTS Coupe', 'Club Cab', 'Convertible', 'Coupe', 'Crew Cab',
       'CrewMax Cab', 'Double Cab', 'E-Series Van', 'Elantra Coupe',
       'Extended Cab', 'G Convertible', 'G Coupe', 'G Sedan', 'Genesis Coupe',
       'Hatchback', 'King Cab', 'Koup', 'Mega Cab', 'Minivan',
       'Promaster Cargo Van', 'Quad Cab', 'Regular Cab', 'SUV', 'Sedan',
       'SuperCab', 'SuperCrew', 'Van', 'Wagon', 'Xtracab', 'club cab',
       'convertible', 'coupe', 'crew cab', 'double cab', 'e-series van',
       'extended cab', 'g convertible', 'g coupe', 'g sedan', 'genesis coupe',
       'hatchback', 'king cab', 'koup', 'mega cab', 'minivan',
       'promaster cargo van', 'quad cab', 'regular cab', 'sedan', 'supercab',
       'supercrew', 'suv', 'van', 'wagon'],
      dtype='object')
Unnamed: 0
year
make
model
trim
body
transmission
vin
state
condition
odometer
color
interior
seller
mmr
sellingprice
saledate
Unnamed: 16
Access Cab
CTS Coupe
Club Cab
Convertible
Coupe
Crew Cab
CrewMax Cab
Double Cab
E-Series Van
Elantra Coupe
Extended Cab
G Convertible
G Coupe
G Sedan
Genesis Coupe
Hatchback
King Cab
Koup
Mega Cab
Minivan
Promaster Cargo Van
Quad Cab
Regular Cab
SUV
Sedan
SuperCab
SuperCrew
Van
Wagon
Xtracab
club cab
convertible
coupe
crew cab
double cab
e-series van
extended cab
g convertible
g coupe
g sedan
genesis coupe
hatchback
king cab
koup
mega cab
minivan
promaster cargo van
quad cab
regular cab
sedan
supercab
supercrew
suv
van
wagon
        suv
0     False
1     False
2     False
3     False
4     False
...     ...
9995  False
9996  False
9997  False
9998  False
9999  False

[10000 rows x 1 columns]
#+end_example

** Binary encoding
If we find the need to one-hot encode a lot of categorical features which would, in turn, create a sparse matrix and may cause problems for our model, a strong alternative to this issue is performing a binary encoder. A binary encoder will find the number of unique categories and then convert each category to its binary representation. Let us take a quick review of binary numbers and keep using our color feature. We know that we have 19 unique colors, so the way to represent the numbers from 1 to 19 in binary format is as follows:

Number 	Binary Number
1  	1
2 	10
3 	11
4 	100
5 	101
6 	110
7 	111
8 	1000
9 	1001
10 	1010
11 	1011
12 	1100
13 	1101
14 	1110
15 	1111
16 	10000
17 	10001
18 	10010
19 	10011

We can easily see that our highest number 19 is 5 digits long, so our binary encoder will need 5 columns to be able to represent all digits. Here is a sample of how our color column will transform each color if we were to perform a binary encoder.

[[./color binary encoding.png]]

Our 19th color, pink, has transformed to be represented in the binary form 10011. If we were to utilize this process instead of the traditional one -hot encoder we would have 5 numerical features instead of 19, reducing our features by about 75%!

To make this happen with Python we'll use a library called category_encoders and import BinaryEncoder. We will determine which column to transform and set drop_invariant to True so it will keep the five binary columns. If it is set to the default 0, then we would have an additional column full of zeros.

#+begin_src python
  from category_encoders import BinaryEncoder

  #This will create a new dataframe with the color column removed and replaced with our 5 new binary feature columns
  colors = BinaryEncoder(cols = ['color'], drop_invariant = True).fit_transform(cars)
#+end_src
