
* Assumptions of Logistic Regression I
We are now ready to delve deeper into Logistic Regression. In this lesson, we will cover the different assumptions that go into logistic regression, model hyperparameters, how to evaluate a classifier, ROC curves, and what to do when there's a class imbalance in the classification problem we're working with.

For this lesson, we will be using the [[https://www.kaggle.com/uciml/breast-cancer-wisconsin-data][Wisconsin Breast Cancer Data Set]] (Diagnostic) to predict whether a tumor is bening (0) or malignant (1) based on characteristics of the cells, such as radius, texture, smoothness, etc. Like a lot of real-world data sets, the distribution of outcomes is uneven (bening diagnoses are more common than malignant) and there is a bias in terms of the importance of the outcomes (classifying all malignant cases correctly is of the utmost importance).

We're going to begin with the primary assumptions about the data that need to be checked before implementing a logistic regression model.

1. *The target variable is binary*

   One of the most basic assumptions of logistic regression is that the outcome variable needs to be /binary,/ which means there are /two possible outcomes./ Multinomial logistic regression is an exception to this assumption and is beyond the scope of this lesson.

2. *Independent observations*

    While often overlooked, checking for independent observations in a data set is important for logistic regression. This can be violated if, in this case, patients are biopsed multiple times (repeated sampling of the same individual).

3. *Large enough sample size*

     Since logistic regression is fit using /maximun likelihood estimation/ instead of /least squares minimization,/ there must be a large enough sample to get convergence. When a model fails to converge, this causes the estimates to be extremely inaccurate. Now, what does a "large enough" sample mean? Often a rule of thumb is that there should be at least 10 samples per feature for the smallest class in the outcome variable.

     For example, if there were 100 samples and the outcome variable diagnosis had 60 bening tumors and 40 malignant tumors, then the max number of features allowed would be 4. To get 4 we took the smallest of the classes in the outcome variable, 40, and divided it by 10.

4. *No influential outliers*

   Logistic regression is sensitive to outliers, so we must remove any extremely influential outliers for model building. Outliers are a broad topic with many different definitions -z scores, scaler of the interquartile range, Cook's distance/influence/leverage, etc- so there are many ways to identify them. But here, we will use visual tools to rule out obvious outliers.

** Task 1
We need to verify that there are two classes in diagnosis, making it binary.

Using ~value_counts()~ print the distinct diagnosis values and their frequency in the dataset.

** Task 2
Now we need to make sure that there are no repeat samples. We can do this by making sure the number of /unique/ ids ia equal to the number of ids.

Use .nunique() to find the number of unique id values and use .count() to find the number of id values. Set this two values equal to each other and print the result.

** Task 3
If classes are very imbalanced, it is important the smallest class still meets the rule of thumb. Based on this how many features should we have at maximun?

Using the outcome variable diagnosis, divide the smallest class size by 10. Save this variable as max_features and print the value.

** Task 4
We're going to view a boxplot ot the mean features (~predictor_var~ has the list of these features) so that we can find and exclude any obvious outliers. Since the features are not normally distributed (since all the values must be greater than zero and are right-skewed), it is more informative to look at the boxplot of the log-transformed z-scores.

Uncomment the relevant lines here and press Run to see the output.

Which features have extreme outliers?

Remember, outliers are the data points that are outside the bulk of the distribution pattern.

** Script.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import seaborn as sns

  # Import models from scikit learn module:
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LogisticRegression
  from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score
  from sklearn.preprocessing import StandardScaler
  from scipy.stats import zscore

  df = pd.read_csv('breast_cancer_data.csv')

  # Encode malignant as 1, benign as 0
  df['diagnosis'] = df['diagnosis'].replace({'M':1, 'B':0})

  # Imports/load data
  predictor_var = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']

  #print(df.head())

  # Print distinct diagnosis values and frequency in dataset
  print(df['diagnosis'].value_counts())

  # Test if the number of unique IDs is equal to sample size, i.e. no repeated patients
  print(df['id'].nunique())
  print(df['id'].count())

  unique_ids = df.id.nunique()==df.id.count()
  print(unique_ids)

  # At a maximum, there should be no more than the smallest size class divided by 10 number of features
  max_features = min(df['diagnosis'].value_counts() / 10)
  print(max_features)

  # See which features have extreme outliers
  sns.boxplot(data=np.log(df[predictor_var]+.01).apply(zscore))
  plt.xticks(rotation=45);
  plt.show()
  plt.close()

  #5. Uncomment the code to remove the samples with extreme fractal_dimensions_mean values:
  q_hi  = df["fractal_dimension_mean"].quantile(0.99)
  df_filtered = df[(df["fractal_dimension_mean"] < q_hi)]
  
  #6. Run the boxplot again but with the filtered dataframe:
  sns.boxplot(data=np.log(df_filtered[predictor_var]+.01).apply(zscore))
  plt.xticks(rotation=45);
  plt.show()
  plt.close()

#+end_src

#+RESULTS:
: diagnosis
: 0    357
: 1    212
: Name: count, dtype: int64
: 569
: 569
: True
: 21.2

* Assumptions of Logistic Regression II

1. *Features linearly related to log odds*

   Similar to linear regression, the underlying assumption of logistic regression is that the features are /linearly related/ to the logit fo the outcome. To test this visually, we can use Seaborn's regplot, with the parameter ~logistic=True~ and the x value as our feature of interest. If this condition is met, the fit model will resemble a sigmoidal curve (as is the case when ~x=radius_mean~).

   We've added code to create another plot using the feature fractal_dimension_mean. How do the curves compare?

2. *Multicollinearity*

   Like in linear regression, one of the assumptions is that there is no multicollinearity in the data. Meaning the features /should not be highly correlated./ Multicollinearity can cause the coefficients and p-values to be inaccurate. With a correlation plot, we can see which features are highly correlated and then we can drop one of the features.

   We're going to look at the "mean" features which are highly correlated with each other using a heatmap correlation plot.

** Task 1
Uncomment the relevant lines of code and press Run to see the heatmap.

There are two features that are highly positively correlated with ~radius_mean~. Can you spot them?

The heatmap shows that ~radius_mean~, ~perimeter_mean~, and ~area_mean~ are all highly positively correlated. It would be beneficial to only keep one of these three features to avoid multicollinearity.

** Task 2
Not including ~radius_mean~, ~perimeter_mean~, or ~area_mean~, there is another pair of features that are highly correlated. Create an array named ~correlated_pair~ containing these two features.

** Script.py
#+begin_src python :results output
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import seaborn as sns

  # Import models from scikit learn module:
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LogisticRegression
  from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score

  df = pd.read_csv('breast_cancer_data.csv')

  #encode malignant as 1, bening as 0
  df['diagnosis'] = df['diagnosis'].replace({'M':1, 'B':0})

  predictor_var = ['radius_mean', 'texture_mean', 'perimeter_mean','area_mean', 'smoothness_mean', 'compactness_mean','concavity_mean', 'symmetry_mean', 'fractal_dimension_mean']

  x = df[predictor_var]

  # Compare the curves
  sns.regplot(x='radius_mean', y='diagnosis', data=df, logistic=True)
  plt.show()
  plt.close()

  sns.regplot(x='fractal_dimension_mean', y='diagnosis', data=df, logistic=True)
  plt.show()
  plt.close()

  # Uncomment the heatmap and identify the two features that are highly correlated with the radius_mean
  plt.figure(figsize = (10, 7))
  sns.heatmap(x.corr(), annot=True)
  plt.show()

  #2. Identify the other highly correlated pair. Define an array called correlated_pair containing the two features you've identified.
  correlated_pair = ['compactness_mean', 'concavity_mean']
#+end_src

#+RESULTS:

* Scikit-learn implementation

** Model Training and Hyperparameters
Now that we have checked the assumptions of Logistic Regression, we can train and predict a model using scikit-learn. We will first set the /hyperparameters/ of our model.

Hyperparameters are set before the model implementation step and tuned later to improve model performance. Conversely, parameters are the result of model implementation, such as the intercept and coefficients.

#+begin_quote
*Note:*
Within scikit-learn these hyperparameters are often referred to as "parameters" which might cause some confusion. It is worth noting that the meaning within scikit-learn documentation refers to these being "parameters" of the function and not of the model itself.
#+end_quote

** Evaluation Metrics
Despite the name, logistic regression is being used as classifier here, son any evaluation metrics for classification tasks will apply. The simplest metric is accuracy - how many correct predictions did we make out of the total? However, when classes are imbalanced, this can be a misleading metric for model performance. Similarly, if we care more about accurately predicting a certain class, other metrics may be more appropriate to use, such as precision, recall, or F1-score may be better to evaluate performance. All of these metrics are available in scikit-learn.

#+begin_src
Accuracy = (TP + TN)/Total

Precision = TP/(TP + FP)

Recall = TP/(TP + FN)

F1 score = 2*((Precision*Recall)/(Precision+Recall))
#+end_src

** Which metrics matter most?
For our breast cancer dataset, predicting ALL malignant cases as malignant is of the utmost importance -and even if there are some false positives (bening cases that are marked as malignant), these likely will be discovered by follow-up tests. Whereas missing a malignant case (classifying it as benign) could have deadly consequences. Thus, we want to minimize false negatives. This in turn will maximize the recall ratio (also known as the sensitivity or true positive rate).

** Task 1 : Model Hyperparameters
Using the mean predictor variables defined in the code editor, define a logistic regression classifier named ~log_reg~ using scikit-learn with no regularization (i.e. no penalty) and an intercept term. This will mean setting:

    - the input argument ~penalty~ to 'none' (the default penalty is '12').

    - the input argument ~fit_intercept~ to True

Print the hyperparameters using the ~.get_params()~ method.

** Model Parameters
Fit the model to the training data and calculate the resulting model coefficients and intercept. Store them as variables coefficients and intercept respectively and print their values.

** Script.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import seaborn as sns

  # Import models from scikit learn module:
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LogisticRegression
  from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score

  df = pd.read_csv('breast_cancer_data.csv')

  #encode malignant as 1, benign as 0
  df['diagnosis'] = df['diagnosis'].replace({'M':1, 'B':0})
  predictor_var = ['radius_mean', 'texture_mean', 'compactness_mean', 'symmetry_mean']

  outcome_var = 'diagnosis'

  x_train, x_test, y_train, y_test = train_test_split(df[predictor_var], df[outcome_var], random_state=0, test_size=0.3)

  # Fit a Logistic Regression model with the specified hyperpatameters
  log_reg = LogisticRegression(penalty='none', fit_intercept=True)
  print(log_reg.get_params())

  # Fit the model to the training data and obtain coefficients and intercept
  log_reg.fit(x_train, y_train)
  coefficients = log_reg.coef_
  intercept = log_reg.intercept_
  print('coefficients: ', coefficients)
  print('intercept: ', intercept)
#+end_src

#+RESULTS:
: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'none', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
: coefficients:  [[ 1.07991529  0.28744917 30.96010078 29.97251652]]
: intercept:  [-30.15566409]
