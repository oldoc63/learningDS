
* Assumptions of Logistic Regression I
We are now ready to delve deeper into Logistic Regression. In this lesson, we will cover the different assumptions that go into logistic regression, model hyperparameters, how to evaluate a classifier, ROC curves, and what to do when there's a class imbalance in the classification problem we're working with.

For this lesson, we will be using the [[https://www.kaggle.com/uciml/breast-cancer-wisconsin-data][Wisconsin Breast Cancer Data Set]] (Diagnostic) to predict whether a tumor is bening (0) or malignant (1) based on characteristics of the cells, such as radius, texture, smoothness, etc. Like a lot of real-world data sets, the distribution of outcomes is uneven (bening diagnoses are more common than malignant) and there is a bias in terms of the importance of the outcomes (classifying all malignant cases correctly is of the utmost importance).

We're going to begin with the primary assumptions about the data that need to be checked before implementing a logistic regression model.

1. *The target variable is binary*

   One of the most basic assumptions of logistic regression is that the outcome variable needs to be /binary,/ which means there are /two possible outcomes./ Multinomial logistic regression is an exception to this assumption and is beyond the scope of this lesson.

2. *Independent observations*

    While often overlooked, checking for independent observations in a data set is important for logistic regression. This can be violated if, in this case, patients are biopsed multiple times (repeated sampling of the same individual).

3. *Large enough sample size*

     Since logistic regression is fit using /maximun likelihood estimation/ instead of /least squares minimization,/ there must be a large enough sample to get convergence. When a model fails to converge, this causes the estimates to be extremely inaccurate. Now, what does a "large enough" sample mean? Often a rule of thumb is that there should be at least 10 samples per feature for the smallest class in the outcome variable.

     For example, if there were 100 samples and the outcome variable diagnosis had 60 bening tumors and 40 malignant tumors, then the max number of features allowed would be 4. To get 4 we took the smallest of the classes in the outcome variable, 40, and divided it by 10.

4. *No influential outliers*

   Logistic regression is sensitive to outliers, so we must remove any extremely influential outliers for model building. Outliers are a broad topic with many different definitions -z scores, scaler of the interquartile range, Cook's distance/influence/leverage, etc- so there are many ways to identify them. But here, we will use visual tools to rule out obvious outliers.

** Task 1
We need to verify that there are two classes in diagnosis, making it binary.

Using ~value_counts()~ print the distinct diagnosis values and their frequency in the dataset.

** Task 2
Now we need to make sure that there are no repeat samples. We can do this by making sure the number of /unique/ ids ia equal to the number of ids.

Use .nunique() to find the number of unique id values and use .count() to find the number of id values. Set this two values equal to each other and print the result.

** Task 3
If classes are very imbalanced, it is important the smallest class still meets the rule of thumb. Based on this how many features should we have at maximun?

Using the outcome variable diagnosis, divide the smallest class size by 10. Save this variable as max_features and print the value.

** Task 4
We're going to view a boxplot ot the mean features (~predictor_var~ has the list of these features) so that we can find and exclude any obvious outliers. Since the features are not normally distributed (since all the values must be greater than zero and are right-skewed), it is more informative to look at the boxplot of the log-transformed z-scores.

Uncomment the relevant lines here and press Run to see the output.

Which features have extreme outliers?

Remember, outliers are the data points that are outside the bulk of the distribution pattern.

** Script.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import seaborn as sns

  # Import models from scikit learn module:
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LogisticRegression
  from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score
  from sklearn.preprocessing import StandardScaler
  from scipy.stats import zscore

  df = pd.read_csv('breast_cancer_data.csv')

  # Encode malignant as 1, benign as 0
  df['diagnosis'] = df['diagnosis'].replace({'M':1, 'B':0})

  # Imports/load data
  predictor_var = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']

  #print(df.head())

  # Print distinct diagnosis values and frequency in dataset
  print(df['diagnosis'].value_counts())

  # Test if the number of unique IDs is equal to sample size, i.e. no repeated patients
  print(df['id'].nunique())
  print(df['id'].count())

  unique_ids = df.id.nunique()==df.id.count()
  print(unique_ids)

  # At a maximum, there should be no more than the smallest size class divided by 10 number of features
  max_features = min(df['diagnosis'].value_counts() / 10)
  print(max_features)

  # See which features have extreme outliers
  sns.boxplot(data=np.log(df[predictor_var]+.01).apply(zscore))
  plt.xticks(rotation=45);
  plt.show()
  plt.close()

  #5. Uncomment the code to remove the samples with extreme fractal_dimensions_mean values:
  q_hi  = df["fractal_dimension_mean"].quantile(0.99)
  df_filtered = df[(df["fractal_dimension_mean"] < q_hi)]
  
  #6. Run the boxplot again but with the filtered dataframe:
  sns.boxplot(data=np.log(df_filtered[predictor_var]+.01).apply(zscore))
  plt.xticks(rotation=45);
  plt.show()
  plt.close()

#+end_src

#+RESULTS:
: diagnosis
: 0    357
: 1    212
: Name: count, dtype: int64
: 569
: 569
: True
: 21.2

* Assumptions of Logistic Regression II

1. *Features linearly related to log odds*

   Similar to linear regression, the underlying assumption of logistic regression is that the features are /linearly related/ to the logit fo the outcome. To test this visually, we can use Seaborn's regplot, with the parameter ~logistic=True~ and the x value as our feature of interest. If this condition is met, the fit model will resemble a sigmoidal curve (as is the case when ~x=radius_mean~).

   We've added code to create another plot using the feature fractal_dimension_mean. How do the curves compare?

2. *Multicollinearity*

   Like in linear regression, one of the assumptions is that there is no multicollinearity in the data. Meaning the features /should not be highly correlated./ Multicollinearity can cause the coefficients and p-values to be inaccurate. With a correlation plot, we can see which features are highly correlated and then we can drop one of the features.

   We're going to look at the "mean" features which are highly correlated with each other using a heatmap correlation plot.

** Task 1
Uncomment the relevant lines of code and press Run to see the heatmap.

There are two features that are highly positively correlated with ~radius_mean~. Can you spot them?

The heatmap shows that ~radius_mean~, ~perimeter_mean~, and ~area_mean~ are all highly positively correlated. It would be beneficial to only keep one of these three features to avoid multicollinearity.

** Task 2
Not including ~radius_mean~, ~perimeter_mean~, or ~area_mean~, there is another pair of features that are highly correlated. Create an array named ~correlated_pair~ containing these two features.

** Script.py
#+begin_src python :results output
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import seaborn as sns

  # Import models from scikit learn module:
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LogisticRegression
  from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score

  df = pd.read_csv('breast_cancer_data.csv')

  #encode malignant as 1, bening as 0
  df['diagnosis'] = df['diagnosis'].replace({'M':1, 'B':0})

  predictor_var = ['radius_mean', 'texture_mean', 'perimeter_mean','area_mean', 'smoothness_mean', 'compactness_mean','concavity_mean', 'symmetry_mean', 'fractal_dimension_mean']

  x = df[predictor_var]

  # Compare the curves
  sns.regplot(x='radius_mean', y='diagnosis', data=df, logistic=True)
  plt.show()
  plt.close()

  sns.regplot(x='fractal_dimension_mean', y='diagnosis', data=df, logistic=True)
  plt.show()
  plt.close()

  # Uncomment the heatmap and identify the two features that are highly correlated with the radius_mean
  plt.figure(figsize = (10, 7))
  sns.heatmap(x.corr(), annot=True)
  plt.show()

  #2. Identify the other highly correlated pair. Define an array called correlated_pair containing the two features you've identified.
  correlated_pair = ['compactness_mean', 'concavity_mean']
#+end_src

#+RESULTS:

* Scikit-learn implementation

** Model Training and Hyperparameters
Now that we have checked the assumptions of Logistic Regression, we can train and predict a model using scikit-learn. We will first set the /hyperparameters/ of our model.

Hyperparameters are set before the model implementation step and tuned later to improve model performance. Conversely, parameters are the result of model implementation, such as the intercept and coefficients.

#+begin_quote
*Note:*
Within scikit-learn these hyperparameters are often referred to as "parameters" which might cause some confusion. It is worth noting that the meaning within scikit-learn documentation refers to these being "parameters" of the function and not of the model itself.Pu
#+end_quote

** Evaluation Metrics
Despite the name, logistic regression is being used as classifier here, son any evaluation metrics for classification tasks will apply. The simplest metric is accuracy - how many correct predictions did we make out of the total? However, when classes are imbalanced, this can be a misleading metric for model performance. Similarly, if we care more about accurately predicting a certain class, other metrics may be more appropriate to use, such as precision, recall, or F1-score may be better to evaluate performance. All of these metrics are available in scikit-learn.

#+begin_src
Accuracy = (TP + TN)/Total

Precision = TP/(TP + FP)

Recall = TP/(TP + FN)

F1 score = 2*((Precision*Recall)/(Precision+Recall))
#+end_src

** Which metrics matter most?
For our breast cancer dataset, predicting ALL malignant cases as malignant is of the utmost importance -and even if there are some false positives (bening cases that are marked as malignant), these likely will be discovered by follow-up tests. Whereas missing a malignant case (classifying it as benign) could have deadly consequences. Thus, we want to minimize false negatives. This in turn will maximize the recall ratio (also known as the sensitivity or true positive rate).

** Task 1 : Model Hyperparameters
Using the mean predictor variables defined in the code editor, define a logistic regression classifier named ~log_reg~ using scikit-learn with no regularization (i.e. no penalty) and an intercept term. This will mean setting:

    - the input argument ~penalty~ to 'none' (the default penalty is '12').

    - the input argument ~fit_intercept~ to True

Print the hyperparameters using the ~.get_params()~ method.

** Task 2: Model Parameters
Fit the model to the training data and calculate the resulting model coefficients and intercept. Store them as variables coefficients and intercept respectively and print their values.

** Task 3: Evaluation metrics (Accuracy, Precision, Recall, F1)
Now we will explore some of the most common evaluation metrics for classification.

First, use .predict() to predict the outcome variable and save it as y_pred. Then complete the following:

    - Calculate accuracy and store it as a variable named accuracy.

    - Calculate precision and store it as precision.

    - Calculate recall and store it as recall.

    - Calculate the F1-score and store it as f1.

Print the evaluation metrics you just stored. Which metric gives  the highest and lowest values?

** Task 4: Evaluation metrics (Confusion Matrix)
Take a look at the confusion matrix by uncommenting the code lines listed in # 4.

How many tumors were predicted correctly? How many people's tumors were predicted as bening but wew actually malignant? And vise versa?

Out of the 171 samples, 155 were predicted correctly (57+98).

6 people were told their tumor was benign and it was actually malignant (false negative) and 10 people were told their tumor was malignant but it was actually benign (false positive).

Even though our model was fairly accurate, we should strive for improvement, especially when dealing with medical care!

** Script.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import seaborn as sns

  # Import models from scikit learn module:
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LogisticRegression
  from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score

  df = pd.read_csv('breast_cancer_data.csv')

  #encode malignant as 1, benign as 0
  df['diagnosis'] = df['diagnosis'].replace({'M':1, 'B':0})
  predictor_var = ['radius_mean', 'texture_mean', 'compactness_mean', 'symmetry_mean']

  outcome_var = 'diagnosis'

  x_train, x_test, y_train, y_test = train_test_split(df[predictor_var], df[outcome_var], random_state=0, test_size=0.3)

  # Fit a Logistic Regression model with the specified hyperpatameters
  log_reg = LogisticRegression(penalty='none', fit_intercept=True)
  print(log_reg.get_params())

  # Fit the model to the training data and obtain coefficients and intercept
  log_reg.fit(x_train, y_train)
  coefficients = log_reg.coef_
  intercept = log_reg.intercept_
  print('coefficients: ', coefficients)
  print('intercept: ', intercept)

  # Calculate the accuracy, precision, recall and f1-score on the testing data
  y_pred = log_reg.predict(x_test)
  accuracy = accuracy_score(y_test, y_pred)
  precision = precision_score(y_test, y_pred)
  recall = recall_score(y_test, y_pred)
  f1 = f1_score(y_test, y_pred)

  print(f'Test set accuracy:\t{accuracy}')
  print(f'Test set precision:\t{precision}')
  print(f'Test set recall:\t{recall}')
  print(f'Test set f1-score:\t{f1}')

  # Remove the comments from the following code block to print the confusion matrix
  test_conf_matrix = pd.DataFrame(
      confusion_matrix(y_test, y_pred),
      index=['actual no', 'actual yes'],
      columns=['predicted no', 'predicted yes']
       )
  print(test_conf_matrix)

#+end_src

#+RESULTS:
#+begin_example
{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'none', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
coefficients:  [[ 1.07991529  0.28744917 30.96010078 29.97251652]]
intercept:  [-30.15566409]
Test set accuracy:	0.9064327485380117
Test set precision:	0.8507462686567164
Test set recall:	0.9047619047619048
Test set f1-score:	0.8769230769230769
            predicted no  predicted yes
actual no             98             10
actual yes             6             57
#+end_example

* Prediction Thresholds
Logistic regression not only predicts the class of a sample, but also the probability of a sample belonging to each class. It provide us with a measure of certainty associated with each prediction. In the default implementation in scikit-learn, a probability greater than 50% means that the predicted outcome will belong to the positive class. This ir referred to as a prediction threshold. If two samples have predicted probabilities of 51% and 99%, both will be considered positive with the default threshold. However, if the threshold is increased to 60%, a predicted probability of 51% will be assigned the negative class.

[[./malignant_benign.png]]

Consider the histogram of the predicted probabilities for the logistic regression classifier shown above. The benign (or negative class) is depicted in blue, and the malignant (or positive class) in orange for the breast cancer data set. The benign cases are heavily clustered around zero, which is good as they will be correctly classified as benign, whereas malignant cases are heavily clustered around one. The vertical lines depict hypothetical threshold values at 25%, 50%, and 75%. For the highest threshold, almost all the samples above 75% belong to the malignant class, but there will be some benign cases that are misdiagnosed as malignant (false positives). In addition, there are a number of malignant cases that are missed (false negatives). If instead the lowest threshold value is used, almost all the malignant cases are identified, but there are more false positives.

Therefore, the value of the threshold is an additional lever that can be used to tune a model's predictions. A higher value is generally associated with fewer false positives and more false negatives.

** Task 1
In the code editor, we've trained a logistic regression model, fit it to training data and calculated the predicted class. The ~predict_proba~ method gets us the predicted probabilities, which are then saved as ~y_pred_prob~:

#+begin_src python
  y_pred_prob = log_reg.predict_proba(x_test)
#+end_src

We've created a new array ~y_pred_class~ that is 1 when ~y_pred_prob~ > 0.5 and 0 otherwise. Let's check if this array is the same as ~y_pred~. Luckily, there is a numpy method for this!

Use ~np.array_equal()~ and place ~y_pred_class~ and ~y_pred~ as the two arguments. Then set it equal to the variable diff and print it. If they are equal, it will return True.

** Task 2
We're now going to take a look at the confusion matrix for the default threshold value (probability of 0.5 or 50%). Recall that there are four possible categories that a prediction can fall under: True Positive (TP), True Negative (TN), False Postive (FP), False Negative (FN). In a confusion matrix the predicted classes are represented as columns and the actual classes are represented as rows.

| .        | Predicted - | Predicted + |
| Actual - | TN          | FP          |
| Actual + | FN          | TP          |

Uncomment the relevant lines in the code editor and press Run to view the confusion matrix.

** Task 3
We can now calculate the confusion matrices for alternative thresholds.

Calculate the confusion matrices for thresholds of 25% and 75% and store them as cm_25 and cm_75 respectively. Print them to examine how they're different from the confussion matrix for the default threshold value.

To get the class predictions for any threshold value t (in decimals and not percentages!), you can do the following:

#+begin_src python
  predicted_class = (y_pred_prob[:,1] > t)*1.0
#+end_src



** Script.py

#+begin_src python :results output
  import pandas as pd
  import numpy as np

  #Import models from scikit learn module:
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LogisticRegression
  from sklearn.metrics import confusion_matrix

  df = pd.read_csv('breast_cancer_data.csv')
  #encode malignant as 1, benign as 0

  df['diagnosis'] = df['diagnosis'].replace({'M':1, 'B':0})
  predictor_var = ['radius_mean', 'texture_mean', 'compactness_mean', 'symmetry_mean',]
  outcome_var = 'diagnosis'

  x_train, x_test, y_train, y_test = train_test_split(df[predictor_var], df[outcome_var], random_state=0, test_size=0.3)

  log_reg = LogisticRegression(penalty=None,
                               fit_intercept=True,
                               tol=0.0000001,
                               solver='newton-cg')
  log_reg.fit(x_train, y_train)
  y_pred = log_reg.predict(x_test)
  y_pred_prob = log_reg.predict_proba(x_test)

  #Using the predicted probabilities to get the predicted class
  y_pred_class = (y_pred_prob[:,1]>0.5)*1.0

  #Check if it's the same as y_pred
  diff = np.array_equal(y_pred_class, y_pred)
  print(diff)

  #Print the confusion matrix
  print("Confusion Matrix: Threshold 50%")
  cm_50 = confusion_matrix(y_test, y_pred_class)
  print(cm_50)

  ## Confusion matrices for thresholds of 0.25 and 0.75
  y_pred_class = (y_pred_prob[:,1]>0.25)*1.0
  print("Confusion Matrix: Threshold 25%")
  cm_25 = confusion_matrix(y_test, y_pred_class)
  print(cm_25)

  y_pred_class = (y_pred_prob[:,1]>0.75)*1.0
  print("Confusion Matrix: Threshold 75%")
  cm_75 = confusion_matrix(y_test, y_pred_class)
  print(cm_75)

#+end_src

#+RESULTS:
#+begin_example
True
Confusion Matrix: Threshold 50%
[[98 10]
 [ 6 57]]
Confusion Matrix: Threshold 25%
[[94 14]
 [ 2 61]]
Confusion Matrix: Threshold 75%
[[104   4]
 [ 11  52]]
#+end_example
